

More implementation for GAMs, boosting, decision rules



RPF
- RPF preparation exercise (s. sheet 2) ??

- bigger  / realistic comparison of EBM and  / or  RPF to more powerful models (RFs & XGBoost)
    => R implementation for RPF exists, but need to make it work first ??
    => For EBM, only python implementation in interpret_ML paket from Microsoft exists => Would need an R wrapper if to by used inside mlr3 (for efficiently integrating into bigger benchmarks / pipelines)
    => very big effort  ?

    - or implement or run different variants (of RPF or of RFs or of XGBoost), i.p. variants with different depths    =>   see interaction-vs-performance tradeoff exercise in Ch. 1

    
EBM:
- implement EBM from scratch ?  "which is not hard"  ?
    FAST Algo is important part of EBM
    => task to calculate this manually at least once
    => Implement from scratch ?



- theoretic / proof exercises from the original papers (EBM / RPF)

        

- Exercise to demonstrate Selection Bias in CART  (2 features, both equally important / informative, one with more values is preferred)
    - or integrate this / do this together with performance-interpertability-tradeoff exercise ????





Programming-HA for showing / visualizing: Correlated features cause interactions in ML model, even if no interactions are actually present
    - E.g.: Decision tree on two features, and true function only depends on one of them
        If both features independent, tree will only consider correct feature
        If features are strongly correlated, tree will learn interaction
        => Only looking at ML model makes the impression as if interactions were present





- Reproduce "Interpretability vs Performance" tradeoff curve from the lecture
    - Train several models of different complexity on one dataset
        - e.g. Decision Trees or Ensembles with different depths => the deeper the tree / ensemble, the more complex
        - e.g. reuse a Tuning exercise from I2ML ?
            Exercise on benchmarking (with random forests) from AppML
            Exercise 8-2 from I2ML (training a DT with CART and different max_depth)
            Exercise 9 from I2ML on RFs  ????
            Exercise 12 on nested resampling from I2ML: Compare all models in this exercise. Although their complexity / performance is all very similar ??

