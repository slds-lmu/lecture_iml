\loesung{

This exercise shows how "inherently interpretable" models (e.g. LMs or GAMs) can become more and more powerful by becoming more and more complex, and that in the end this leads to models not interpretable at all.
It also shows that manual feature construction (and therefore manual labor) is necessary to make simple or interpretable models powerful, hence people designing these models have to actually understand the underlying problem structure.

This is of course unfeasible for many applications.

}