\loesung{

This exercise shows how "inherently interpretable" models (e.g. LMs or GAMs) can become more and more powerful by becoming more and more complex, and that in the end this leads to models not interpretable at all.
It also shows that manual feature construction (and therefore manual labor) is necessary to make simple or interpretable models powerful, and hence people designing these models have to actually understand the underlying problem structure.

This is, of course, unfeasible for many applications.

The first data set only contains simple interactions and demonstrates that both a linear model with exactly the correct interaction term as well as a normal GAM (without interactions) cannot model this very well.

The second data set is designed particularly for strong GAMs (very complicated, but smooth, interaction terms).

The third data set could be modeled by a very simple tree (more specifically, a tree with linear models in the leaves, i.e., an MOB).
So EBMs perform very well here, and LMs as well as GAMs do not due to the large step.
Although GAMs could perform well if they have a large enough degree of flexibility.

}