\textbf{Quiz:}

\begin{enumerate}
    \item Explain the feature importance provided by model-based boosting. What is the difference from the (Gini / L2) feature importance from decision trees?
    
    \item When calculating feature importances in decision trees (or in tree ensembles), can you still use the efficient split selection algorithm explained in the lecture? Why or why not?
    
    \item Does a GAM contain hyperparameters? Which?

    \item Is a GAM always preferred over a linear model, or are there also disadvantages?

    \item When using boosting, can we always combine different base learners by simply adding the parameters?

    \item Is the following true or false: ``When using boosting with linear models as base learners, the final model will also be a linear model.''

    % \item Is the following true or false: ``When using boosting, the final model will always be a linear model.''

    \item For which model classes X (e.g. linear model, decision tree, neural network) does the following hold ``When performing boosting with model type X as base learners, the final model will again be a model of type X.''
    
    \item Of which model class introduced in the I2ML lecture are EBMs a special case?
    
    \item How are the final components in an EBM computed and interpreted?

    \item How is the number of features considered in each single tree inside an EBM restricted? Why?

    \item What is the advantage of using low-depth trees in EBMs?

    \item In which scenarios are EBMs most useful?

    % \textit{Advanced, because relies on inclusion-exclusion principle / calculations in FAST:}
    % \item How can FAST reuse computational results from the first (univariate) part of the EBM training algorithm?
    
\end{enumerate}

