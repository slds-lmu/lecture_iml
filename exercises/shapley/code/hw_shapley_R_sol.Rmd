---
title: "Exercise Shapley"
output:
  pdf_document: default
  html_document: default
---

# Exercise 1: The Shapley Value

### Libraries
```{r}
library(sets)
library(dplyr)
library(randomForest)
library(combinat)

set.seed(123456)
```


## 1a)
```{r}
### Define the payoff function.
payoff <- function(coalition) {
  ### Define boolean variables that indicate whether Timnit, Margret, Samy, Jeff
  ### and Larry are in the set or not.
  t <- 't' %in% coalition
  s <- 's' %in% coalition
  m <- 'm' %in% coalition
  j <- 'j' %in% coalition
  l <- 'l' %in% coalition

  ### Definition of the payoff as given in the task.
  10*t + 10*m + 10*s + 2*j + 20*(t & m) + 20*(t & m & s) - 30*((t | m | s) & j)
}

### Testing
population <- c('j', 'l', 'm', 's', 't')
payoff(c('t','m'))
payoff(c('t','j','s'))
payoff(c())
payoff(population)
```

## 1b)
```{r}
# https://www.rdocumentation.org/packages/methods/versions/3.6.2/topics/setClass
# https://www.rdocumentation.org/packages/sets/versions/1.0-25/topics/set
# all_unique_subsets_2 <- function(population) {
#   if (length(population) == 0) {
#     return(set(set()))
#   } else {
#     population <- population[!duplicated(population)] # Remove double elements
#     subsets_wo_member <- all_unique_subsets(population[-c(1)])
#     subsets_w_member <- sapply(subsets_wo_member, function(s){
#       set_union(s, set(population[1]))
#     })
#     return(set_union(subsets_wo_member, subsets_w_member))
#   }
# }

all_unique_subsets <- function(population) {
    ### Use built-in set_power function and convert to a list for further processing.
    as.list(set_power(population))
}

### Testing
print(all_unique_subsets(c()))
# as.list(all_unique_subsets_2(c()))
print(all_unique_subsets(c(2)))
# as.list(all_unique_subsets_2(c(2)))
print(all_unique_subsets(c(2,4)))
# as.list(all_unique_subsets_2(c(2,4)))
print(all_unique_subsets(population))
# as.list(all_unique_subsets_2(population))
```

## Definition of useful functions, used several times below
```{r}
### Define a function which returns a list of all possible
  ### coalitions (represented as sets) without (one or more) specific members.
all_coalitions_without_members <- function(members, population) {
  ### Get all players except the member players.
  population_without_members <- setdiff(population, members)

  ### Get all sets of players without the member players.
  as.list(all_unique_subsets(population_without_members))
}

### Define a payoff difference function (which will be useful for several
  ### subtasks. The v_function argument will be helpful for checking the additivity
  ### property in task 1c).
payoff_diff <- function(member, coalition, v_function=payoff) {
  v_function(c(member, coalition)) - v_function(coalition)
}

### Define a function that calculates the payoff differences given a list of
  ### coalitions (which will be useful for several subtasks. The v_function argument
  ### will be helpful for checking the additivity property in task 1c)).
payoff_diff_list <- function(member, coalitions, v_function=payoff) {
  sapply(1:length(coalitions), function(i) payoff_diff(member, coalitions[[i]], v_function))
}
```

## 1b) (continued)
```{r}
### Calculate the Shapley value for a player using the set definition according to
  ### the formula given in the lecture or in chapter 17 of Christoph Molnar's
  ### book Interpretable Machine Learning
  ### (https://christophm.github.io/interpretable-ml-book/shapley.html#definition).
  ### Input: the set of all players and the value function (this will
  ### be helpful for checking the additivity property in task 1c)
shapley_set <- function(member, population, v_function=payoff) {
  population <- population[!duplicated(population)] # Remove double elements
  all_coalitions_without_member <- all_coalitions_without_members(member, population)

  ### For this, compute the payoff differences with and without the member player for all
  ### coalitions at once.
  payoff_diffs <- payoff_diff_list(member, all_coalitions_without_member, v_function)
  
  #### Calculate the weights which are the combinatorical number of occurrences  
  ### of each set.
  F <- length(population)
  weights <- sapply(1:length(all_coalitions_without_member), function(i) {
    cardinality_coalition <- length(all_coalitions_without_member[[i]])
    factorial(cardinality_coalition) * factorial(F - cardinality_coalition - 1) / factorial(F)
  })
  
  #### The Shapley value now is the weighted sum of the differences.
  sum(weights * payoff_diffs)
}
```

```{r}
### Show the result.
print(shapley_set("t", population))
```

## Another helper function for calculating all Shapley values

```{r}
### Define a function that returns all Shapley values for a given population
  ### (which will be useful for several subtasks. The v_function argument will be
  ### helpful for checking the additivity property in task 1c))
all_shapley_values <- function(population, shapley_fct, v_function=payoff) {
  sapply(1:length(population), function(i) shapley_fct(population[i], population, v_function))
}

shapley_test <- function(population, shapley_fct, v_function=payoff) {
  shapley_values <- all_shapley_values(population, shapley_fct, v_function)
  print(shapley_values)
  sum_ <- sum(shapley_values)
  print(sum_)
  print(v_function(population))
  print(sum_ == v_function(population))
}
```

```{r}
### Testing
shapley_test(population, shapley_set)
```

## 1c)
```{r}
### Calculate a Shapley value using the the permutation based definition.
shapley_perm <- function(member, population, v_function=payoff) {
  ### Generate all permutations from the population (as list of vectors).
  all_perms <- combinat::permn(population)

  ### Find the index of the member player for every permutation.
  member_idxs <- sapply(all_perms, function(x) match(member, x))

  ### Get the coalition for each permutation (which are just all the players with
  ### an index lower than the member player).
  coalitions <- lapply(1:factorial(length(population)), function(i) all_perms[[i]][1:member_idxs[i] - 1])

  ### Estimate the Shapley value according to the order definition formula given
  ### in the lecture. This is just the mean of all the payoff differences.
  differences <- payoff_diff_list(member, coalitions, v_function)
  mean(differences)
}
```

```{r}
### Testing
shapley_test(population, shapley_perm)
```

## 1d)
```{r}
### Calculate the permutation based approximation of the Shapley value with  
  ### num_iter iterations.
shapley_perm_approx <- function(member, population, v_function=payoff, num_iter=100) {
  ### Sample num_iter permutations from the population (the single columns are the permutations).
  perms <- replicate(num_iter, sample(population))
  
  ### Find the index of the member player for every permutation (so in every column,
  ### that´s what the 2 is indicating).
  member_idxs <- apply(perms, 2, function(x) match(member, x))
  
  ### Get the coalition for each permutation (which are just all the players with  
  ### an index lower than the member player).
  ### They are now saved in a list with the length being the number of iterations.
  coalitions <- lapply(1:num_iter, function(i) perms[1:member_idxs[i] - 1, i])
  
  ### Estimate the Shapley value according to the algorithm given in the lecture or
  ### in chapter 17 of Christoph Molnar's book Interpretable Machine Learning
  ### (https://christophm.github.io/interpretable-ml-book/shapley.html#estimating-shapley-values).
  ### This is just the arithmetic mean of the payoff differences 
  ### over all sampled coalitions, same as for the exact order definition.
  differences <- payoff_diff_list(member, coalitions,v_function)
  mean(differences)
}
```

```{r}
### Show the result.
print(shapley_perm_approx('t', population, num_iter=1000))
### Not too bad an estimate compared to the result from tasks b) and c).
### Further Testing
for (i in 3:4) {
  print(shapley_perm_approx('t', population, num_iter=10**i))
}
shapley_test(population, shapley_perm_approx)
for (member in population) {
  print(shapley_perm_approx(member, population, num_iter=10**4))
}
```

The exact numbers here depend of course on the specific parameters, the seed ...

## 1e)
### Symmetry Check
```{r}
### Check the symmetry property of the Shapley value as defined in the lecture or  
  ### in chapter 17 of Christoph Molnar's book Interpretable Machine Learning  
  ### (https://christophm.github.io/interpretable-ml-book/shapley.html#definition).
symmetry_check <- function(j, k, population) {
  all_coalitions_without_candidates <- all_coalitions_without_members(c(j, k), population)
  
  ### Calculate the payoff differences for all possible coalitions including j  
  ### (but excluding k) and for all possible coalitions including k (but excluding j).
  payoff_diff_j <- payoff_diff_list(j, all_coalitions_without_candidates)
  payoff_diff_k <- payoff_diff_list(k, all_coalitions_without_candidates)
  
  ### This logical statement now checks two things:
  ### 1. If the condition is violated.
  ### 2. If the Shapley values are identical.
  ### If the condition is violated the symmetry property holds because of the  
  ### implication "condition => Shapley values identical" which does not give us  
  ### any information whether the Shapley values are identical in this case  
  ### (because from FALSE one can not imply anything).
  ### On the other hand, when the condition is met then the Shapley values have  
  ### to be identical. This is ensured because in this case the condition is not  
  ### violated meaning that the left hand side of the OR is FALSE. Therefore the  
  ### right hand side has to hold which checks whether the Shapley values are identical.
  all(any(payoff_diff_j != payoff_diff_k) | shapley_set(j, population) == shapley_set(k, population))
}
```

```{r}
### Check the symmetry property for all possible combinations of two players.
#### All combinations of two players.
all_combinations_two_players <- as.list(set_combn(population, m = 2))
#### The results of the application of the individual symmetry checks.
symmetry_check_all_combinations <- sapply(1:length(all_combinations_two_players),
        function(i) symmetry_check(as.list(all_combinations_two_players[[i]])[1],
                                   as.list(all_combinations_two_players[[i]])[2],
                                   population))
#### The combined result.
print(all(symmetry_check_all_combinations))
### Seems to work for our toy example.
```

### Dummy Property Check
```{r}
### Check the dummy property of the Shapley value as defined in the lecture or  
  ### in chapter 17 of Christoph Molnar's book Interpretable Machine Learning  
  ### (https://christophm.github.io/interpretable-ml-book/shapley.html#definition).
dummy_check <- function(j, population) {
  all_coalitions_without_candidate <- all_coalitions_without_members(j, population)
  
  ### Calculate the payoff differences for all possible coalitions, once including
  ### and once excluding j.
  payoff_diff_j <- payoff_diff_list(j, all_coalitions_without_candidate)
  
  ### This logical statement now checks two things:
  ### 1. If the condition is violated.
  ### 2. If the Shapley value is 0.
  ### If the condition is violated the dummy property holds because of the implication  
  ### "condition => Shapley value is 0" which does not give us any information  
  ### whether the Shapley value is 0 in this case (because from FALSE one can not  
  ### imply anything).
  ### On the other hand, when the condition is met then the Shapley value has to  
  ### be 0. This is ensured because in this case the condition is not violated  
  ### meaning that the left hand side of the OR is FALSE. Therefore the right hand  
  ### side has to hold which checks whether the Shapley value is 0.
  all(any(payoff_diff_j != 0) | shapley_set(j, population) == 0)
}
```

```{r}
### Check the dummy property for all possible players.
#### The results of the application of the individual dummy property checks.
dummy_check_all_players <- sapply(1:length(population),
                                  function(i) dummy_check(population[i],
                                                          population))
#### The combined result.
print(all(dummy_check_all_players))
### Seems to work for our toy example.
```

### Additivity Check 
```{r}
### Check the additivity property of the Shapley value as defined in the lecture  
  ### or in chapter 17 of Christoph Molnar's book Interpretable Machine Learning  
  ### (https://christophm.github.io/interpretable-ml-book/shapley.html#definition).
### Here we assume that the user is giving us two payoff functions that sum up
### to the original payoff function.
additivity_check <- function(population, payoff_func_1, payoff_func_2) {
  ### Define the combined payoff function.
  payoff_func_1_and_2 <- function(x) payoff_func_1(x) + payoff_func_2(x)
  
  ### Get all the Shapley values when using payoff_func_1 and when using  
  ### payoff_func_2.
  all_shapley_values_payoff_func_1 <- all_shapley_values(population, shapley_set, payoff_func_1)
  all_shapley_values_payoff_func_2 <- all_shapley_values(population, shapley_set, payoff_func_2)
  
  ### Get all the Shapley values when using the combined payoff function.
  all_shapley_values_payoff_func_1_and_2 <- all_shapley_values(population,
                                                               shapley_set,
                                                               payoff_func_1_and_2)
  
  ### Check if the combined Shapley value equals the sum of the Shapley values  
  ### for every member of the population (rounding is necessary here since the  
  ### values can differ because of numerical instabilities during computation).
  all(round(all_shapley_values_payoff_func_1_and_2, 5)
      == round(all_shapley_values_payoff_func_1 + all_shapley_values_payoff_func_2, 5))
}
```

```{r}
### Testing:
### Define a new payoff function (which just consists of the first three summands  
  ### of the original payoff function).
payoff_func_1 <- function(coalition) {
  ### Define boolean variables that indicate whether Timnit, Margret, Samy, Jeff  
  ### and Larry are in the set or not.
  t <- 't' %in% coalition
  s <- 's' %in% coalition
  m <- 'm' %in% coalition
  j <- 'j' %in% coalition
  l <- 'l' %in% coalition
  
  ### Definition of the overall performance.
  10*t + 10*m + 2*j
}

### Define the second payoff function accordingly (which hence consists of the  
  ### second three summands of the original payoff function).
payoff_func_2 <- function(coalition) {
  ### Define boolean variables that indicate whether Timnit, Margret, Samy, Jeff  
  ### and Larry are in the set or not.
  t <- 't' %in% coalition
  s <- 's' %in% coalition
  m <- 'm' %in% coalition
  j <- 'j' %in% coalition
  l <- 'l' %in% coalition
  
  ### Definition of the overall performance.
  20*(t & m) + 20*(t & m & s) - 30*((t | m | s) & j)
}

### Check the additivity property (at least for the above decomposition of the  
  ### original payoff function. Of course others are possible). 
additivity_check_result <- additivity_check(population, payoff_func_1, payoff_func_2)
### Show the result.
print(additivity_check_result)
### Seems to work for our toy example.
```

### Efficiency Check 
```{r}
### Check the efficiency property of the Shapley value as defined in the lecture  
  ### or in chapter 17 of Christoph Molnar´s book Interpretable Machine Learning  
  ### (https://christophm.github.io/interpretable-ml-book/shapley.html#definition).
efficiency_check <- function(population, v_function=payoff) {
  ### The left hand side gives us the total payoff of the population. On the right  
  ### hand side we sum over the Shapley values of each individual player. For the  
  ### efficiency property to hold these must be equal.
  v_function(population) == sum(all_shapley_values(population, shapley_set, v_function))
}
```

```{r}
### Check the efficiency property for the whole population.
#### The result of the application of the efficiency property check for the population.
efficiency_check_result <- efficiency_check(population)
#### Show the result.
print(efficiency_check_result)
### Seems to work for our toy example.
```



# Exercise 2: SHAP

```{r}
### For modelling a random forest we use the ranger package.
library(ranger)

set.seed(123456)
```

## 2a)
```{r}
### Read the dataset via the read.csv()-function.
df <- read.csv(file = "fifa.csv")

### Now transform the character variable Man.of.the.Match which only contains the  
  ### values "Yes" and "No" into a logical variable with values TRUE (former "Yes")
  ### and FALSE (former "No"). Note that this step has to be done before deleting
  ### all character variables because else we would remove our target variable as well.
df$Man.of.the.Match <- ifelse(df$Man.of.the.Match == "Yes", TRUE, FALSE)

### Remove all variables of type character.
df <- df[, !sapply(df, is.character)]

### Here we delete all columns containing at least one NA value.
df <- df[ , apply(df, 2, function(x) !any(is.na(x)))]

### Classical training and testing split.
train_idx <- sample(nrow(df), 0.7 * nrow(df))
training_data <- df[train_idx, ]
test_data <- df[-train_idx, ]

### Split of our training and testing datasets into features and  
  ### target which will be used later on.
X_train <- training_data[ , -which(names(training_data) == "Man.of.the.Match")]
X_test <- test_data[ , -which(names(test_data) == "Man.of.the.Match")]
y_train <- factor(training_data[ , which(names(training_data) == "Man.of.the.Match")])
y_test <- factor(test_data[ , which(names(test_data) == "Man.of.the.Match")])

### Modelling a random forest using the ranger package. Other packages are possible  
  ### as well.
classifier_RF <- ranger(Man.of.the.Match ~ ., training_data)
classifier_RF

### Example instance
predict(classifier_RF, X_test[1:5,])$predictions

### As a classifier, this model of course produces probabilities as outputs, which are then
### transformed into a binary prediction. This is not very well suited for Shapley values,
### since they basically require a numerical or continuous target, in particular for the
### efficiency axiom to hold up. We will nevertheless work with the binary predictions
### (interpreted numerically as 0 or 1) in this exercise, but since the Shapley or SHAP values
### are both obtained through averaging, they then still always represent probabilities
### or relative frequencies.
```

## 2b) 
```{r}
### Computing the marginal sampling based SHAP value function as defined in the  
  ### lecture or in chapter 18 of Christoph Molnar's book Interpretable Machine
  ### Learning (https://christophm.github.io/interpretable-ml-book/shap.html#kernelshap).
marginal_vfunc <- function(S, observation, X, model, nr_samples = 100) {
  ### Get nr_samples random observations from the given dataset.
  X_tmp <- X[sample(nrow(X), nr_samples, replace = TRUE), ]
  
  ### Now replace the features given in S for our samples by the values of the  
  ### features in S given by the observation.
  X_tmp[ , S] <- observation[ , S]
  
  ### Use the model to predict the target and take the arithmetic mean of the  
  ### predictions.
  mean(predict(model, X_tmp)$predictions)
}

marginal_vfunc(names(X_train)[1:3], X_test[1, ], X_train, classifier_RF, nr_samples = 1000)
marginal_vfunc(names(X_train)[1], X_test[1, ], X_train, classifier_RF, nr_samples = 1000)
marginal_vfunc(names(X_train)[1:2], X_test[1, ], X_train, classifier_RF, nr_samples = 1000)
marginal_vfunc(names(X_train)[1:2], X_test[2, ], X_train, classifier_RF, nr_samples = 1000)
```

## 2c)
```{r}
### Testing the approximation algorithm
model_specific_marginal_vfunc <- function (coalition) {
  marginal_vfunc(coalition, X_test[1, ], X_train, classifier_RF, nr_samples = 100)
}
print(shapley_perm_approx(names(X_test)[3], names(X_test),
                          v_function = model_specific_marginal_vfunc))
print(shapley_test(names(X_test), shapley_perm_approx, v_function = model_specific_marginal_vfunc))
### Here we see exactly the problem mentioned earlier: The Shapley value (or the approximated Shapley value) represent a probability, but could be any value, whereas the prediction of the model is either 0 and 1, and efficiency does not hold in this specific situation anymore.
```

## 2d)
```{r}
### A function to sample for every entry in our data whether to include it or not.
sample_mask <- function(nrow, ncol) {
  ### Sample nrow * ncol times from a fair binomial distribution and arrange the  
  ### result in an accordingly shaped matrix.
  matrix(rbinom(nrow * ncol, 1, 0.5), nrow = nrow)
}

### A function to return the weights for our randomly drawn mask. The formula can  
  ### be found in chapter 18 of Christoph Molnar's book Interpretable Machine
  ### Learning (https://christophm.github.io/interpretable-ml-book/shap.html#kernelshap),  
  ### it's the defintion of pi_x(|z'|) where |z'| is the 1-norm (so in our case just
  ### the number of included features given by a 1 in the mask).
shap_weights <- function(mask) {
  ### The number of features.
  p <- ncol(mask)
  
  ### The number of included features, |z'|.
  z_prime_abs <- rowSums(mask)
  
  ### The application of the formula.
  (p - 1) / (choose(p, z_prime_abs) * z_prime_abs * (p - z_prime_abs))
}

### A function to create the dataset generated during the SHAP calculations. This  
  ### are just n_samples of the original observations
### where the positions in the mask indicate where we replace the original  
  ### values with the ones of the given observation obs.
replace_dataset <- function(obs, X, n_samples, mask) {
  ### Get n_samples random observations from the given dataset.
  X_new <- X[sample(nrow(X), n_samples, replace = TRUE), ]
  
  ### Blow up the size of the given observation to be compatible to our sampled  
  ### dataset.
  obs_rep <- obs[rep(1, n_samples), ]
  
  ### Now apply the mask. At the positions where the mask is 0 we just get the first
  ### term. At the positions where the mask is 1 we basically subtract X_sample  
  ### from X_sample and what is left is the respective entry in df_obs. So at  
  ### these positions we effectively replace the original entries by the ones in df_obs.
  X_sample <- X_new - mask * (obs_rep - X_new)
  
  ### Return the result.
  X_sample
}

### Now, we put things together and implement the sampling based KernelSHAP funtion.
kernel_shap <- function(obs, X, n_samples, model) {
  ### Get the SHAP weights and the predictions of a random sample of the given data.
  mask <- sample_mask(n_samples, ncol(X))
  df <- replace_dataset(obs, X, n_samples, mask)
  weights <- shap_weights(mask)
  pred <- predict(model, df)$predictions
  
  ### Now save the mask as well as the respective predictions.
  df_shap <- as.data.frame(mask)
  names(df_shap) <- names(X)
  df_shap$pred <- pred
  
  ### Fit a weighted linear model using the above calculated weights. Since we  
  ### are in a binary setting, we perform logistic regression.
  wls_model <- glm(pred ~ ., family = binomial, data = df_shap, weights = weights)
  wls_model$coefficients
}
```

## 2e)
```{r}
### Calculate the KernelSHAP values for the first instance of the test set.
obs <- X_test[1, ]
shap_vals <- kernel_shap(obs, X_train, 1000, classifier_RF)
shap_vals
```

## 2f)
SHAP values are expensive to compute. TreeSHAP offers a more efficient 
implementation that exploits the structure of tree-based models. Advanced 
knowledge: A further advantage of TreeSHAP is that it resamples the variables 
such that the joint distribution is preserved.

NOTE: treeshap historically did not work for some specific versions of R.
If problems occur, you can resort to python or up- or downgrade your R version.
This code here was tested with R 4.5.0 and treeshap 0.3.1.
```{r}
### To install the package.
library(treeshap)
unified_RF <- ranger.unify(classifier_RF, X_train)
explainer <- treeshap(unified_RF, X_test)
plot_contribution(explainer, obs = 1)
```

# Exercise 3: LIME

```{r}
library("DALEX")
library("randomForest")
library("DALEXtra")
library("iml")
library("lime")

set.seed(123456)
```

## 3a)
```{r}
### Apply a complex model to the data.
rf_model <- randomForest(Man.of.the.Match ~ ., training_data)

model_ex <- DALEX::explain(model = rf_model,  
                           data = X_train,
                           y = y_train, 
                           label = "Random Forest")

### Now we try to get local explanations for the complicated model via LIME.
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

iml_obs <- predict_surrogate(explainer = model_ex, 
                  new_observation = X_test[1, ],
                  type = "lime")

plot(iml_obs)
```

## 3b)
```{r}
### Set the kernel width to 10.
iml_obs <- predict_surrogate(explainer = model_ex, 
                  new_observation = X_test[1, ],
                  type = "lime",
                  kernel_width = 10)

plot(iml_obs)

### Set the kernel width to 0.001.
iml_obs <- predict_surrogate(explainer = model_ex, 
                  new_observation = X_test[1, ],
                  type = "lime",
                  kernel_width = 0.001)

plot(iml_obs)
```

## 3c)
Both SHAP and Lime rely on a linear model approximation of the model. For Lime, 
the normal feature values are used, for SHAP a transformed distribution 
indicating coalition membership for a sample is relied upon. Lime weights the 
different samples according to their distance to the observation of interest. In 
contrast, SHAP weights them according to the Shapley kernel weights 
(which simulate sampling random permutations).
