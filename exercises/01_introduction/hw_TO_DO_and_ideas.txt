

Explicitly derive correlation measure or \(R^2\) for non-linear models / generalizations of \(R^2\)

    Or generalization of Pearson coefficient for non-linear relationships ?  (see also in-class quiz task)



Explicit calculating example for mutual information:

    - for independent variables
    - for dependent variables with correlation coefficient \(\neq 0\)
    - dependent variables with 0 correlation (because not linearly dependent)
    - MI also for categorical variables



    => Directly as extension of HA 1:
    - first apply def. of MI directly => no meaning, because distrbution not estimated
    - estimate distribute very easily using buckets



- review of basics on (in-)dependence, conditional probabilities / expectations, joint / marginal distributions



- Interactions

    Some complicated function using sine and cosine, which actually exhibits no interactions (which can only be seen after applying Angle sum identities (Additionstheoreme))

