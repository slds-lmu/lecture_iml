

- interpretability vs. performance tradeoff
    After having trained an ML-model, when training a different, more interpretable model, this always increases performance.
    Wrong. There is no rule for the general case, this may happen, but not necessarily. In most cases, performance will decrease. (interpretability-performance-tradeoff) Another situation may be that insights from interpreting a model (or using an inherently interpretable model) can of course lead to a model with better performance.

- For using IML methods, the training data is always enough.
    Wrong. Often additional data points are necessary to obtain unbiased interpretation results. This is why some methods use model refitting.

- Machine learning interpretability methods never consider changing the original data.
    Wrong, e.g. counterfactual explanations explicitly consider changes in a data point (can also be a training data point). Also e.g. ALE or permutation-based feature importance can be considered to do this.

- Feature interactions can be easily learned by an ML-model.
    Wrong. Depends on the type of model whether the model is able at all to learn the interactions (e.g. for big neural networks possible, for linear models this depends on the terms included). Additionally, interactions are often difficult to leanr / to identify if not using a very complex model (e.g. an ensemble or NN), i.p. if one has many features (number of possible interactions increases exponentially) or there are dependencies between the features (as is often the case in practice).

- more on specific motivations / goals for interpretability ??

- Data Attribution

- Counterfactual Explanations

- levels of interpretability



- review of basics on (in-)dependence, conditional probabilities / expectations, joint / marginal distributions



