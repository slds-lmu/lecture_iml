\textbf{Quiz:}\\ \noindent
\vspace{0.1cm}

Which of the following statement(s) is/are correct?  
\begin{enumerate}
    \item Interpretation methods are \textit{only} used to explain the global behavior of a model. 

    %	\item Model-agnostic methods need access to gradients to explain a model. 
    
    \item If a model-agnostic and a model-specific interpretation method are applied on the same ML model, the output of the two methods will always be the same.
    
    \item In IML we distinguish between global IML methods, which explain the behavior of the model over the entire feature space, and local IML methods, which only explain the prediction of individual observations. 
    
    %   \item We can also draw conclusions about feature importance from feature effect methods. 
    
    \item Technically, Pearson's correlation is a measure of \textit{linear} statistical dependence. 
    
    \item All in the lecture mentioned measures for correlation and dependencies are limited to continuous random variables.
    
    \item Features that have an equal feature effect are correlated. 

    % Interactions - definition
    \item A feature interaction between two features $x_j$ and $x_k$ is apparent if a change in $x_j$ influences the impact of $x_k$ on the target.
        
    % interpretability vs. performance tradeoff
    \item When training two different ML-models (denote them with model \(m_1\) and model $m_2$), and if $m_1$ is inherently more interpretable than \(m_2\), then $m_1$ will always perform better (i.e. make more accurate predictions) than $m_2$.

    % Dimension: Model refitting
    \item For using IML methods, access to the training data is always enough.

    % Feature interactions vs. data dependence, interpretability-performance-tradeoff
    \item Feature interactions are difficult to learn for an ML-model.
    
    % Dimension: Model refitting, permuting / changing data
    \item Machine learning interpretability methods never consider changing the original data.
    
    % Feature effect methods vs. feature importance methods
    \item While feature effect methods show the influence of a feature on the target, feature importance methods focus on a feature's impact on the model performance.
    
\end{enumerate}