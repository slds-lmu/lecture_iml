
\begin{enumerate}[a)]
\item 
Overall, all customers, regardless of their personal status and gender, have on 
average a high probability of being a low (good) risk for the bank. 
The average marginal prediction for divorced or separated male customers
reveals a slightly higher risk for this group. 
\item 
The ALE is faster to compute and unbiased. Unbiasedness means that it does 
not suffer from the extrapolation problem which is especially aparent in PDPs 
when features are correlated and interaction terms are part of the ML model.
% SAY in CLASS:
% Since \texttt{personal\_status\_sex} is a categorical feature, we cannot 
% use the Pearson correlation coefficients (even for 
% numerical features it is not always a good idea (see in-class exercise)). 
% For two categorical features we can use a $\Chi^2$-test 
% and for categorical vs. a numerical features we could rely on a one-way ANOVA F-test. 
\item 
The following code computes the pairwise sum of the absolute differences of relative 
frequencies in the categories of a categorical feature $x_j$ based on a 
feature $x_k$

<<echo= FALSE, results='hide'>>= 
order_levels = function(data, feature.name) {
  #'  Orders levels of a nominal scaled feature according to other features in 
  #'  a dataset.
  #'  
  #'  @param data (data.frame): data which contains feature which 
  #'  should be ordered but also other features used for ordering.
  #'  @param feature.name (character(1)): name of categorical feature which shoul 
  #'  be ordered
  #'      
  #'  Returns: the vector of ordered class labels
  feature = data[, feature.name]
  others = setdiff(colnames(data), feature.name)
  feature.lev = unique(feature)
  
  # Iterate over other features 
  dists = lapply(others, function(k) {
    feature.k = data[, k]
    if (inherits(feature.k, "factor") | inherits(feature.k, "character")) {
      dists = get_diff_cat(feature.k, feature)
    } else {
      dists = get_diff_numeric(feature.k, feature)
    }
    dists
  })
  dists.cumulated.long = as.data.frame(Reduce(function(d1, d2) {
    d1$dist = d1$dist + d2$dist
    d1
  }, dists))
  # Create a matrix of distances
  dists.cumulated = reshape2::dcast(dists.cumulated.long, class1 ~ class2, value.var = "dist")[, -1]
  rownames(dists.cumulated) = colnames(dists.cumulated)
  # conduct multi-dimensional scaling (here:principal coordinates analysis)
  # based on dissimilarity matrix it assigns to each item a location in a low dimensional space 
  # the closer the location, the more similar the items are
  scaled = cmdscale(dists.cumulated, k = 1)
  feature.lev[order(scaled)]
}

get_diff_numeric = function(feature.k, feature.j) {
  #'  Calculates the pairwise distances of classes of j of a numeric feature k 
  #'  
  #'  @param feature.k (character|factor): vector of values of numeric feature for which relative 
  #'  frequencies per class are calculated 
  #'  @param feature.j (character|factor): vector of values of categorical feature for which similarity 
  #'  based on feature k should be assessed. 
  #'      
  #'  Returns:
  #'      a data.frame with three columns: 
  #'      * class1: name of the first class
  #'      * class2: name of the second class 
  #'      * dist: the distributional distance between the two classes for feature k
  
  # set up data.frame which we will fill later
  dists = expand.grid(unique(feature.j), unique(feature.j))
  colnames(dists) = c("class1", "class2")
  # get decentiles
  quants = quantile(feature.k, probs = seq(0, 1, length.out = 10), na.rm = TRUE, names = FALSE)
  # derive empirical distribution function for each category
  ecdfs = data.frame(lapply(unique(feature.j), function(lev) {
    x.ecdf = ecdf(feature.k[feature.j == lev])(quants)
    return(x.ecdf)
  }))
  colnames(ecdfs) = unique(feature.j)
  # get pairwise absolute distances of empirical distribution function 
  # between the different categories
  ecdf.dists.all = abs(ecdfs[, dists$class1] - ecdfs[, dists$class2])
  # get maximum distance over decentiles for each pair of categories
  dists$dist = apply(ecdf.dists.all, 2, max)
  return(dists)
}

@

<<echo=TRUE, results='hide'>>=
get_diff_cat <- function(feature.k, feature.j) {
  
  #'  Calculates the pairwise distances of classes of j of a categorical feature k 
  #'  
  #'  @param feature.k (character|factor): vector of values of categorical 
  #'  feature for which relative frequencies per class are calculated 
  #'  @param feature.j (character|factor): vector of values of categorical 
  #'  feature for which similarity based on feature k should be assessed. 
  #'      
  #'  Returns:
  #'      a data.frame with three columns: 
  #'      * class1: name of the first class
  #'      * class2: name of the second class 
  #'      * dist: the distributional distance between the two classes for feature k
  
  # set up data.frame which we will fill later
  dists <- expand.grid(unique(feature.j), unique(feature.j))
  colnames(dists) <- c("class1", "class2")
  # get relative frequency table
  x.count <- as.numeric(table(feature.j))
  A <- table(feature.j, feature.k) / x.count
  dists$dist <- rowSums(abs(A[dists[, "class1"], ] - A[dists[, "class2"], ])) / 2
  return(dists)
}
@

For our task at hand, we obtain the following distances

<<echo=TRUE, results='markup'>>=
credit  = data.frame(read.csv("code/datasets/credit.csv"))
get_diff_cat(feature.k = credit[,"employment_duration"], feature.j = credit[,"personal_status_sex"])
@

Overall, the following ordering of \texttt{personal\_status\_sex} was returned by the method: 
<<echo=TRUE, results='markup'>>=
order_levels(credit, "personal_status_sex")
@
The ordering seems to be feasible, since categories including females are close to each other 
and also categories with males. 
Also the ordering of males according to their relationship status seems to make sense, since typically 
the process is: single, then married and then divorced :-).

\item \textbf{Bonus:} ALE and PDP are global interpretation tools which base their insights on averages (of predictions or 
prediction differences) over whole test sets.
Indeed vulnerable groups are typically not the majority of a population but have a low proportion, 
and biases might be overlooked.
Therefore, local explanation tools should be consulted, in addition to these methods in order to 
identify pointwise biases or discriminative behavior.

\end{enumerate}