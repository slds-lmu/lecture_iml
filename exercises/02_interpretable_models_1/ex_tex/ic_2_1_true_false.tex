\textbf{Quiz:}
\begin{enumerate}
   	\item In which scenarios are inherently interpretable models usually much harder to interpret?
   	\item Why does interpretability usually become worse or more difficult when the generalization performance of the model improves?
   	\item Should we always prefer interpretable models? Explain and describe for which use cases interpretable models would be inconvenient.
    \item Can we always fully interpret "inherently interpretable" models without needing further, more advanced interpretability methods?
    
  	\item ``In the linear model, the effect and importance of a feature can be inferred from the estimated $\beta$-coefficients.'' Is this statement true or false? Explain your answer.
    \item What assumptions does the classical linear model impose on the data?
    \item How can we model categorical or other non-numerical features and / or targets with a linear model? How about a decision tree?
   	\item What is so special about LASSO compared to an LM with respect to interpretability? Would you always prefer LASSO over an LM?
    
   	\item Do the beta-coefficients of GLM always provide simple explanations with respect to the target outcome to be predicted? 
    \item How would the interpretation of single coefficients for GLMs other than logistic regression work?
    
  	\item Since one can add arbitrary complex terms to a linear model, why is it still not used much more for more complex problems?
   	\item How can we use inherently interpretable models to provide insights as to whether two features are dependent?
    
   	\item What are the disadvantages of CART? Which methods address them and how?
\end{enumerate}


    
    