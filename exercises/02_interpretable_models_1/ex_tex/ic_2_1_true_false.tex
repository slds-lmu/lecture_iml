\textbf{Quiz:}
\begin{enumerate}
   	\item In which scenarios are inherently interpretable models usually much harder to interpret?
   	\item Why does usually interpretability become worse or more difficult if the generalization performance of the model improves?
   	\item Should we always prefer interpretable models? Explain and describe for which use cases interpretable models would be inconvenient?
  	\item In the linear model, the effect and importance of a feature can be inferred from the estimated $\beta$-coefficients. Is this statement true or false. Explain!
   	\item What is so special about LASSO compared to a LM with regards to interpretability? Would you always prefer LASSO over a LM?
   	\item Do the beta-coefficients of GLM always provide simple explanations with respect to the target outcome to be predicted? 
   	\item Explain the feature importance provided by model-based boosting. What is the difference to the (Gini) feature importance from decision trees?
   	\item How can we use inherently interpretable models to provide insights whether two features are dependent?
   	% x1 auf x2 modellieren (linear oder nicht-linear) und auf die goodness of fit measures wie R^2 gucken
   	\item What are the disadvantages of CART? What methods address them and how?
\end{enumerate}