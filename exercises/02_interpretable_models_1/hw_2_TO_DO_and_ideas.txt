

Basic idea for chapters 2 & 3 (interpretable models): Implement all these methods (all these models) once



- exercise 2 as a recap for standard error



- see chapter 1: Exercise to demonstrate that model learns interactions, even though none are present, but features are correlated
    -> could use a linear model here for this?



- Interpretability-Performance-Tradeoff exercise from chapter 1
    - Here in chapter 2: Exercise to compare low-complexity vs. high-complexity interpretable models (in order to understand that "interpretable models" are not always interpretable, if they are too complex)





On homework 2-4: CART Implementation:

    Ideas of possible tasks:
    
        Starting with slow CART split implementation
    
            - given slow CART implementation, implement fast version        DONE
    
            TO DO: Benchmark different fast CART implementations, to find out which one actually is faster in practice, and how each of them actually scale in practice

            TO DO: Implement the even faster version which does not calculate the sum of squares at all, since this is not necessary (s. Slides Chapter 3 EBM slide no. 3)
    
            - demonstrate selection bias in CART
                2 (or more) features, all of them equally important / informative, one with more values is preferred
                fits well here, because needs several random reruns to get statistically significant results => requires fast implementation
                problem: we need to generate a well-suited dataset where this effect actually happens...
    
        Starting with implementation of full tree (i.e. including node classes or structure in an array)
    
            - implement growing full tree with CART
                Start with some data set, recursively call best_split on current node, and then sort indices into two parts to get two subsets
                Better: Sort indices by each feature once, and keep all these sortings, and then for each node save only the ranges w.r.t. each feature  (memory-efficient)
    
            - compute leaf type / node type for given node in a tree
                -> this serves as preperation for RPFs and for intro of chapter 5 func. decompositions
    
            More ideas:
    
            1.
            given two datasets with some function in DGP (i.e. same function generating target), but one with, one without dependencies  ->  run CART & show that 1 tree learns interactions, the other does not
            "dependencies can lead to interactions"
    
            2.
            - implement growing tree with restricted leaf type  (like in RPFs)
    
            3.
            interpretability-performance-tradeoff   both w.r.t. depth and w.r.t. leaf type  (two dimensions of complexity / of performance)
            NB: needs to be test-run to check whether desired result actually produced
            Problem: needs a suitable dataset actually showing desired result (e.g. containing interactions)
                Use a real-world dataset?
    
    
    
    Idea of organization: 3 files
    
        - functions
            - splitting     DONE
            - growing tree
                - also with restricted interactions order / interactions type
            - node class ??? -> No, better directly in array
            - determine leaf type
    
        - test at beginning:
            different CART implementations     DONE
            testing implementation of tree structure
    
        - test for all to-be-programmed new functionalities:
            determine leaf type
            demonstrate selection bias
            grow full tree with CART
            grow full tree with CART and restricted interaction type














