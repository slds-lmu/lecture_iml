\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\newcommand{\pathiml}{../../slides/06_local-explanations}
\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\newcommand{\gh}{\hat{g}}
\newcommand{\pih}{\fh}

\newcommand{\mysectionslide}{
\begin{frame}[plain]
    %\global\advance\c@lecture by -1
    \vspace*{0.5 cm}
    \LARGE\bfseries\inserttitle
    \vspace*{0.5 cm}

    \ifx\lecturesection\@empty\relax\else%
    {\lecturesection}%
    \fi%

\ifcsname learninggoals\endcsname
  {\vspace*{0.5 cm}
  \begin{minipage}{0.4\textwidth}
  \ifcsname titlefigure\endcsname
    {\begin{center}
    \begin{figure}[!b]
    \includegraphics[width=0.9\textwidth, keepaspectratio]{\titlefigure}
     \ifcsname titlecaption\endcsname
     \caption*{\titlecaption}
     \fi
    \end{figure}
    \end{center}}
  \else
    $\;$
  \fi
  \end{minipage}
  \begin{minipage}{0.55\textwidth}
  \normalsize
  Learning goals
   \normalfont
   \footnotesize
  \begin{itemize}
  \learninggoals
  \end{itemize}
  \end{minipage}}
\fi

  \end{frame}
}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments
 \newcommand{\titlefigure}{\pathiml/figure/lime.png}
\newcommand{\learninggoals}{
\item Understand motivation for local explanations 
\item Develop an intuition for possible use-cases
\item Know characteristics of local explanation methods}

\lecturechapter{Local Explanations}
\lecture{Interpretable Machine Learning}

% ------------------------------------------------------------------------------

\begin{frame}[c]{Methodological Motivation}

% Marius: We cannot write such stuff if we want to stay flexible how to combine chunks
%Except for Shapley values and ICE curves, we focused so far mostly on global explanation methods that explain global model behavior (e.g. PDP, PFI, etc.). However, there are also many methods that explain the \textbf{local} behavior of a model.

	\begin{itemize}
	    \item Purpose of local explanations:
	    \begin{itemize}
	        \item Insight into the driving factors for a \textbf{particular decision}
	        \item Understand the ML model's decisions in a \textbf{local neighborhood} of a given input\\ (e.g., feature vector)
	    \end{itemize}
	    \medskip
	    \pause
		\item Local Methods can address questions such as: 
		\begin{itemize}
		    \item \textbf{Why} did the model decide to predict $\yh$ for input $\xv$?
		    \item \textbf{How} does the model decide for observations that are similar to $\xv$?
		    \item \textbf{What} would the ML model have decided if $\xv$ its values in $\Xspace$ were different?
		    \item  \textbf{Where} (in which regions in $\Xspace$) does the model fail?
		\end{itemize}  
	\end{itemize}
\end{frame}

\begin{frame}[c]{Social Motivation}

%All these questions can indeed be relevant for the ML modeler. However, unlike in global methods that require expert ML understanding or even domain knowledge, many local methods aim to provide explanations also for laypersons. 
	\begin{itemize}
		\item Explanations for laypersons must be tailored to the \textbf{explainee} (who receives the explanation)\\ %(i.e., the person receiving the explanation)
		%\pause
		$\leadsto$ \textbf{case specific}, \textbf{easy} for humans to understand, and \textbf{faithful} to the explained mechanism
		\pause
		\item If algorithms make decisions in \textbf{socially/safety critical domains}, end users have a justified interest in receiving explanations
		\pause
		\item Local explanations cannot only increase \textbf{user trust}, but also help to detect \textbf{critical local biases} in algorithmic decision making
		\pause
		\item European citizens have the legally binding \textbf{right to explanation} as given in the General Data Protection Regulation (GDPR)
		\begin{itemize}
		    \item[$\leadsto$] Instead of explaining the entire (complex) model (with potential market secrets), explanations in a case-by-case usage is more reasonable
		\end{itemize}

	\end{itemize}
\end{frame}


\begin{frame}[c]{GDPR: The Right to Explanation}

    ``The data subject should have the right not to be subject to a decision, which may include a measure, evaluating personal aspects relating to him or her which is based solely on automated processing and which produces legal effects concerning him or her or similarly significantly affects him or her, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention.

$[\ldots]$

In any case, such processing should be subject to suitable safeguards, which should include specific information to the data subject and the \textbf{right} to obtain human intervention, to express his or her point of view, \textbf{to obtain an explanation of the decision reached after such assessment and to challenge the decision}.
'' \\[0.2cm] (\href{https://gdpr-text.com/read/recital-71/}{Recital 71, GDPR})
\end{frame}


\begin{frame}[c]{Example: Husky or Wolf?}
	\begin{itemize}
		\item We trained a model to predict if an image shows a wolf or a husky 
		\item Below the predictions on six test images are given 
		\item Do you trust our predictor? 
	\end{itemize}
	
	\begin{columns}
	
	\begin{column}{0.6\textwidth}
	    
    	\begin{center}
    		\includegraphics[width=0.58\textwidth]{\pathiml/figure/lime-wolfhusky.png}\\
    		\includegraphics[width=0.58\textwidth]{\pathiml/figure/lime-wolfhusky2.png}\\
    		{\textbf{Source:} [\href{http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf}{Sameer Singh 2018}]}
    	\end{center}
	    
	\end{column}
	
	\begin{column}{0.4\textwidth}
	    
	\begin{itemize}
		\item Sometimes the ML model is wrong
		\item Can you guess the pattern the ML model learned to identify a wolf?
	\end{itemize}
	    
	\end{column}
	    
	\end{columns}

\end{frame}


\begin{frame}[c]{Example: Husky or Wolf? Using LIME}

	\begin{columns}
	
	\begin{column}{0.7\textwidth}
	    
	\begin{center}
		\includegraphics[width=0.9\textwidth]{\pathiml/figure/lime-wolfhusky3.png}\\
		{\textbf{Source:} [\href{http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf}{Sameer Singh 2018}]}
	\end{center}
	    
	\end{column}
	
	\begin{column}{0.3\textwidth}
	    
	\begin{itemize}
		\item Local explanations highlight the parts of an image which led to the prediction
		\item[$\leadsto$] our predictor is actually a snow detector 
	\end{itemize}
	    
	\end{column}
	    
	\end{columns}







\end{frame}


\begin{frame}{Example: Loan Application}

\begin{columns}[c]
    
    \begin{column}{0.45\textwidth}
        	\begin{center}
		\includegraphics[width=1\textwidth]{\pathiml/figure/IntroJudge.png}\\
		{\textbf{Source:} [\href{https://www.elte.hu/content/trendfordulo-az-mi-fejlesztesekben.t.19025}{https://www.elte.hu}]}
	\end{center}
	
	\end{column}
	
	\begin{column}{0.55\textwidth}
	
    	\begin{itemize}
    	    \item<1-> Imagine: You apply for a loan at an online bank and are immediately rejected without reasons
    	    %Imagine: You are applying at a bank's online portal for a loan and your application gets immediately rejected without reasons
    	    \item<2-> Bank could e.g. provide a counterfactual explanation using local explanation methods:
    	    \begin{itemize}
    	        \item[] ``If you were older than 21, your loan application would have been accepted."
    	    \end{itemize}
    	    \item<3->[$\leadsto$] helps to understand the decision and to take actions for recourse (if req.)
    	\end{itemize}
    	
    \end{column}
\end{columns}

\end{frame}

\begin{frame}[c]{Example: Stop or Right-of-Way?}

\begin{columns}[c]

	\begin{column}{0.55\textwidth}
	\begin{itemize}
	    \item Imagine: 
	        \begin{itemize}
	            \item You work at a car company that develops image classifiers for autonomous driving
	            \item You show your model the following image (an adversarial example)
	            \pause
	            \item Classifier is $99\%$ sure it describes a right-of-way sign
	        \end{itemize}  
	    \item Would you entrust other peoples lives into the hands of this software?
	\end{itemize}
	
	 \end{column}
	
	\begin{column}{0.45\textwidth}

	\begin{center}
		\includegraphics[width=0.7\textwidth]{\pathiml/figure/IntroStop.jpg}\\
		{\textbf{Source:} [\href{https://arxiv.org/abs/1707.08945}{Eykholt et. al 2018}]}
	\end{center}

     \end{column}

\end{columns}

\end{frame}

\begin{frame}[c]{Characteristics of Local Explanations}
	\begin{itemize}[<+->]
		\item \textbf{Explanation scope:} Specific prediction, local environment
		\item \textbf{Model classes:} Model-agnostic by definition, model-specific for computational reasons\\
		$\leadsto$ very popular also for deep learning models
		\item \textbf{Audience:} ML modelers and laypersons
		\item \textbf{Data types:} Often agnostic, including tabular, image, text and audio data
		\item \textbf{Methods:} Many, most prominent are counterfactual explanations, shapley values, local interpretable model-agnostic explanations (LIME), adversarial examples, single ICE curve%, anchors
		%\footnote{For Shapley values and ICE curves see other lectures}
		\item \textbf{Special:} Due to audience, strong interactions with social sciences and strong connections to cognitive science and neurosciences due to data types
	\end{itemize}
\end{frame}

% \begin{frame}{Recap: ICE Curve}
% 		\begin{itemize}
% 		\item Visualize how the model prediction $\fh(\xv)$ of a individual observation $\xv$ change by varying the feature values of one or two features while keeping all other features fixed. 
% 		\item Warning: Careful interpretation for highly correlated features or feature regions with a few observations. 
% 	\end{itemize}
% \vspace{0.5cm}
% \begin{columns}
% 	\begin{column}{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{\pathiml/figure/bike-sharing-dataset01.png}
% 	\end{column}
% 	\begin{column}{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{\pathiml/figure/bike-sharing-dataset02.png}
% 	\end{column}
% \end{columns}
% \end{frame}

% \begin{frame}{Recap: Shapley Values}
% 	\begin{itemize}
% 		\item Shapley values were original proposed in game theory.
% 		\item They are a local explanation method because they tell us how each feature $x_j$ contributes to the overall prediction $\fh(\xv)$ of a specific observation $\xv$. 
% 	\end{itemize}
% \vspace{0.5cm}
% \begin{center}
% 	\includegraphics[width=0.8\textwidth]{\pathiml/figure/shapley_valuefct}
% \end{center}
% \end{frame}

\begin{frame}{Credit Dataset}

	\begin{itemize}
		\item We illustrate local explanation methods on the German credit data 
		\citebutton{see Kaggle}{https://www.kaggle.com/uciml/german-credit}
		\item 522 complete observations, 9 features containing credit and customer information
		\item Binary target ``risk'' indicates if a customer has a `good' or `bad' credit risk
		\item We merged categories with few observations 
	\end{itemize}
		\begin{center}
			\footnotesize
			\begin{tabular}{ccc}
				\toprule
				name & type & range\\
				\midrule
				age & numeric & [19, 75]\\
				sex & factor & \{male, female\}\\
				job & factor & \{0, 1, 2, 3\}\\
				housing & factor & \{free, own, rent\}\\
				saving.accounts & factor & \{little, moderate, rich\}\\
				checking.accounts & factor & \{little, moderate, rich\}\\
				credit.amount & numeric & [276, 18424]\\
				duration & numeric &  [6, 72]\\
				purpose & numeric &  \{others, car, furniture, radio/TV\}\\
				risk & factor & \{good, bad\}\\
				\bottomrule
			\end{tabular}
		\end{center}
\end{frame}

\endlecture

 \renewcommand{\titlefigure}{\pathiml/figure/AEturtle.jpg} 
\renewcommand{\learninggoals}{
\item Understand the definition of ADEs 
\item Understand first methods that generate ADEs
\item Discuss potential causes of ADEs and standard defenses against them}

\lecturechapter{\Large{Local Explanations: Adversarial Examples}}
%\lecture{Interpretable Machine Learning}
\mysectionslide

% ------------------------------------------------------------------------------

\begin{frame}[c]{Adversarial Machine Learning}
\begin{itemize}
    \item What happens if a computer system gets an erroneous input?
    \item Even worse:\\ What happens if someone feeds in a malicious input on purpose to attack a system?
    \item[$\leadsto$] \textbf{Robustness} is important to ensure a safe service!
    \medskip
    \item \textbf{Adversarial ML} studies the robustness of machine learning (ML) algorithms to malicious input
    \item Two different kinds of attacks:
    \begin{itemize}
        \item \textbf{Evasion attacks} mislead an employed ML model with manipulated inputs (our focus)
        \item \textbf{Data Poisoning}: Malicious inputs to the training dataset
    \end{itemize}
    \end{itemize}
\end{frame}

\begin{vbframe}[c]{Adversarial Examples}
%The inputs by which evasion attacks can be conducted are called \textbf{Adversarial Examples (ADEs)}
\begin{itemize}
    \item \textbf{Informal Definition}: An ADE is an input to a model that is deliberately designed to "fool" the model into misclassifying it
    \item Even possible with low generalization error
    \item Both deep learning models (e.g., CNNs) and classical ML can be vulnerable to such attacks
    \item ADEs created from a real data observation $\xv$ can be indistinguishable from $\xv$ by a human observer 
    \item Since the model misclassifies this input, it does not seem to have a real understanding of the underlying concepts of the provided inputs
\end{itemize}
\end{vbframe}

\begin{vbframe}{Examples: Model-Attacks \citebutton{Gong \& Poellabauer 2018}{https://arxiv.org/pdf/1803.09156.pdf}}

\begin{columns}

\begin{column}{0.6\textwidth}

\begin{figure}[h]
\centering
  \includegraphics[width=1\linewidth]{\pathiml/figure/AEduckSound.png}
  \label{fig:mnist}
\end{figure} 

\end{column}

\begin{column}{0.4\textwidth}

    \vspace{3em}
    \begin{itemize}
        \item Is this a duck or a horse?
        \item Small (hard-to-see) noise can change the prediction
    \end{itemize}

\end{column}

\end{columns}

\end{vbframe}

\begin{vbframe}[c]{Examples: Image Data \citebutton{Eykholt et al. (2018)}{https://arxiv.org/pdf/1807.07769.pdf}  \citebutton{Athalye et al. (2018)}{https://arxiv.org/pdf/1707.07397.pdf}}
\begin{figure}[h]
\centering
\includegraphics[width=0.46\linewidth]{\pathiml/figure/AEstop.png}\quad \includegraphics[width=0.45\linewidth]{\pathiml/figure/AEturtle.jpg}
  \label{fig:mnist}
\end{figure} 

\begin{columns}

\begin{column}{0.5\textwidth}

\begin{itemize}
    \item Stop signs can be missclassified\\ e.g., because of graffiti
    \item With some well-placed patches, the model identifies it as a ``right of way'' sign
\end{itemize}

\end{column}

\begin{column}{0.5\textwidth}

\begin{itemize}
    \item 3D-print of a turtle
    \item Misclassified as a rifle (from every angle)
    \item Video: 
    \citebutton{MITCSAIL
    (2017)}{https://www.youtube.com/watch?v=piYnd_wYlT8}
\end{itemize}

\end{column}

\end{columns}


\end{vbframe}

\begin{vbframe}{Example: Tabular Data \citebutton{Ballet (2019)}{https://arxiv.org/pdf/1911.03274.pdf}}
What is imperceptibility on tabular data?
\begin{itemize}
    \item Idea: experts focus on the most important features in their judgment
    \item An ADE arises from manipulating features the model deems important but experts do not
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{\pathiml/figure/AEloanApplication.png}\\
   \centering
  {Decision boundary of a classifier deciding loan applications. ADE via ``number of pets''}
  \label{fig:mnist}
\end{figure} 

\end{vbframe}

\begin{vbframe}[c]{ADE and Interpretability}

\begin{enumerate}
    \item ADEs show where models fail $\leadsto$ improved model understanding
    \item Because of ADEs, we need more interpretability
    \item Interpretation can lead to robustness against ADEs
    \medskip
    \item Explanations can be used to construct ADEs (e.g., see numer of pets on previous slide)
\end{enumerate}

\end{vbframe}

\begin{vbframe}[c]{Formal Definition}
\begin{block}{Adversarial Input}
Let $\epsilon>0$, $f:\Xspace \rightarrow \Yspace$ be an ML model and $\xv \in \Xspace$ be a real data point that is correctly classified: $f(\xv)=y_{\xv,true}$. \\\medskip
 We call $\mathbf{a}_{\xv}$ an \textbf{adversarial input} to $\xv$ if:
\begin{equation*}
    \| \mathbf{a}_{\xv}- \xv\|<\epsilon\text{ and } f(\mathbf{a}_{\xv})\neq y_{\mathbf{a}_{\xv},true}=f(\xv)
\end{equation*}
\end{block}
\begin{itemize}
    \item $\mathbf{a}_{\xv}$ an is a data point close to a real, correctly classified input that is misclassified
    \item $\mathbf{a}_{\xv}$ is called \textbf{targeted} if the class it is assigned to is determined\\
    $f(\mathbf{a}_{\xv})=y'$ with $y'$ being a desired prediction
    \item Can be generalized to regression problems
\end{itemize}
\end{vbframe}


% \begin{vbframe}{ADE and Counterfactual Explanations}
% It seems as if ADEs and counterfactual explanations (CEs) are defined similarly. Both ADEs and CEs describe inputs close to a given input $\xv$ that gets a different assignment. What are their differences?
% \begin{itemize}
%     \item Counterfactuals do not have to be misclassified.
% %    \item Counterfactuals should be maximally close to $\xv$.
%     \item Different notions of distance $\|\cdot\|$ are applied, e.g., $p_{2,\infty}$-norm for ADEs or $p_{0,1}$-norm for CEs.
%     \item Informal difference I: ADEs are mostly considered for high-dimensional data, while CEs are mostly considered in the context of low-dimensional data.
%     \item Informal difference II: ADEs hide changes while CEs highlight them.
%     \item \textbf{Shared example:} ``If you had two more pets, your loan application would have been granted" is an example of both ADEs and CEs.
% \end{itemize}
% \end{vbframe}


\begin{frame}{Why Do ADE Exist?}
Non-exhaustive list of hypotheses: \\[0.5cm]
    
\only<1>{
    \textbf{1. Low-probability spaces hypotheses:} ADEs live in low-probability yet dense spaces in the data manifold that are not well represented in the training samples \citebutton{Szegedy et al. (2013)}{https://arxiv.org/abs/1312.6199}
        
    \begin{figure}
        \centering
        \includegraphics[width = .5\textwidth]{\pathiml/figure/adversarial examples.png}
        \caption{Binary classification example (dark blue vs. green dots). Dotted line represents the true decision boundary, bold line the trained one. Low probability space close to decision boundary allow for adversarial examples (turquoise dot).}
    \end{figure}
}
    
\only<2>{
    \textbf{2. Linearity hypotheses (most popular):}\\ Adversarial examples are omnipresent in the data manifold\\
    $\leadsto$ occur, because commonly used models often show linear behavior\\
    $\leadsto$ small changes of $\epsilon$ in every feature cause a change of $\epsilon\|\thetab\|_1$ in prediction \citebutton{Goodfellow et al. (2014)}{https://arxiv.org/abs/1412.6572}

    \vspace{.5cm}
    
    \textbf{Example:} linear model
    
    Original: $f(\xv) = \xv^T \thetab$ 
    
    Small changes: $f(\xv + \epsilon) = (\xv + \epsilon)^T \thetab$
    
    Difference: $f(\xv + \epsilon) - f(\xv) = \epsilon \cdot \thetab$
}
    
\only<3>{
    \textbf{3. The boundary tilting hypothesis:} Linearity is neither necessary nor sufficient to explain ADEs\\
    $\leadsto$ ADEs mostly result from overfitting the sampled manifold \citebutton{Tanay and Griffin (2016)}{https://arxiv.org/abs/1608.07690}
    
    \begin{figure}
        \centering
        \includegraphics[width = .35\textwidth]{\pathiml/figure/tilting_ae.png}
        \caption{Linear binary classification example. Due to overfitting the decision boundary (red) is close to the manifold of the training data. Techniques like regularization could help to make the decision boundary more robust (green). \citebutton{Kim et al. (2019)}{https://arxiv.org/abs/1903.11626}
}
    \end{figure}
}
    
\only<4>{
    \textbf{4. Human-centric hypotheses:} ML models make use of predictive but non-robust features -- meaning they are highly correlated with the prediction target, but not used by humans \citebutton{Ilyas et al. (2019)}{https://papers.nips.cc/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html}
}
    
\end{frame}


\begin{vbframe}[c]{Ways to Generate ADE}
Different ways for constructing ADEs:
There exist various ways in the literature to generate ADEs for a given model in feasible time
\begin{itemize}
    \item Formulate the search for ADEs as an \textbf{optimization problem}, e.g. 
    \begin{equation*}
        \label{eq:optimization}
        \underset{\xv'\in \Xspace}{\text{argmin}}\; \underbrace{\| \xv-\xv' \|_{\Xspace}}_{\text{minmize}} - \lambda\;   \underbrace{ \|f(\xv')-y'\|_{\Yspace}}_{\text{maximize}}
    \end{equation*}
    \item Use \textbf{sensitivity analysis} to identify features that influence the target class
    \item Train a generative adversarial network (GAN) \citebutton{Goodfellow et al. (2014)}{https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf}
\end{itemize}
Moreover, depending on the attacker's model access, we can distinguish between
\begin{itemize}
    \item \textbf{Full-access attacks}: the attacker has full access to the internals of the model
    \item \textbf{Black-box attacks}: the attacker can only query the model on some inputs and receives the model's outputs
\end{itemize}
\end{vbframe}

\begin{vbframe}{Fast-Gradient-Sign-Method (FGSM)
\citebutton{Goodfellow et al. (2015)}{https://arxiv.org/pdf/1412.6572.pdf}}
% Since we have seen optimization methods for CEs, we focus on a method using sensitivity analysis, namely FGSM.
% Source of (modified) figure: https://docs.google.com/presentation/d/1l-ng618qrisSorutii9I30qL8VhZGYD9aRBuwvvBbSY/edit?usp=sharing
\begin{itemize}
    \item FGSM is based on the linearity hypothesis
    \item FGSM finds ADEs from:
    \begin{equation*}
        a_{\xv}=\xv+\epsilon\cdot\text{sign}(\nabla_{\xv} J(\theta,\xv,y_{\xv,true}))
    \end{equation*}
    where $\text{sign}(\nabla_{\xv} J(\theta,x,y_{\xv,true}))$ describes the component-wise signum of the gradient of cost function $J$ in $\xv$ with true label $y_{\xv,true}$
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{\pathiml/figure/AEpanda.png}
  \label{fig:mnist}
\end{figure} 

\end{vbframe}

\begin{vbframe}[c]{Notes on FGSM \citebutton{Goodfellow et al. (2015)}{https://arxiv.org/pdf/1412.6572.pdf} \citebutton{Lyu et al. 2015}{https://arxiv.org/abs/1511.06385}}
\begin{itemize}
    \item FGSM works particularly well for linear(-like) models in high-dimensional spaces,\\ e.g., LSTMs, logistic regressions or CNNs with ReLU activations
    \item Not every $\mathbf{a}_{\xv}$ generated by FGSM is an ADE, especially if $\epsilon$ is too small
    \item FGSM attacks can be also generated without model access by approximating the gradient,\\ e.g. with finite difference methods
    \item The notion of similarity in FGSM is based on $\|\cdot\|_{\infty}$ $\leadsto$ there are generalizations of FGSM to other norms
\end{itemize}
\end{vbframe}


\begin{vbframe}[c]{Black-Box Attacks with Surrogates \citebutton{Papernot et al. (2016)}{https://arxiv.org/abs/1605.07277}}

\begin{itemize}
    \item So far, we assumed full access to the predictive model
    \item Black-box attacks only assume query-access
    \item Large risk of attacks since often one can query predictive models many times
\end{itemize}

\medskip
\begin{enumerate}
    \item Query the model you aim to attack as often as allowed on data similar to the training data
    \item Use the labeled data you received to train a surrogate model
    \item Generate ADEs for the surrogate model
    \item Use these ADEs to attack the original model
\end{enumerate}

\medskip
\begin{itemize}
    \item[$\leadsto$] Known as the \textbf{transferability} of ADEs.
\end{itemize}



\end{vbframe}

\begin{vbframe}[c]{Defenses Against ADE}
There are several ways to protect your network against such attacks -- we distinguish between two broad types of defenses, differing in the position in which they act
\begin{itemize}
    \item \textbf{Guards} act on the inputs a model receives
    \begin{itemize}
        \item \textbf{Detect anomalies:} e.g., statistical testing, or discriminator networks from GANs
        \item \textbf{Conduct transformations} on inputs (e.g. PCA)
    \end{itemize}
    \item \textbf{Defense by design} act on the model itself
    \begin{itemize}
        \item \textbf{Adversarial training}: train model on adversarials
        \item \textbf{Architectural defenses}: e.g., removing low predictive features from the model
    \end{itemize}
\end{itemize}
\end{vbframe}

%\begin{vbframe}{Regularization Against ADEs}
%The use of regularization techniques against ADEs is motivated by most hypotheses that explain ADEs. As an example we look at a technique based on the FGSM and the linearity hypotheses.
%\begin{itemize}
%    \item Goodfellow et al. suggest to specfiy the cost function s.t.
%    \begin{equation*}
%        \tilde{J}(\theta,\xv,y):=\alpha J(\theta,\xv,y)+(1-\alpha) J(\theta,\xv+\epsilon\cdot\text{sign}(\nabla_{\xv} J(\theta,\xv,y)),y)
%    \end{equation*}
%    with $\alpha=0.5$.
%    \item This increased the model robustness against FGSM generated ADEs.
%    \item However, model are not only still prone to other ADEs but also to many FGSM-ADEs.
%    \item Applying such regularization terms can increase test error.
%\end{itemize}
%\end{vbframe}

\begin{vbframe}[c]{Summary}
\begin{itemize}
    \item ADEs are not explanations themselves but are conceptually connected to them
    \item ADEs can be generated in diverse settings $\leadsto$ crucial modeling decisions are the distance measure, the local environment, and the target level (model or process)
    \item There are various hypotheses on the existence of ADEs which also motivate different defense strategies
\end{itemize}
\begin{center}
    \includegraphics[width = .6\textwidth]{\pathiml/figure/AEpanda_simple.png}
    
    {\citebutton{Goodfellow et al. (2017)}{https://openai.com/blog/adversarial-example-research/}}
\end{center}
\end{vbframe}

%\begin{vbframe}{Outlook}
%\begin{itemize}
%    \item Even in highly non-linear models ADEs occur. As long as their existence is not well-understood, defenses against them will have limited power.
%    \item More and more different distance measures are considered, e.g. $p_0$ for one-pixel attacks, the Wasserstein-metric or psychologically motivated measures like the Perceptual Adversarial Similarity Score (PASS).
%    \item Recent work considered ADEs that fool both, humans and ML models. ADEs may be a case where research on human and machine perception can profit from each other.
%\end{itemize}
%@misc{rozsa2016adversarial,
%      title={Adversarial Diversity and Hard Positive Generation}, 
%      author={Andras Rozsa and Ethan M. Rudd and Terrance E. Boult},
%      year={2016},
%      eprint={1605.01775},
%      archivePrefix={arXiv},
%      primaryClass={cs.CV}
%}
%\end{vbframe}

% \begin{vbframe}{Central References}
% \begin{itemize}
%     \item Serban, A., Poll, E., \& Visser, J. (2020). Adversarial examples on object recognition: A comprehensive survey. ACM Computing Surveys (CSUR), 53(3), 1-38.
%     \item Goodfellow, I. J., Shlens, J., \& Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.
%     \item Yuan, X., He, P., Zhu, Q., \& Li, X. (2019). Adversarial examples: Attacks and defenses for deep learning. IEEE transactions on neural networks and learning systems, 30(9), 2805-2824.
% \end{itemize}
% \end{vbframe}

\endlecture

\renewcommand{\titlefigure}{\pathiml/figure/lime5}
\renewcommand{\learninggoals}{
	\item Understand motivation for LIME
	\item Develop a mathematical intuition}

\lecturechapter{LIME}
%\lecture{Interpretable Machine Learning}
\mysectionslide

% Prerequisite: le-intro

% ------------------------------------------------------------------------------
\begin{frame}[c]{LIME}
\begin{itemize}
		\item Local Interpretable Model-agnostic Explanations (LIME) assume that even if a ML model is very complex, the local prediction can be described with a simpler model
		\smallskip\pause
		\item  LIME explains \textbf{individual} predictions of \textbf{any} black-box model by approximating the model \textbf{locally} with an interpretable model
		\smallskip\pause
		\item Called local surrogate models $\leadsto$ often inherently interpretable models such as linear models or classification/regression trees are chosen\\
		\smallskip\pause
		\item LIME should answer why a ML model predicted $\hat y$ for input $\xv$
		\smallskip\pause
		\item LIME is model-agnostic and can handle tabular, image and text data 
\end{itemize}
\end{frame}


\begin{frame}[c]{LIME: Characteristics}

    \textbf{Definition:}\\
	LIME provides a local explanation for a black-box model $\fh$ in form of a model $\gh \in \Gspace$ with $\Gspace$ as the class of potential (interpretable) models\\[2em]
	
	
	Model $g$ should have two characteristics:
	\begin{enumerate}
		\item \textbf{Interpretable}: relation between the input variables and the response are easy to understand  
		\item \textbf{Locally faithful / Fidelity}: similar behavior as $\fh$ in the vicinity of the obs. being predicted
	\end{enumerate}
	
	\vspace{2em}
	Formally, we want to receive a model $\gh$ with \textbf{minimal complexity and maximal local-fidelity} 
\end{frame}


\begin{frame}{Model Complexity}
    
    We can measure the complexity of a model $\gh$ using a complexity measure $J(\gh)$ \lz

 	\textbf{Example: Linear model}\\
 	\begin{itemize}
 	    \item Let $\Gspace = \left\{g: \Xspace \to \R ~|~g(\xv) = s(\thetab^\top \xv)\right\}$ be the class of linear models
 	    \item $s(\cdot)$: identity function for linear regression or logistic sigmoid function for logistic regression
 	    \item[$\leadsto$] $J(g) = \sum_{j = 1}^p \Ind_{\{ \theta_j \neq 0 \}}$ could be the L$_0$ loss, i.e., the number of non-zero coefficients 
 	\end{itemize}
 	\lz\pause
 	
 	\textbf{Example: Tree}\\
 	\begin{itemize}
 	    \item Let $\Gspace = \left\{g:\Xspace \to \R ~|~g(\xv) = \sum_{m=1}^M c_m \Ind_{\{ \xv \in Q_m \}}\right\}$ be the class of trees\\
 	     i.e., the class of additive models (e.g., constant $c_m$)  over the leaf-rectangles $Q_m$
 	    \item[$\leadsto$] $J(g)$ could measure the number of terminal/leaf nodes
 	\end{itemize}
 	
\end{frame}
 
\begin{frame}{Local model fidelity}
 	\begin{itemize}
 		\item $g$ is locally faithful to $\fh$ w.r.t. $\xv$ 
 		if for $\zv \in \Zspace \subseteq \R^p$ close to $\xv$, predictions of $\gh(\zv)$ are close to $\fh(\zv)$ 
 		\item In an optimization task: the closer $\zv$ is to $\xv$, the closer $\gh(\zv)$ should be to $\fh(\zv)$  
 		\pause
 		\item Two required measures:
 		\begin{enumerate}
 			\item A proximity (similarity) measure $\neigh(\zv)$ between $\zv$ and $\xv$, e.g. the exponential kernel:
 			$$\neigh(\zv) = exp(-d(\xv, \zv)^2/\sigma^2)$$ 
 			with $\sigma$ as the kernel width and $d$ as the Euclidean distance (numeric features) or the Gower distance (mixed features) 
 			\pause
 			\item A distance measure or loss function $L(\fh(\zv), \gh(\zv))$, e.g. the L$_2$ loss/squared error
 			$$L(\fh(\zv), \gh(\zv)) = (\gh(\zv) - \fh(\zv))^2$$ 
 		\end{enumerate}
 		\pause
 		\item Given points $\zv$, we can measure local fidelity of $g$ with respect to $\fh$ in terms of a weighted loss
 		$$ L(\fh, g, \neigh) = \sum_{\zv \in \Zspace} \neigh(\zv) L(\fh(\zv), \gh(\zv)) $$
 		%\item Note that identifying \textbf{locally} faithful explanations that are interpretable is less of a challenge than identifying \textbf{globally} faithful explanations. Yet, global fidelity implies local fidelity but not vice versa.
 	\end{itemize}
\end{frame}

\begin{frame}[c]{Minimization task}
	\begin{itemize}
		\item Optimization objective of LIME: 
		$$ \argmin_{g \in \Gspace} L(\fh, \gh, \neigh) + J(g)$$
		\item In practice:
		\begin{itemize}
		    \item LIME only optimizes $L(\fh, \gh, \neigh)$ (model-fidelity) 	
		    \item Users decide threshold on model complexity $J(g)$ beforehand
		\end{itemize}
		\item Goal: \textbf{model-agnostic} explainer
		\begin{itemize}
    		\item[$\leadsto$] optimize $L(\fh, \gh, \neigh)$ without making any assumptions about $\fh$ 
    		\item[$\leadsto$] learn $\gh$ only approximately  
		\end{itemize}
		\end{itemize}
\end{frame} 

\begin{frame}[c]{LIME Algorithm: Outline}
		
		\textbf{Input}:
		\begin{itemize}
		    \item Pre-trained model $\fh$
		    \item Observation $\xv$ whose prediction $\fh(\xv)$ we want to explain
		    \item Model class $\Gspace$ for local surrogate (to limit the complexity of the explanation)
		\end{itemize}
		
		\pause
		\medskip
		
		\textbf{Algorithm}:
		\begin{enumerate}
    		\item Independently sample new points $\zv \in \Zspace$ 
    		\item Retrieve predictions $\fh(\zv)$ for obtained points $\zv$ 
    		\item Weight $\zv \in \Zspace$ by their proximity $\neigh(\zv)$
    		\item Train an interpretable surrogate model $g$ on weighted data points $\zv \in \Zspace$\\ $\leadsto$ predictions $\fh(\zv)$ are the target of this model
    		\item Return the interpretable model $\gh$ as the explainer
		\end{enumerate}
		

	
\end{frame} 

\begin{frame}[c]{LIME Algorithm: Example}

    	\textbf{Illustration} of LIME based on a classification task:
		\begin{itemize}
			\item Light/dark gray background: prediction surface of a classifier
			\item Yellow point: $\xv$ to be explained
			\item $\Gspace$: class of logistic regression models 
		\end{itemize}
		\begin{center}
			\includegraphics[width=0.35\textwidth]{\pathiml/figure/lime2}
		\end{center}

\end{frame} 


\begin{frame}[c]{LIME Algorithm: Example (Step 1+2: Sampling) \citebutton{Ribeiro. 2016}{https://github.com/marcotcr/lime}}
		
		Strategies for sampling: 
		\begin{itemize} 
			\item Uniformly sample new points from the feasible feature range 
			\item Use the training data set with or without perturbations
			\item Draw samples from the estimated univariate distribution of each feature
			\item Create an equidistant grid over the supported feature range  
		\end{itemize}
		\vspace{-.5cm}
		\begin{columns}
        \begin{column}{0.5\textwidth}  %%<--- here
            \begin{figure}
             \includegraphics[width=.8\textwidth]{\pathiml/figure/lime3} 
             \vspace{-0.3cm}
             \caption{Uniformly sampled}
             \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}  %%<--- here
		    \begin{figure}
			\includegraphics[width=.8\textwidth]{\pathiml/figure/lime3a}
			  \vspace{-0.3cm}
    		    \caption{Equidistant grid}
    		\end{figure}   
    \end{column}
\end{columns}
\end{frame}
		
\begin{frame}{LIME Algorithm: Example (Step 3: Proximity) \citebutton{Ribeiro. 2016}{https://github.com/marcotcr/lime}}

	In this example, we use the exponential kernel defined on the Euclidean distance $d$
		 $$\neigh(\zv) = exp(-d(\xv, \zv)^2/\sigma^2).$$ 
		\begin{center}
			\includegraphics[width=0.4\textwidth]{\pathiml/figure/lime4}
		\end{center}
		
% MARIUS: Not relevant for the example; if we want to introduce it, we should do it somewhere else
% 	An alternative is the Gower proximity: 
% 	$\neigh(\zv) = 1 - \Gower(\zv, \xv) =  1 - \frac{1}{p}\sum_{j = 1}^{p} \delta_G(z_j, x_j) $ 
% 	$\textnormal{ with } \delta_G(z_j, x_j) = 
% 	\begin{cases}
% 	\frac{1}{\widehat{R}_j}|z_j- x_j| & \text{if $x_j$ and $z_j$ are numerical} \\
% 	\mathbb{I}_{z_j \neq x_j} & \text{if $x_j$ and $z_j$ are categorical}
% 	\end{cases}.$
		
\end{frame}
		
\begin{frame}[c]{LIME Algorithm: Example (Step 4: Surrogate) \citebutton{Ribeiro. 2016}{https://github.com/marcotcr/lime}}
		In our example, we fit a \textbf{logistic regression} model (consequently, $L(\fh(\zv), \gh(\zv))$ is the Bernoulli loss)
		\begin{center}
			\includegraphics[width=0.4\textwidth]{\pathiml/figure/lime5}
		\end{center}
\end{frame}

\endlecture

\renewcommand{\titlefigure}{\pathiml/figure/lime_credit_ice2.pdf}
\renewcommand{\learninggoals}{
    \item See real-world data examples
    \item See application to image and text data}

\lecturechapter{LIME Examples}
%\lecture{Interpretable Machine Learning}
\mysectionslide

% Prerequisite: le-intro

% ------------------------------------------------------------------------------

\begin{frame}[c]{Example on Credit Dataset (Tabular)}
	\begin{itemize}
		\item Model: SVM with RBF kernel
		\item $\xv$: first data point of the dataset with $\fh_{bad}(\xv) = 0.658$
		\item $\zv$: training data $\leadsto$ weighted by the Gower proximity 
		\item Surrogate model $\gh$: L$_1$-regularized linear model with 5 features 
	\end{itemize}

    \bigskip

	\begin{table}[ht]
		\centering
		\scriptsize
		\begin{tabular}{rlrlllrrl}
			\hline
			age & sex & job & housing & saving & checking & credit.amount & duration & purpose \\ 
			\hline
			 22 & female &   2 & own & little & moderate & 5951 &  48 & radio/TV \\ 
			\hline
		\end{tabular}
	\end{table}

\end{frame}

\begin{frame}[c]{Example on Credit Dataset (cont'd)}


\begin{center}
	\includegraphics[width=0.45\textwidth]{\pathiml/figure/lime_credit.pdf}\\
	{Effects of surrogate model, i.e. $\thetah^T \xv$}
\end{center}

\begin{itemize}
	\item The local model prediction for $\xv$ is $\gh(\xv) = 0.64$ vs. $\fh(\xv) = 0.658$ 
	\item $\gh$ has a local fidelity of $L(\pih, \gh, \neigh) = 4.82$ with $\neigh(\zv)$ as the Gower proximity and $L(\pih_{bad}(\zv), g(\zv))$ as the euclidean distance 
\end{itemize}

\end{frame}
	
\begin{frame}[c]{Example on Credit Dataset (cont'd)}

\begin{itemize}	
	\item 2-dim ICE plots (aka. prediction surface plot) of credit amount and duration show how the surrogate model $g$ linearly approximates the previously nonlinear prediction surface of $\pih_{bad}$ 
\end{itemize}
\vspace{-0.4cm}
 \begin{columns}
	\begin{column}{0.47\textwidth}
		\begin{center}
		\includegraphics[width=1\textwidth]{\pathiml/figure/lime_credit_ice1.pdf}
		\end{center}		
	\end{column}
	\begin{column}{0.46\textwidth}  
		\begin{center}
				\includegraphics[width=1\textwidth]{\pathiml/figure/lime_credit_ice2.pdf}
		\end{center}
			
	\end{column}
\end{columns}
\vspace{-0.4cm}
\begin{center}
		{2-dim ICE plot of $\fh_{bad}$ (\textbf{left}) and surrogate $g$ (\textbf{right}) for features duration and credit amount. \\The white dot is $\xv$. The histograms display the marginal distribution of the training data $\Xmat$.}
\end{center}

\end{frame}

%\begin{frame}[containsverbatim,allowframebreaks]{Bike Sharing Dataset}
%\vspace{-.3cm}
%
%\begin{center}
%\includegraphics[width=0.7\textwidth]{\pathiml/figure/bike-figure.png}
%\end{center} 
%
%\footnotesize \textbf{Figure:} LIME for two example instances of the bike sharing dataset.
%
%\normalsize
%\vspace{0.2cm}
%The plots show the feature effect of the sparse linear model, i.e. the model coefficients times the feature value of the instance.
%Warmer temperature has a positive effect on the prediction, 
%while the year 2011 has a large negative effect as well as the springtime.
%\end{frame}

\begin{frame}{LIME for Text Data \citebutton{Shen, Ian, (2019)}{https://medium.com/just-another-data-scientist/explain-sentiment-prediction-with-lime-f90ae83da2da}}
    LIME can also be applied to text data: 
	\begin{itemize}
		\item Raw text representations: 
		\begin{itemize}
		    \item Binary vector indicating the presence or absence of a word 
		    \item A vector of word counts
		\end{itemize}
		\item Examples for \textit{``This text is the first text."} and \textit{``Finally, this is the last one."}:
		\begin{center}
			\begin{tabular}{c|c|c|c|c|c|c|c} 
				this & text & is & the & first & finally & last & one \\ 
				\hline
				1 & 2 & 1 & 1 & 1 & 0 & 0 & 0 \\
				1 & 0 & 1 & 1 & 0 & 1 & 1 & 1 \\
			\end{tabular}
		\end{center} 
		\item \textbf{Sampling}: Randomly set the entry of individual words to $0$; equal to removing all occurrences of this word in the text. 
		\item \textbf{Proximity}: Exponential kernel with cosine distance. 
		\begin{itemize}
		    \item Neglects words that do not occur in both texts 
		    \item Measures the distance irrespective of the text size
		\end{itemize}
	\end{itemize}
\end{frame}
	
\begin{frame}{LIME for Text Data (cont'd) \citebutton{Shen, Ian, (2019)}{https://medium.com/just-another-data-scientist/explain-sentiment-prediction-with-lime-f90ae83da2da}}	 
	\begin{itemize}
		\item Random forest classifier labeling movie reviews from IMDB 
		\begin{itemize}
		    \item \textcolor{blue}{0}: negative
		    \item \textcolor{orange}{1}: positive
		\end{itemize}
		\item Surrogate model is a sparse linear model 
	\end{itemize}
	
	\begin{figure}
		\begin{center}
			%\captionsetup{font = scriptsize, labelfont = {bf, scriptsize}}
			\includegraphics[width=0.9\textwidth]{\pathiml/figure/lime_movier}
		\end{center}
	\end{figure}
	
	{Words like ``worst`` or ``waste`` indicate negative review while words like ``best`` or ``great`` indicate positive review}
	
	\end{frame}
	
\begin{frame}[c]{LIME for image data}
	\begin{columns}
		\begin{column}{0.67\textwidth}
			LIME also works for image data:  
			\begin{itemize}
				\item \textbf{Idea}: Each obs. is represented by a binary vector indicating the presence or absence of superpixels \citebutton{Achanta et al. 2012}{https://ieeexplore.ieee.org/document/6205760}
				\item Superpixels are interconnected pixels with similar colors (absence of a single pixel might not have a (strong) effect on the prediction)
				\item \textbf{Warning}: Size of superpixels needs to be determined before the segmentation takes place
				\item \textbf{Sampling}: Randomly switching some of the super pixels ``off", i.e., by coloring some superpixels uniformly
			\end{itemize}		
		\end{column}
		\begin{column}{0.26\textwidth}  
			\begin{center}
				\includegraphics[width=1\textwidth]{\pathiml/figure/superpixel_woman}
				{Example for superpixels of different sizes}
			\end{center}
		\end{column}
	\end{columns}
    
\end{frame}
\begin{frame}{LIME for image data (cont'd) \citebutton{Ribeiro. 2016}{https://github.com/marcotcr/lime}}
	\begin{itemize}
		\item Explaining prediction of pre-trained inception neural network classifier
		\item \textbf{Sampling}: Graying out all superpixels besides 10 superpixels
		\item \textbf{Surrogate}: Locally weighted sparse linear models 
		\item \textbf{Proximity}: Exponential kernel with euclidean distance
	\end{itemize}
% https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_image
	\vspace{-0.3cm}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{\pathiml/figure/lime-images}
		
		{Top 3 classes predicted}
	\end{center}
	
\end{frame}
\endlecture

	\renewcommand{\titlefigure}{\pathiml/figure/lime5}
    \renewcommand{\learninggoals}{
    	\item Learn why LIME should be used with caution
    	\item Possible pitfalls of LIME}
	
	\lecturechapter{LIME Pitfalls}
	%\lecture{Interpretable Machine Learning}
	\mysectionslide
	
	% Prerequisite: le-into, le-lime
	
	% ------------------------------------------------------------------------------


\begin{frame}[c]{LIME Pitfalls}
  \begin{itemize}
  	\item %Despite being a popular interpretation method, several papers caution to be careful in LIME
  	LIME is one of the best known interpretable ML methods\\ 
  	$\leadsto$ But several papers caution to be careful in practice 
  	\item Problems can occur on different levels which are described subsequently: 
  	\begin{itemize}
  	    \item Sampling procedure (extrapolation)
  	    \item Definition of locality (sensitivity)
  	    \item Scope of feature effects (local vs. global)
  	    \item Faithfulness (trade-off with sparsity)
  	    \item Surrogate model (hiding biases, robustness)
  	    \item Definition of superpixels in case of image data (sensitivity)
  	\end{itemize}
  	%\item These are discussed in more detail in the following 
  \end{itemize}
  
\end{frame}
  
\begin{frame}[c]{Pitfall: Sampling}
	\begin{itemize}
	\itemsep1em
	  \item \textbf{Pitfall}: Common sampling strategies for $\zv \in Z$ do not account for correlation between features 
      \item \textbf{Implication}:  Unlikely data points might be used to learn local explanation models
      \pause
      \item \textbf{Solution I}: Use a local sampler directly on $\Xspace$\\
      $\leadsto$ derivation is particularly difficult for high dimensional or mixed feature spaces 
      \item \textbf{Solution II}: Use training data to fit surrogate model\\
      $\leadsto$ only works well with enough data near $\xv$
    \end{itemize}
    
\end{frame}

\begin{frame}[c]{LIME Pitfall: Locality}

	\begin{itemize} 
     \item \textbf{Pitfall}: Difficult to define locality (= how samples are weighted locally) \\
    % \begin{itemize}
     %    \item[$\leadsto$] 
     $\leadsto$ Strongly affects local model, but there is no automatic procedure for choosing neighborhood
     %\end{itemize}
     \item Originally, an exponential kernel as proximity measure between $\xv$ and $\zv$ was proposed:\\
     	$\neigh(\zv) = exp(-d(\xv, \zv)^2/\sigma^2)$ where $d$ is a distance measure and $\sigma$ is the kernel width 
    %  	 \begin{center}
    %  		\includegraphics[width=0.6\textwidth]{\pathiml/figure/lime_locality}
    %  		\vspace{-0.5cm}
     		
    %  		\scriptsize{Linear surrogate models for two observations based on the same model with one target and one feature. Each line displays one linear surrogate model with different kernel width. In the right figure, larger kernel sizes are more severe.}
     		
    %  	\end{center}
     \end{itemize}
     \pause
     \begin{columns}[T, totalwidth=\linewidth]
        \begin{column}{0.65\textwidth}
        \includegraphics[width=\textwidth, trim = 0px 0px 30px 40px, clip]{\pathiml/figure/lime_locality}
         \end{column}
         \begin{column}{0.35\textwidth}
         \begin{itemize}
             \item Surrogate models for 2 obs. (green points) for same model with one feature $x_1$
             \item Each line refers to a linear surrogate model with different kernel width
             \item Right figure: larger kernel widths influence lines more
         \end{itemize}
         \end{column}
     \end{columns}
\end{frame}

\begin{frame}[c]{LIME Pitfall: Locality \citebutton{Kopper et al. 2019}{https://slds-lmu.github.io/iml_methods_limitations/}}
    \begin{itemize} 
         \item \textbf{Solution I}: Kernel width strongly interacts with locality:
         \begin{itemize}
             \item Large kernel width leads to interaction with points further away (unwanted)
             \item Small kernel width leads to small neighborhood\\
             $\leadsto$ risk of few data points\\
             $\leadsto$ potentially fitting more noise
         \end{itemize}
         \pause
    	\item \textbf{Solution II}: Use Gower distance where no kernel width needs to be specified 
    	\begin{itemize}
    	    \item \textbf{Problem}: data points far away receive weight $ > 0$\\
    	    $\leadsto$ resulting explanations are rather global than local surrogates   
    	\end{itemize}
    \end{itemize}
\vspace{0.3cm}

\end{frame}

\begin{frame}[c]{Pitfall: Local vs. Global Features \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}

\begin{itemize}%[<+->]
	\item<1-> \textbf{Problem}: \\
	By sampling obs. for the surrogate model from the whole input space, the influence of local features might be hidden in favor of features with global influence (even for small kernel width)
	\item<2-> \textbf{Implication}: 
	\begin{itemize}
	    \item Some features influence the \textbf{global} shape of the black-box model
	    \item Other \textbf{local} features impact predictions only in smaller regions of $\Xspace$ %for a small area of $\Xspace$ 
	\end{itemize}
	\item<3-> \textbf{Example}: Decision trees\\
	$\Rightarrow$ Split features close to root have a more global influence than the ones close to leaves
\end{itemize}

\end{frame}


\begin{frame}{Pitfall: Local vs. Global Features -- Example \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}

\begin{columns}
	\begin{column}{0.6\textwidth}
		\begin{itemize}
		\item Binary classification model
		\item Right figure: %Given in figure to the right:
		\begin{itemize}
		    \item Black and grey crosses: training data
		    \item Green dot: Obs. to be explained
		    \item Background color: Classification of random forest
		    \item Dark grey curve: Classifier's decision boundary
		    \item Dotted lines: Local decision boundary
		\end{itemize}
		\item \textbf{Observation:} Decision boundaries of LIME with different kernels (blue and green lines) do not match the direction of the local decision boundary\\ (which appears steeper)
	\end{itemize}
\end{column}
\begin{column}{0.39\textwidth}
%\vspace{0.3cm}

	\begin{center}
	\includegraphics[width=1\textwidth]{\pathiml/figure/lime-globallocal2}

	{Half-moons dataset}
	
\end{center}

	\end{column}
\end{columns}
%\footnote[frame]{Laugel et al. (2018). Defining Locality for Surrogates in Post-hoc Interpretability. \url{https://arxiv.org/pdf/1806.07498.pdf}.}
\end{frame}

\begin{frame}{Pitfall: Local vs. Global Features -- Solution \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}
\begin{columns}[T, totalwidth=\textwidth]
	\begin{column}{0.6\textwidth}
	%\vspace{-.5cm}
		\begin{itemize}
		\item \textbf{Solution}: 
		Find closest point to $\xv$ from other class and sample new points $\zv$ around it for higher local accuracy
		\begin{center}
		\includegraphics[width=\linewidth]{\pathiml/figure/laugel_method}
		\scriptsize{\textbf{Example:} $\xv$ (red point), closest point from other class (black cross)}
	    %{Local surrogate (LS) method by \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}
		\end{center}
		%Sample new instances $\zv$ around the decision boundary closest from point $\xv$ for higher local accuracy
		\pause
		\item Red dot (right figure): Closest point from other class

		\item Red line: Local surrogate (LS) method \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}\\
		$\leadsto$ better approximates the local direction of the decision boundary 
	\end{itemize}
% 	\begin{center}
% 		\includegraphics[width=1\textwidth]{\pathiml/figure/laugel_method}
% 	    {Local surrogate (LS) method by \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}
% 		\vspace{-0.3cm}
% 		\end{center}
\end{column}
\begin{column}{0.39\textwidth}
%\vspace{0.3cm}

	\begin{center}
	\includegraphics[width=1\textwidth]{\pathiml/figure/lime-globallocal2}
	
	{Half-moons dataset}
	
\end{center}

	\end{column}

\end{columns}
%\footnote[frame]{Laugel et al. (2018). Defining Locality for Surrogates in Post-hoc Interpretability. \url{https://arxiv.org/pdf/1806.07498.pdf}.}
\end{frame}


\begin{frame}[c]{Pitfall: Faithfulness}
\begin{itemize}
%\itemsep1em
	\item \textbf{Problem}: Trade-off between local fidelity vs. sparsity
	\item \textbf{Observation I}: Low fidelity $\leadsto$ unreliable explanations
	\item \textbf{Observation II}: High fidelity requires complex models $\leadsto$ difficult to interpret surrogate model %surrogate model cannot easily be interpreted
	\pause
	\item \textbf{Example: Credit data} 
	\begin{itemize}
	\itemsep0em
	    \item Original prediction by random forest for one data point $\xv$: 
	    $$\fh(\xv) = \hat{\P}(y = 1 ~|~ \xv) = 0.143$$
	    \item %Regularized linear model with only three selected features (\code{sex}, \code{checking.account}, \code{duration}) $g_{lm}(\xv) = 0.283$
	    Linear model with only three selected features (\code{age}, \code{checking.account}, \code{duration}):
	    $$g_{lm}(\xv) = \thetah_0 + \thetah_1 x_{age} + \thetah_2 x_{checking.account} + \thetah_3 x_{duration} = 0.283$$
	    \item Generalized additive model (with all 9 features) is more complex:
    $$%\begin{equation*} 
    %\begin{split}
    g_{gam}(\xv) = \thetah_0 + f_{age}(x_{age}) + f_{checking.account}(x_{checking.account}) + f_{duration}(x_{duration}) +  \dots %& = \thetah_0 + s_{age}(x_{age}) +s_{credit.amount}(x_{credit.amount}) s_{duration}(x_{duration}) + \thetah_{sex = male} \Ind_{sex = male}   \\
    %& + \thetah_{job}(x_{job}) + \thetah_{housing = own} \Ind_{housing = own} +   \thetah_{housing = rent} \Ind_{housing = rent} \\
    %& + \thetah_{saving.accounts = moderate} \Ind_{saving.accounts = moderate} + \thetah_{saving.accounts = rich} \Ind_{saving.accounts = rich} \\
    %& + ... + \thetah_{purpose = radio/TV} \Ind_{purpose = radio/TV}  
    = 0.148$$
    %\end{split}
    %\end{equation*}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Pitfall: Hiding biases \citebutton{Slack et al. 2020}{https://arxiv.org/abs/1911.02508}}

\begin{itemize}
	\item \textbf{Problem}: Developer could manipulate their model to hide biases 
	\item \textbf{Observation}: LIME can sample out-of-distribution points (extrapolation)
	\pause
	\item \textbf{Attack} with adversarial model:
	    \begin{enumerate}
	    \item classifier to discriminate between in-distribution and out-of-distribution data points
	    \item for in-distribution points, use the original (biased) model
	    \item for out-of-distribution points produced for local explanation, use an unbiased model
	    \item[$\leadsto$] LIME samples out-of-distribution points and uses the unbiased model for local explanation\\
	    \item[$\leadsto$] this hides the bias of the true model
	    \end{enumerate}
\end{itemize}
	\begin{columns}[T, totalwidth=\linewidth]
		\begin{column}{0.5\textwidth}
	    \centering
	    \includegraphics[width=\textwidth]{\pathiml/figure/attack_biased_unbiased.jpg}
	    \citebutton{Vres, Sikonja (2021)}{https://arxiv.org/abs/2101.11702}
	    \end{column}
\pause
	    \begin{column}{0.5\textwidth}
	    \textbf{Example}: Not using `gender` to approve a loan % Credit dataset %
	    \begin{itemize}
	        \item biased model trained on features correlated with `gender` (e.g. duration of parental leave)\\
	        $\leadsto$ used 
	        %for in-distribution points (realistic values) 
	        to make biased / unfair predictions
	        \item unbiased model trained on features uncorrelated with `gender`\\
	        $\leadsto$ used to produce explanations based on unbiased predictions to hide bias
	        %$\leadsto$ used for (extrapolated) LIME samples\\
	        %$\Rightarrow$ produced local explanations seem fair as they are based on unbiased model
	        %could be trained on features uncorrelated with `gender`
	    \end{itemize}
	    \end{column}
	\end{columns}
\end{frame}

\begin{frame}[c]{Pitfall: Robustness \citebutton{Alvarez-Melis, D., \& Jaakkola, T. 2018}{https://arxiv.org/abs/1806.08049}}
\begin{itemize}
	\item \textbf{Problem}: Instability of explanations 
	\item \textbf{Observation}: Explanations of two very close points could vary greatly 
	\begin{itemize}
	    \item[$\leadsto$] can happen because of other sampled data points $\zv$
	\end{itemize}
\end{itemize}
\vspace{-0.7cm}
\begin{columns}
	\begin{column}{0.48\textwidth}
		\begin{center}
		
		\includegraphics[width=0.55\textwidth]{\pathiml/figure/lime_robustness_1.png}
		
		{Linear prediction task (logistic regression). \\Linear surrogate returns similar coefficients for similar points.}
		
		\end{center}
	\end{column}
	\begin{column}{0.48\textwidth}
		\begin{center}
	\includegraphics[width=0.55\textwidth]{\pathiml/figure/lime_robustness_2.png}
	
	{Circular prediction task (random forest). \\Linear surrogate returns different coefficients for similar points.}
	
	\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Pitfall: Definition of Superpixels \citebutton{Achanta et al. 2012}{https://ieeexplore.ieee.org/document/6205760}}

\begin{columns}
    
    \begin{column}{0.6\textwidth}
        
        \begin{itemize}
        	\item \textbf{Problem}: Instability because of specification of superpixels for image data 
        	\item \textbf{Observation}: Multiple specification of superpixels exist, influencing both the shape and size 
        	\pause
        	\item \textbf{Implication}: The specification of superpixel has a large influence on the explanations 
        	\item \textbf{Attack}: Change superpixels as part of an adversarial attack $\leadsto$ changed explanation
        \end{itemize}
        
    \end{column}
    
    \begin{column}{0.4\textwidth}
    
        \centering
        \includegraphics[width=0.7\textwidth]{\pathiml/figure/superpixel_woman}
        
    \end{column}
    
\end{columns}

\end{frame}


\endlecture

\renewcommand{\titlefigure}{\pathiml/figure/counterfactuals_obj}
\renewcommand{\learninggoals}{
\item Understand the motivation behind CEs
\item See the mathematical foundation of CEs}

\lecturechapter{Counterfactual Explanations}
%\lecture{Interpretable Machine Learning}
\mysectionslide

% ------------------------------------------------------------------------------

\begin{frame}[c]{Example: Credit Risk Application}
	\begin{itemize}
		\item $\textbf{x}$: customer and credit information
		\item $y$: grant or reject credit
	\end{itemize}
	\begin{center}\includegraphics[width=0.6\linewidth, page=1]{\pathiml/figure/counterfactuals_credit.pdf} \end{center}

	Questions:
	\begin{itemize}
		\item Why was the credit rejected?
		\item Is it a fair decision?
		\item \textbf{How should $\xv$ be changed so that the credit is accepted?}
	\end{itemize}
\end{frame}


\begin{frame}[c]{Example: Credit Risk Application}
	Counterfactual Explanations provide answers in the form of "What-If"-scenarios.
	\begin{center}\includegraphics[width=0.6\linewidth, page=2]{\pathiml/figure/counterfactuals_credit.pdf} \end{center}

	``If the person was more skilled and the credit amount had been reduced to \$8.000,\\ the credit would have been granted."  \\[0.2cm]

\end{frame}


\begin{frame}[c]{Counterfactual Explanations: Main Idea}
	\begin{itemize}[<+->]
	    \item Counterfactual explanations == counterfactuals == CEs
	    \item Explain particular predictions of an ML model by presenting an alternative input whose prediction equals a desired outcome
		\item Represent \textbf{close neighbors} of a data point we are interested in,\\ but belonging to the \textbf{desired outcome}
		\item Reveal which minimal changes to the input are sufficient to receive a different outcome\\
		$\leadsto$ Useful if there is a chance to change the input features (e.g., by changing behaviour)
		\item The targeted audience of CEs are often end-users%, not ML engineers
	\end{itemize}
\end{frame}


\begin{frame}[c]{Aims \& Roles}
	CEs can serve various purposes, the user can decide what to learn from them.
	For example:  \\[0.2cm]
	``If the person had been \textbf{one year older} and the \textbf{credit amount had been increased} to \$12.000,\\ the credit would have been granted."  \\[0.2cm]
	\pause
	\begin{itemize}[<+->]
		\itemsep1.2em
		\item \textbf{Guidance for future actions:}\\ \textit{Ok, I will apply again next year for the higher amount.}
		\item \textbf{Provide reasons:}\\ \textit{Interesting, I did not know that age plays a role in loan applications.}
		\item \textbf{Provide grounds to contest the decision:}\\ \textit{How dare you, I do not want to be discriminated for my age in an application.}
		\item \textbf{Detect model biases:}\\ \textit{There is a bug, an increase in amount should not increase approval rates.}
	\end{itemize}
\end{frame}


\begin{frame}{Philosophical Basis}
%Interestingly, counterfactuals have a long-standing tradition in philosophy and, in fact the IML discussion of CEs is based on the work of Lewis (1973).
Counterfactuals have a long-standing tradition in analytic philosophy\\
$\leadsto$ %and, in fact the IML discussion of CEs is based on the work of Lewis (1973).
Accoding to \citebutton{Lewis (1973)}{https://doi.org/10.2307/2273738}, a \textbf{counterfactual conditional} is a statement of the form:

\begin{center}
``If $S$ was the case, $Q$ would have been the case."
		%\label{eq:sent}
\end{center}

\pause

	\begin{itemize}[<+->]
		\item $S$ is an event that must relate to a past event that didn't occur\\ $\leadsto$ counterfactuals run \textbf{contrary} to the \textbf{facts}
		\item Above statement is true, if in all possible worlds most similar to the actual world where $S$ had been the case, $Q$ would have been the case
		\item A world is similar to another if laws are maximally preserved between the worlds and only a few facts are changed
%		\item Lewis's proposal is hotly debated in philosophy, particularly his notion of similarity between worlds remains controversial.
	\end{itemize}
\end{frame}

\begin{frame}{Philosophical Basis}
% According to Lewis:\newline
%``$Q$ causally depends on $S$ iff, if $S$ were not the case $Q$ would not have been the case.''
	\begin{itemize}[<+->]
	\item Counterfactuals have largely been studied to explain causal dependence
		\item Causal dependence underlies the explanatory power\\
		$\leadsto$ good CEs point to critical causal factors that drove the algorithmic decision
		\item If maximal closeness is relaxed, causally irrelevant factors can become part of the explanation\\
		$\leadsto$ e.g., decreasing loan amount by \$20.000 and being one year older is recommended by the explainer although only loan amount might be causally relevant
		%Think of a case when a decrease in amount by \$20.000 and being one year older is recommended by the explainer to receive the loan while changing only the former suffices
		\item CEs are often contrastive, i.e., they explain a decision by referring to an alternative outcome\\
		$\leadsto$ e.g., if the loan applicant was 30 instead of 60 years old, the approved loan would have been over \$100.000 instead of \$40.000%This is also the reason why counterfactuals must be maximally close to initial inputs, otherwise changes are only sufficient but some of them might be unnecessary.
%		\item Current research on causality is based on Pearl's (2009) causal graphs. Instead of defining causality in terms of counterfactuals, Pearl's approach turns the story around and necessitates a representation of causal mechanisms to define CEs.
	\end{itemize}
%\footnote[frame]{Lewis, David (1973). Counterfactuals. Cambridge, MA: Harvard University Press. ISBN 9780631224952. }
\end{frame}

%\begin{frame}{Philosophical Basis: CEs as Contrastive}
%CEs are or can be easily translated into contrastive explanations and often vice versa. Contrastive explanations are answers to a question of the form ``why did Q' occur instead of Q?''.
%	\begin{itemize}
%	    \item  Contrastive explanations answer these questions by \textbf{contrasting} the actual scenario with a different scenario $S$ in which $Q$ had occurred.
%		\item According to psychologists [Miller (2019)], contrastive explanations are the gold standard of explanations in human-to-human interaction.
%		\item A CE becomes contrastive if its antecedent $S$ is presented in contrast to the actual scenario.
%	\end{itemize}
%\end{frame}

\begin{frame}{Mathematical Perspective}
	Terminology:
	\begin{itemize}
		\item $\xv$: original/factual datapoint whose prediction we want to explain
		\item $y' \subset \R^g$: desired prediction ($y' = 1000$ or $y' =$ ``grant credit") or interval ($y' = [1000, \infty[$)
	\end{itemize}
	\lz\pause
	A \textbf{valid} counterfactual $\xv'$ is a datapoint:
	\begin{enumerate}
		\item whose prediction $\fh(\xv')$ is equal to the desired prediction $y'$
		\item that is maximally close to the original datapoint $\xv$
	\end{enumerate}
	\lz\pause
	Reformulate these two objectives (denoted by $o_1$ and $o_2$) as optimization problem:
	
$$\argmin_{\xv'} \lambda_1 o_p(\fh(\xv'), y') + \lambda_2 o_f(\xv', \xv)$$

	\begin{itemize}
		\item $\lambda_1$ and $\lambda_2$ balance the two objectives
		\item Choice of $o_p$ (distance on prediction space) and of $o_f$ (distance on feature space) is crucial
	\end{itemize}
\end{frame}


\begin{frame}{Mathematical Perspective \citebutton{Dandl et al. (2020)}{https://arxiv.org/abs/2004.11165}}
	
	\begin{itemize}
		\item Regression: $o_p$ could be the L$_1$-distance $o_p(\fh(\xv'), y') = |\fh(\xv')-y'|$
		\item Classification:
		L$_1$-distance for scores and 0-1 Loss for labels, e.g., $o_p(\fh(\xv'), y') = \Ind_{\{ \fh(\xv') \neq y' \}}$
		\pause
		\item $o_f$ could be the Gower distance (suitable for mixed feature space):
		$$o_f(\xv', \xv) = \Gower(\xv', \xv) = \frac{1}{p}\sum_{j = 1}^{p} \delta_G(x'_j, x_j)	\in [0, 1]$$
		The value of $\delta_G$ depends on the feature type (numerical or categorical):
		\begin{equation*}
		\delta_G(x'_j, x_j) =
		\begin{cases}
		\frac{1}{\widehat{R}_j}|x_j'- x_j| & \text{if $x_j$ is numerical} \\
		\Ind_{\{ x_j' \neq x_j \}} & \text{if $x_j$ is categorical}
		\end{cases}
		\end{equation*}
		with $\widehat{R}_j$ as the value range of feature $j$ in the training dataset (to ensure that $\delta_G(x'_j, x_j)	\in [0, 1]$)
	\end{itemize}
%\footnote[frame]{Dandl S., Molnar C., Binder M., Bischl B. (2020) Multi-Objective Counterfactual Explanations. In: Bck T. et al. (eds) Parallel Problem Solving from Nature  PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham.}
%\footnote[frame]{Verma et al. (2020). \href{https://arxiv.org/pdf/2010.10596.pdf}{Counterfactual Explanations for Machine Learning: A Review.}}
\end{frame}

\begin{frame}{Further Objectives}
	%While validity is a necessary condition for counterfactuals,
	Additional constraints can improve the explanation quality of the corresponding CEs\\
	$\leadsto$ popular constraints include sparsity and plausibility
	
	\lz

	\textbf{Sparsity:}
	\begin{itemize}[<+->]
		\item End-users often prefer short over long explanations\\
		$\leadsto$ %changes made to obtain
		counterfactuals should be \textbf{sparse} %(i.e., only few feature values should change)
		\item Objective $o_f$ can take the number of changed features into account (but does not have to)\\
		$\leadsto$ e.g., the L$_0$- and the L$_1$-norm (similar to LASSO) can do this
%		\item There could be a trade-off between the number of features changed and the total amount of change made to obtain a certain prediction.
        \item Independently from $o_f$, sparsity in the changes can be additionally considered by another objective that counts the number of changed features via the L0-norm:
        %we can also account for sparsity by adding an extra term to our objective that counts the number of changed features via the L0-norm
        $$o_s(\xv', \xv) = \sum_{j = 1}^p {\Ind}_{\{ x'_j \neq x_j \}}$$
	\end{itemize}
\end{frame}

\begin{frame}{Further Objectives}
		\textbf{Plausibility:}
		\begin{itemize}
			%\item CEs should suggest alternatives that are plausible -- e.g, it is a bad idea to suggest a loan applicant to raise her income and get unemployed at the same time
			\item<1-> CEs should suggest plausible alternatives\\
			$\leadsto$ e.g., not plausible to suggest to raise your income and get unemployed at the same time
			\item<2-> CEs should be realistic and adhere to data manifold or originate from distribution of $\Xspace$\\
			$\leadsto$ avoid unrealistic combinations of feature values
			%We desire realistic CEs in the sense that they originate from the distribution of $\Xspace$ or adhere to the data manifold
			\item<3-> Estimating joint distribution of training data is complex, especially for mixed feature spaces\\
			$\leadsto$ Proxy: ensure that $\xv'$ is close to training data $\Xmat$
		\end{itemize}	
	\only<4>{
	\begin{columns}[c]
	\begin{column}{0.4\textwidth}
	\includegraphics[width=1\textwidth]{\pathiml/figure/counterfactuals_obj}			
	\end{column}
	\begin{column}{0.6\textwidth}

\textbf{Example from \citebutton{Verma et al. (2020)}{https://arxiv.org/abs/2010.10596}} 
\begin{itemize}
    \item Two possible paths for \textcolor{blue}{$\xv$},			originally classified to $\pmb{\circleddash}$
    \item Two valid CEs in class $\pmb\oplus$: {\color{red} CF1} and {\color{green} CF2}
    \item {\color{red} Path \textbf{A} for CF1} is shorter
    \item {\color{green} Path \textbf{B} for CF2} is longer but adheres to data manifold
\end{itemize}
%Two possible paths for \textcolor{blue}{$\xv$},			originally classified in the negative class. The two counterfactuals (\textcolor{red}{CF1} and \textcolor{green}{CF2}) are valid. Note that the red path $A$ for CF1 is the shortest, whereas the green path $B$ for CF2 adheres closely to the manifold of the training data, but is longer.
\end{column}
\end{columns}
}
%\footnote[frame]{Verma et al. (2020). \href{https://arxiv.org/pdf/2010.10596.pdf}{Counterfactual Explanations for Machine Learning: A Review.}}

\end{frame}


\begin{frame}{Further Objectives}
	%\begin{itemize}
	%\item 
%For example, $o_4$ could then be the Gower distance of $\xv'$ to the nearest data point of the training dataset $\xv^{[1]}$

To ensure plausibility, $o_4$ could, e.g., be the Gower distance of $\xv'$ to its nearest data point of the training dataset which we denote $\xv^{[1]}$:
	
$$o_4(\xv', \Xmat) = \Gower(\xv', \xv^{[1]}) = \frac{1}{p} \sum_{j = 1}^{p}  \delta_G(x_j', x^{[1]}_j)$$
%\end{itemize}

We can extend the previous optimization problem by adding $o_s$ (for sparsity) and $o_4$ (for plausibility):

$$\argmin_{\xv'} \lambda_1 o_p(\fh(\xv'), y') + \lambda_2 o_f(\xv', \xv) + \lambda_3 o_s(\xv', \xv) + \lambda_4 o_4(\xv', \Xmat)$$

\end{frame}


\begin{frame}{Remarks: The Rashomon Effect}
%The solution to the optimization problem might not be unique, there can be many equally close inputs that obtain the desired classification. Correspondingly, there can be many different equally good explanations for the same decision. This is called the \textbf{Rashomon effect}.

\textbf{Issue (\textbf{Rashomon effect}):}
\begin{itemize}
    \item Solution to the optimization problem might not be unique
    \item Many equally close CE might exist that obtain the desired prediction\\
    $\Rightarrow$ Many different equally good explanations for the same decision exist
\end{itemize}

\lz\pause

\textbf{Possible solutions:}
	\begin{itemize}
	\item Present all CEs for a given $\xv$ (but: time and human processing capacity is limited)
		%\item We could present all CEs for a given case; however, time is limited and so is the human processing capacity
		%\item Another solution is to focus on one or few CEs; however, by which criterion should they be selected?
		\item Focus on one or few CEs (but: by which criterion should they be selected?)
	\end{itemize}
	
\lz\pause

\textbf{Note:}
	\begin{itemize}
        \item As the model is generally non-linear, inconsistent and diverse CEs can arise\\
        e.g. suggesting either an increase or decrease in credit duration (confuses the explainee)
		\item How to deal with the Rashomon effect is considered an open problem in IML
	\end{itemize}
\end{frame}

\begin{frame}[c]{Remarks: Model or Real-World}

	\begin{itemize}[<+->]
	\item Most CEs provide explanations of model predictions, but CEs might appear to explain the real-world for end-users\\
$\leadsto$ Transfer of model explanations to explain real-world is generally not permitted
	\item Consider a CE that proposes to increase the feature age by 5 to obtain the loan\\
	$\leadsto$ a loan applicant takes this information and applies 5 years later for the loan
	\item However, by then, many 
	%of their 
	other feature values 
	%properties 
	might have changed\\
	$\leadsto$ not only age, also other causally dependent features e.g. job status might have changed \\%after 5 years% since they are causally dependent on age like income or the job status
	$\leadsto$ \citebutton{Karimi et al. (2020)}{https://arxiv.org/abs/2002.06278} avoid this by considering causal dependencies between features
	\item Also, the bank's algorithm might change and previous CEs are not applicable anymore
	%\item Even worse, the user might have typos in the application and in fact no changes are necessary to obtain the loan.
%		\item Actionability: We could further strengthen above's plausibility criterion by requiring counterfactuals that do not change immutable features (e.g., race, city of birth, sex). Therefore, we could  search for counterfactuals only among a defined feasible set of counterfactuals $\mathcal{A}$.
%		\item Causality: We could also restrict the search to counterfactuals that maintain any known causal relations. In the real world, if one feature is changed it affects also other features. E.g., better skills lead to better salary, but also a higher age due to the necessary training.
	\end{itemize}
%\footnote[frame]{Karimi et al. (2021). Algorithmic Recourse: From Counterfactual Explanations to Interventions.  Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 353362.}
\end{frame}


\endlecture

\renewcommand{\titlefigure}{\pathiml/figure/counterfactuals_heat.png}
    \renewcommand{\learninggoals}{
    	\item See two strategies to generate CEs
    	\item Know problems and limitations of CEs}
	
	\lecturechapter{Methods \& Discussion of CEs}
	%\lecture{Interpretable Machine Learning}
	\mysectionslide
	
	% ------------------------------------------------------------------------------


\begin{frame}{Overview of Methods}
	Currently, multiple methods exist to calculate counterfactuals. They mainly differ in: 
	\begin{itemize}[<+->]
		\item \textbf{Targets:} Most methods focus on classification models, only few cover regression models\\
		$\leadsto$ so far, all methods remain in the supervised learning paradigm
		\item \textbf{Data:} Methods mainly focus on tabular data, few on visual/text data, none on audio data
		\item \textbf{Feature space:} Some methods can only handle numerical features, few can process mixed (numerical and discrete) feature spaces
		\item \textbf{Objectives:} Many methods focus on action guidance, plausibility and sparsity, few on other objectives like fairness or individual preferences
		\item \textbf{Model access:} Methods either require access to complete model internals, access to gradients, or only to prediction functions $\Rightarrow$ Model-agnostic and model-specific methods exist
		\item \textbf{Optimization tool:} Gradient-based algorithms (only for differentiable models), mixed-integer programming (only linear), or gradient-free algorithms e.g. Nelder-Mead, genetic algorithm
		\item \textbf{Rashomon Effect:} Many methods return a single counterfactual per run, some multiple counterfactuals, others prioritize CEs or let the user choose
	\end{itemize}
\end{frame}

\begin{frame}{First Optimization Method \citebutton{Wachter et. al (2018)}{http://dx.doi.org/10.2139/ssrn.3063289}}
Introduced counterfactual explanations in the context of ML predictions by solving
		\begin{equation}
			\argmin_{\xv'} \max_{\lambda} \lambda \underbrace{(\fh(\xv') - y')^2}_{o_p(\fh(\xv'), y')} + \underbrace{\sum\nolimits_{j = 1}^p |x'_j - x_j|/MAD_j}_{o_f(\xv', \xv)}
			%\label{eq:wachter}
		\end{equation}
	$MAD_j$ is the median absolute deviation of feature $j$. In each iteration, optimizers like Nelder-Mead solve the equation for $\xv'$ and then $\lambda$ is increased until a sufficiently close solution is found \\[0.2cm]
	
	%\pause
	
	This optimization problem has several shortcomings: 	
	\begin{itemize}%[<+->]
		\item We do not know how to choose $\lambda$ a priori 
		\item Due to the maximization of $\lambda$, we focus primarily on the minimization of $o_p$\\
		$\leadsto$ only if $\fh(\xv') = y'$, we focus on minimizing $o_f$ 
		\item Definition of $o_f$ only covers numerical features 
		\item Other objectives such as sparsity and plausibility of counterfactuals are neglected
	\end{itemize}
	


	%\footnote[frame]{Wachter S, Mittelstadt B, Russel C (2017). Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR. Harvard Journal of Law \& Technology, 31 (2), 2018. \url{http://dx.doi.org/10.2139/ssrn.3063289}}
\end{frame}

\begin{frame}{Multi-Objective Counterfactual Explanations \citebutton{Dandl et al. (2020)}{https://arxiv.org/abs/2004.11165}}
	\begin{itemize}
		\item \textbf{Multi-Objective Counterfactual Explanations (MOC):} Instead of collapsing objectives into a single objective, we could optimize all four objectives simultaneously
	$$	\argmin_{\xv'} \left(o_p(\fh(\xv'), y'), o_f(\xv', \xv), o_s(\xv', \xv), o_4(\xv', \Xmat) \right). $$
		
		\item Note that weighting parameters like $\lambda$ are not necessary anymore
		%\item This approach is called Multi-Objective Counterfactual Explanations (MOC) and was developed by Dandl et al. in 2020. 
		\item Uses an adjusted multi-objective genetic algorithm (NSGA-II) to produce a set of diverse counterfactuals for mixed discrete and continuous feature spaces
		\item Instead of one, MOC returns multiple counterfactuals that represents different trade-offs between the objectives and are constructed to be diverse in feature space
	\end{itemize}

	%\footnote[frame]{Dandl et al. (2020) Multi-Objective Counterfactual Explanations. In: Bck T. et al. (eds) Parallel Problem Solving from Nature  PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham.}
\end{frame}

\begin{frame}{Example: Credit Data}
	\begin{itemize}
		\item Model: SVM with RBF kernel
		\item $\xv$: First data point of credit data with $\P(y = good)  = 0.34$ of being a ``good" customer
		\item Goal: Increase the probability to $[0.5, 1]$
		\item MOC (with default parameters) found 69 CEs after 200 iterations that met the target
		\item All counterfactuals proposed changes to credit duration and many of them to credit amount
	\end{itemize}
\end{frame}

\begin{frame}{Example: Credit Data \citebutton{Dandl et al. (2020)}{https://arxiv.org/abs/2004.11165}}
\begin{itemize}
	\item<1-> We can visualize feature changes with a parallel plot and 2-dim surface plot
		\item<1-> Parallel plot reveals that all counterfactuals had values equal to or smaller than the values of $\xv$
		\item<2-> Surface plot illustrates why these feature changes are recommended 
		\item<2-> Counterfactuals in the lower left corner seem to be in a less favorable region far from $\xv$, but they are in high density areas close to training samples (indicated by histograms)
	\end{itemize}
	\begin{columns}\only<1->{
	\begin{column}{0.5\textwidth}  
	\centering
			%\begin{center}
		\includegraphics[width=0.75\textwidth]{\pathiml/figure/counterfactuals_credit_parallel}\\
			%\end{center}
		%\vspace{-0.2cm}
			\scriptsize{\textbf{Parallel plot:} Grey lines show feature values of CEs $\xv'$, blue line are values of $\xv$. Features without proposed changes are omitted. Bold numbers refer to range of numeric features.} 
			
		\end{column}}
		\visible<2->{
		\begin{column}{0.5\textwidth}
			%\begin{center}
			\centering
			\includegraphics[width=\textwidth]{\pathiml/figure/counterfactuals_credit_heat}\\
			%\end{center}
		%\vspace{-0.2cm}
			\scriptsize{\textbf{Surface plot:} White dot is $\xv$, black dots are CEs $\xv'$. \\
			Histograms show marginal distribution of training data $\Xmat$.} 
		\end{column}
		}
	\end{columns}
%\footnote[frame]{Dandl S., Molnar C., Binder M., Bischl B. (2020) Multi-Objective Counterfactual Explanations. In: Bck T. et al. (eds) Parallel Problem Solving from Nature  PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham.}
\end{frame}


%\begin{frame}{Example: Bike Sharing Dataset}
%	\begin{itemize}
%		\item Model: Random Forest with 500 trees
%		\item $\xv$ is the first data point of the dataset with $\fh(\xv) = 1767.93$ rental bikes. 
%		\item Our desired goal is to increase the count of total rental bikes to $y' = [3000, \infty[$
%		\item MOC (with default parameters) found 56 counterfactuals after 200 iterations that met the target.
%		\item Most counterfactuals proposed to decrease the humidity (94.6 \%) and more than half to increase the temperature (55.4\%). 
%		\item Some counterfactuals proposed additional changes to the year (2012 instead of 2011) and month (December instead of Januar).
%		\framebreak 
%		\item We can visualize feature changes with a parallel plot. 
%		\item For humidity and temperature, we can additionally show a 2-dim surface plot. 
%	\end{itemize}
%	\vspace{-0.5cm}
%	\begin{columns}
%		\begin{column}{0.5\textwidth}
%			\begin{center}
%				\includegraphics[width=1\textwidth]{\pathiml/figure/counterfactuals_bike_sp}
%			\end{center}
%		
%			\scriptsize{\textbf{Figure:} Response surface plot. 
%				The white dot is $\xv$, black dots are $\xv'$. The histograms display the marginal distribution of the training data $\Xmat$.} 
%				
%		\end{column}
%		\begin{column}{0.5\textwidth}  
%			\begin{center}
%				\includegraphics[width=1\textwidth]{\pathiml/figure/counterfactuals_bike_para}
%			\end{center}
%		
%		\scriptsize{\textbf{Figure:} Parallel plot. 
%			The grey lines show the feature values of the counterfactuals $\xv'$, the blue line corresponds to the values of $\xv$. Features without proposed changes are omitted. The bold numbers give minima and minima of numeric features while character strings indicate categories of character features.} 
%		
%		\end{column}
%	\end{columns}
%\end{frame}

\begin{frame}{Problems, Pitfalls, \& Limitations}
\begin{itemize}[<+->]
    \item \textbf{Illusion of model understanding:} 
    CEs explain ML decisions by pointing to few specific alternatives which reduces complexity, but is limited in explanatory power\\
    $\leadsto$ %psychologists showed that even though the perceived model-understanding of end-users increases, the objective model-understanding remained unchanged
    Psychologists have shown that although perceived model understanding of end-users increases, the objective model understanding remains unchanged
    
    %\item \textbf{Finding the right metric:} Similarity is the crucial concept for finding good CEs. However, our concept of similarity is context and domain dependent. E.g. while L1 can be a reasonable notion for tabular data, it is counterintuitive for image data. Sparsity is often desirable for end-users but not for data scientists searching for biases in the model.
    \item \textbf{Right metric:} Similarity measures are crucial to find good CEs (depends on context/domain)\\
    $\leadsto$ e.g., $L_1$ can be reasonable for tabular data but not for image data\\
    $\leadsto$ sparsity can be desirable for end-users but not for data scientists searching for model bias

    %\item \textbf{Confusing Model and Real-World:} Explanations of the model do not easily transfer to the process in which a model is applied. This information should be conveyed to the end-user.
    \item \textbf{Confusing Model and Real-World:} Model explanations are not easily transferable to reality\\
    $\leadsto$ End-users need to be aware that CE provide insights into a model not the real world %models are only approximations %This information should be conveyed to the end-user
    \item \textbf{Disclosing too much information:} \\
    CEs can reveal too much information about the model and help potential attackers
    \end{itemize}
\end{frame}

\begin{frame}{Problems, Pitfalls, \& Limitations}
\begin{itemize}[<+->]
    \item \textbf{Rashomon effect:} One, few, all? Which CEs should be shown to the end-user?\\
    $\leadsto$ No perfect solution, depends on end-users computational resources and knowledge
    \item \textbf{Actionability vs. fairness:} Some authors suggest to focus only on the actionability of CEs\\
    $\leadsto$ Counteract contestability, e.g., if ethnicity is not changed in a CE since it is not actionable, this could hide racial biases in the model
    \item \textbf{Assumption of constant model:} To provide guidance for the future, CEs assume that their underlying model does not change in the future\\
    $\leadsto$ in reality this assumption is often violated and CEs are not reliable anymore 
    \item \textbf{Attacking CEs:} Researchers can create models with great performance, which generate arbitrary explanations specified by the ML developer\\
    $\leadsto$ how faithful are CEs to the models underlying mechanism?
\end{itemize}

%	\textbf{Pitfall 3:} Rashomon Effect
%	\begin{itemize}
%		\item Due to the Rashomon Effect, multiple counterfactual explanations could be found for an instance. 
%		\item If all counterfactuals are reported the user could be overwhelmed. Instead of a comprehensible explanations for a prediction, users received an even more complex explanations.
%		\item Another option is to only report the ``best" ones. But this requires a notion for ``superiority".  
%		\item Furthermore, users might not be interested in the ``best" but most ``diverse" counterfactuals.
%		\item The best option might be to report all counterfactuals but let the user decide which one to select, e.g., based on their previous knowledge. 
%	\end{itemize}
%	\textbf{Pitfall 4:} 
%	\begin{itemize}
%		\item 
%	\end{itemize}
%	\textbf{Pitfall 5:} Confusing model explanation with real data process explanations
%	\begin{itemize}
%		\item Causal dependencies
%		\item Fixed model at time $t$ 
%		\item Wrong input by user
%	\end{itemize}
\end{frame}

\endlecture

\renewcommand{\titlefigure}{\pathiml/figure/dbscan.jpg}
\renewcommand{\learninggoals}{
\item Understand the aspects that undermine users' trust in an explanation
\item Learn diagnostic tools that could increase trust }

\lecturechapter{Increasing Trust in Explanations}
%\lecture{Interpretable Machine Learning}
\mysectionslide

% Prerequisites: LIME and SHAP

% ------------------------------------------------------------------------------


% ------------------------------------------------------------------------------

\begin{frame}{Motivation \& Important Properties}
	\begin{itemize}
		\item Local explanations should not only make a model interpretable but also reveal if the model is trustworthy
	    \pause
	    \item \textbf{Interpretable}: ``Why did the model come up with this decision?''
	    \pause
	    \item \textbf{Trustworthy}: ``How certain is this explanation?''
	    \begin{enumerate}
	        \item accurate insights into the inner workings of our model
	        \begin{itemize}
	            \item Failure case: generation is based on inputs in areas where the model was trained with little or no training data (extrapolation)
	        \end{itemize}
	        \pause
	        \item robust (i.e. low variance)
	        \begin{itemize}
	            \item Expectation: similar explanations for similar data points with similar predictions
	            \item However, multiple sources of uncertainty exist
	            \item[$\leadsto$] measure how robust an IML method is to small changes in the input data or parameters
	            \item[$\leadsto$] Is an observation out-of-distribution?
	        \end{itemize}
	    \end{enumerate}
	    \pause
		\item Failing in one of these $\leadsto$ undermining users' trust in the explanations\\ $\leadsto$ undermining trust in the model 
		%\item In the upcoming section, we will, therefore, discuss methods to detect if a point is out-of-distribution and to measure how robust IML methods are.
	\end{itemize}
\end{frame}

\begin{frame}[c]{Out-of-distribution Detection}
	\begin{itemize}
		\item Models are unreliable in areas with little data support\\ $\leadsto$ explanations from local explanation methods are unreliable
		\pause
		\item For local explanation methods, the following components could be out-of-distribution (OOD): 
		\begin{itemize}
			\item The data for LIME's surrogate model
			\item Counterfactuals themselves
			\item Shapley value's permuted observations to calculate the marginal contributions 
			\item ICE curves grid data points 
		\end{itemize}
		\pause
		\item Two very simple and intuitive approaches
		\begin{itemize}
		    \item Classifier for out-of-distribution
		    \item Clustering
		\end{itemize}
		\item More complicated also possible, e.g., variational autoencoders [\href{https://arxiv.org/abs/1912.05651}{Daxberger et al. 2020}]
	\end{itemize}
\end{frame}


\begin{frame}[c]{Out-of-distribution Detection: OOD-Classifier}
	\begin{itemize}
	    \item Problem: we have only in-distribution data
	    \item Idea: Hallucinate new (out-of-distribution) data by randomly sample data points
	    \item[$\leadsto$] Learn a binary classifier to distinguish between the origins of the data
	    \medskip
	    \pause
	    \item Study whether an explanation approach can be fooled \citebutton{Dylan Slack et al. 2020}{https://arxiv.org/abs/1911.02508}
	    \begin{itemize}
	        \item Hide bias in the true (deployed) model, but use an unbiased model for all out-of-distribution samples
	    \end{itemize}
	    \item[$\leadsto$] Important way to diagnose an explanation approach
	\end{itemize}
\end{frame}

\begin{frame}[c]{Out-of-distribution Detection: Clustering via DBSCAN}
\begin{itemize}
	\item DBSCAN is a data clustering algorithm \citebutton{Martin Ester et al. 1996}{https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf}\\ (Density-Based Spatial Clustering of Applications with Noise) 
	\pause
	\item For this method, we define an $\epsilon$-neighborhood: \\
	Given a dataset $X = \{\xi\}_{i = 1}^n$, an $\epsilon$-neighborhood for $\xv \in \Xspace$ is defined as 
	$$ \mathcal{N}_{\epsilon}(\xv) = \{\xi \in \Xspace | d(\xv, \xi) \le \epsilon\}.$$
	 $d(\cdot)$ is a distance measure (e.g., Euclidean or Gower distance) 
	 \pause
	\item Core observations $\xv$
	\begin{itemize}
	    \item Have at least $m$ data points within $\mathcal{N}_{\epsilon}(\xv)$
	    \item Forms an own cluster with all its neighborhood points
	\end{itemize}
	\pause
    \item Border points
    \begin{itemize}
        \item Within $\mathcal{N}_{\epsilon}(\xv)$
        \item Part of a cluster defined by a core point
    \end{itemize}
    \pause
    \item Noise points
    \begin{itemize}
        \item Are not within $\mathcal{N}_{\epsilon}(\xv)$
        \item Not part of any cluster
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[c]{Out-of-distribution Detection}
\vspace{-0.6cm}
\begin{columns}

	\begin{column}{0.5\textwidth}
	    \vspace{-2em}
		\begin{center}
			\includegraphics[width=0.6\textwidth]{\pathiml/figure/dbscan.jpg}\\
			\tiny{Example for DBSCAN, circles display $\epsilon$-neighborhoods, $m = 4$}
		\end{center}
	\end{column}

	\begin{column}{0.5\textwidth}
	
		\begin{itemize}
			\item Green points A and B are core points and form one cluster since they lie in each others neighborhood, all yellow points are border points of this cluster 
			\pause
			\item Since D is not part of the neighborhood of core points, it is a noise point 
			\pause
			\item In-distribution: new point lies within a cluster
			\pause
		    \item Out-of-distribution: new point lies outside the clusters 
		\end{itemize}
	\end{column}

\end{columns}

\pause

\hspace{1em}
\begin{itemize}
		\item Disadvantages:
		\begin{itemize}
		    \item Depending on the distance metric $d(\cdot)$, DBSCAN could suffer from the ``curse of dimensionality'' 
		    \item The choice of $\epsilon$ and $m$ is not clear a-priori 
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[c]{Robustness}
		\begin{itemize}
		\item Differentiate between different kinds of uncertainty: 
		\begin{enumerate}
			\item \textbf{Explanation uncertainty}: Change of explanation if we repeat the process, e.g., the explanation could differ depending on which subset of data we use for the explanation method and which hyperparameters 
			\pause
			\item \textbf{Process uncertainty}: Change of explanation if the underlying model is changed\\ $\leadsto$ are ML models non-robust, e.g., because they are trained on noisy data?
		\end{enumerate}
		\pause
		\item We focus on explanation uncertainty 
		\begin{itemize}
		    \item Even with the same model and same (or similar) data points, we can receive different explanations
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[c]{Robustness Measure for LIME and SHAP}  
	\begin{itemize}
		\item Objective: Similar explanations for similar inputs (in a neighborhood) 
		\pause
		%\item In the previous chapter on the limitations of LIME, we already saw an example where LIME produced different explanations for very similar data points (with similar predictions).
		\item For LIME and SHAP, notion of stability based on \textbf{locally Lipschitz continuity} \citebutton{Alvarez-Melis and Jaakkola 2018}{https://arxiv.org/abs/1806.08049}:\\
		An explanation method $g:\Xspace \rightarrow \R^m$ is locally Lipschitz if 
		\begin{itemize}
		    \item for every $\xv_0 \in \Xspace$ there exist $\delta > 0$ and  $\omega \in \R$
		    \item such that $||\xv - \xv_0|| < \delta$ implies $||g(\xv) - g(\xv_0)|| < \omega ||\xv - \xv_0||$
		\end{itemize}
		\footnotesize Note that, for LIME, $g$ returns the $m$ coefficients of the surrogate model \normalsize
		\pause
		\item According to this, we can quantify the robustness of explanation models in terms of $\omega$:
		\begin{itemize}
		    \item[$\leadsto$] The closer $\omega$ is to 0, the more robust our explanation method is 
		\end{itemize}
		\pause
		\item $\omega$ is rarely known a-priori but it could be estimated as follows: 
		$$\hat{\omega}_{X}(\xv) \in \underset{\xi \in \mathcal{N}_{\epsilon}(\xv)}{\arg \max} \frac{||g(\xv) - g(\xi)||_2}{d(\xv, \xi)},$$
		where $\mathcal{N}_{\epsilon}(\xv)$ is the $\epsilon$-neighborhood of $\xv$
	\end{itemize}
\end{frame}


\begin{comment}
\begin{frame}{Sources of Uncertainty}
Two sources of uncertainty could be identified for local explanation methods: 
\begin{itemize}
\item Sampling variance in explaining a single data point. 
E.g., to train a surrogate model for LIME we sample a new data set.   
\item Sensitivity to choice of parameters. E.g., the user needs to determine the sample size and the kernel width to explain a model with LIME. 
\end{itemize}
These sources could lead to different explanations although we analyse the same model and the same (or a very similar) data point. 
\footnote[frame]{Zhang et al. (2019). ``Why Should You Trust My Explanation?'' Understanding Uncertainty in LIME Explanations. arXiv preprint arXiv:1904.12991.} 
\end{frame}
\end{comment}


\begin{comment}
\begin{frame}{Noisy data}
	\begin{itemize}
		\item What if the ML model was trained on noisy data? Should the explanation model also take up these noisy patterns? 
		\item If we use an explanation model to debug the ML model, it is okay, if our explanation model also takes up the noisy patterns learned in the ML model. 
		\item If we use an explanation model to understand both the predictor but also the underlying true data generating process, we want to focus on the stable patterns learned by the ML model and receive robust explanations. 
	\end{itemize}
\end{frame}

\begin{frame}{Addtional notes}
	\begin{itemize}
		\item We cannot only impose that the explanation of one particular method is similar for similar data points, but also across multiple IML methods. 
	\end{itemize}
\end{frame}

content...
\end{comment}

\endlecture

\end{document}
