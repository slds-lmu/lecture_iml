\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}
\newcommand{\open}{}
\newcommand{\close}{}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Understanding why fANOVA theoretically works and what constraints it satisfies
\item Understand the important role further constraints play for any functional decomposition?
\item ??}

\lecturechapter{Theory of Functional Decompositions}
\lecture{Interpretable Machine Learning}

\begin{frame}{Example: fANOVA Algorithm}

    See before: The definition of functional decompositions is by far not unique \\

    BUT: The fANOVA Algo. yields a unique output / unique decomposition, seemingly "arbitrarily chosen".\\

    \(\rightarrow\) Idea: Uniquely (Eindeutig?) characterize the solution of the fANOVA by its properties. These properties are expressed as mathematical constraints that describe the calculated decomposition.

    \(\rightarrow\) Two equivalent ways of uniquely / completely defining this decomposition: Either by computation formula defining the single components, or by mathematical equations (the "constraints") defining what properties the components must fulfill
    
\end{frame}

\begin{frame}{Constraints for standard fANOVA Algorithm}

    One can prove that our definition of the components fulfills the following conditions, called \textit{vanishing condition}:
    \begin{equation*}
        \mathbb{E}_{X_j}\left[ g_{S}(\xv_S) \right] = \int g_{S}(\xv_S) d \mathbb{P}(x_j) = 0, \forall j \in S, \forall S \subseteq \{1, \ldots, p\}
    \end{equation*}

    % For independent inputs, the \textit{vanishing condition} is required to obtain a unique solution:
    % $$\mathbb{E}_{X_j} (g_{S}(\xv_S)) = \int g_{S}(\xv_S) d \mathbb{P}(x_j) = 0, \forall j \in S, \forall S \subseteq \{1, \ldots, p\}$$

    \textit{Proof that our definition fulfills these constraints? Not here}
    
    \pause 
    
    Vanishing conditions have the following implications:
    
    \begin{itemize}
        \item Marginalizing out $x_j, \forall j \in S$ for component $g_S(\xv_S)$ yields a constant 0\\
        %As we integrate out the marginal effect of $x_j, \forall j \in S$ on component $g_S(\xv_S)$ is zero (vanishes)\\
        $\leadsto$ Makes sure that component $g_S(\xv_S)$ does not contain effects of $x_j, \forall j \in S$
        %For $|S| = 1$ this is equivalent to mean-centering the component $g_S(\xv_S)$. For $|S| > 1$
        \item Components are orthogonal (i.e., mutually independent and uncorrelated):
        $$\mathbb{E}_{X} (g_{V}(\xv_V) g_{S}(\xv_S)) = 0, \forall V \neq S$$
        \item Variance decomposition and Sobol indices:
    $ Var[\fh(\xv)] =  \textstyle\sum_{S \subseteq \{1,\ldots,p\}}  Var\left[ g_{S}(\xv_S)\right]$
    \end{itemize}
    
\end{frame}

\begin{frame}{Example revisited}
\textbf{Example:} $\fh(\xv) = 2 + x_1^2 - x_2^2 + x_1 \cdot x_2$ (e.g., for $x_1 = 5$ and $x_2 = 10$ we have $\fh(\xv) = -23$)

\begin{itemize}
    \item Computation of components using feature values $x_1 = x_2 = (-10, -9, \ldots, 10)^\top$ gives:
    \begin{columns}[c, totalwidth=\linewidth]
    \begin{column}{0.75\textwidth}
        \includegraphics[width = \textwidth]{figure/decomposition}
    \end{column}
    \begin{column}{0.25\textwidth}
    For $x_1 = 5$ and $x_2 = 10$:\\
    \begin{itemize}
        \item $g_{\open \emptyset \close} = 2$
        \item $g_{\open 1 \close}(x_1) = -9.67$
        \item $g_{\open 2 \close}(x_2) = -65.33$
        \item $g_{\open 1,2 \close}(x_1, x_2) = 50$
        \item[$\Rightarrow$] $\fh(\xv) = -23$
    \end{itemize}
    \end{column}
    \end{columns}
% \pause
    \item Vanishing condition means:
    \begin{itemize}
        \item $g_1$ and $g_2$ are mean-centered w.r.t. marginal distribution of $x_1$ and $x_2$
        \item Integral of $g_{1,2}$ over marginal distribution $x_1$ (or $x_2$) is always 0.
        %, i.e., for each slice at $x_1$ (and $x_2$), the integral of $g_{1,2}$ 
    \end{itemize}
\end{itemize} 
\end{frame}

\begin{frame}{...}

    [Proofs that these constraints hold / are fulfilled by our definition?? Only briefly]
    
\end{frame}

\begin{frame}{...}

    NB: One can also start the other way around, i.e. define fANOVA using the constraints it should fulfill, and then mathematically prove that the single components must have this form.
    
\end{frame}

\begin{frame}{Connection to PDP, H-statistics}

    It can be shown: A PDP for a group of variables is exactly equal to the sum of all fANOVA components which are part of this group.
    The other way round, a fANOVA component can be described as the difference between the respective PDP and all lower-degree PDPs. (plus / minus some other lower-degree PDPs ...)
    \\
    Vanishing condition implies: No lower-degree terms / interactions are included in any fANOVA component.
    \(\implies\)
    In particular, the function contains interactions of a specific type if and only if the corresponding fANOVA component is not 0.
    \\
    Together, we achieve an alternative definition / characterization of interactions:
    Interactions of certain type exist \(\iff\) corresponding fANOVA component \(\neq 0\) \(\iff\) the respective difference of PDPs, the so-called \textit{H-statistic}, is \(\neq 0\).

    More generally, the H-statistic is a measure of the strength of a specific interaction, which can in general be applied to any model or any function.
     
\end{frame}










\endlecture
\end{document}
