\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}
\newcommand{\open}{}
\newcommand{\close}{}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Limitations of classical fANOVA
\item Alternatives: generalized fANOVA and ALE
% \item Overcoming these limitations with generalized fANOVA
% \item How ALE Plots can be used as another, different approach to obtain functional decompositions
% \item Sobol-Hoeffding decomposition ??
\item Advantages and Relevance of functional decompositions
}

\lecturechapter{Functional Decompositions: Further Methods}
\lecture{Interpretable Machine Learning}

\begin{frame}{Limitations of classical fANOVA}

    \begin{itemize}
    
        \item Standard fANOVA builds on PD-functions
        
        \item \textit{Remember:} Problems of PDPs for \textbf{correlated / dependent features}
        \item \textit{Similar:} Dependent features $\implies$ standard fANOVA does NOT fulfill vanishing conditions
    \end{itemize}
    \begin{example}
        Assume dependency \(2x_1^2 = x_2\) and
        \begin{equation*}
            \fh(x_1, x_2, x_3) = - 2x_1 - 2\sin(x_3) + |x_1|x_2 + 0.5 x_2 x_3 + 1.
        \end{equation*}
        % $$
        % \fh(x_1, x_2, x_3) = - 2x_1 - 2\sin(x_3) + |x_1|x_2 + 0.5 x_1 x_2 x_3 - \sin(x_2 x_3) + 1
        % $$
        $\leadsto$ Following two decompositions would both ``make sense'':
        \begin{align*}
            \fh(x_1, x_2, x_3)
            = \underbrace{1}_{g_\emptyset}
                + \underbrace{(- 2x_1)}_{g_1(x_1)} 
                + \underbrace{(- 2\sin(x_3))}_{g_3(x_3)}
                + \underbrace{|x_1|x_2}_{g_{1,2}(x_1, x_2)} 
                + \underbrace{0.5 x_2 x_3}_{g_{2,3}(x_2, x_3)} \\
            \fh(x_1, x_2, x_3)
            = \underbrace{1}_{g_\emptyset}
                + \underbrace{(- 2x_1 + 2 |x_1|^3)}_{g_1(x_1)} 
                + \underbrace{(- 2\sin(x_3))}_{g_3(x_3)}
                + \underbrace{x_1^2 x_3}_{g_{2,3}(x_1, x_3)}
        \end{align*}
    \end{example}
    $\rightarrow$ Extreme example, but again: Problem of definition

    % [Example going wrong with correlated features]

    
    
\end{frame}

\begin{frame}{Alternative: Generalized Functional ANOVA}

\begin{itemize}
    \item Algorithm proposed by \citebutton{Hooker (2007)}{http://www.tandfonline.com/doi/abs/10.1198/106186007X237892}
    \item Generalizes standard fANOVA to situations with dependent features
    \item Showed: Generalized fANOVA is solution to so-called ``relaxed vanishing conditions'' \\
    (i.e., weaker form of vanishing condition) \\
    \item ``Relaxed vanishing conditions'' do not imply orthogonality, but ``hierarchical orthogonality'':
    $$
    \mathbb{E}_{\Xv} \bigl[ g_{V}(\xv_V) g_{S}(\xv_S) \bigr] = 0 \quad \forall V \subsetneq S
    $$
    $\leadsto$ Only components are orthogonal where $g_{V}(\xv_V)$ is ``lower in hierarchy'' than $g_{S}(\xv_S)$
    \item[$\implies$] Generalized fANOVA provides functional decomposition for arbitrary settings
    \item \textbf{Problems:}
    \item Difficult to estimate, involves manual choice of a ``weight function''
    \item Computationally very costly
    
\end{itemize}

    % \textbf{N.B.:} For dependent inputs, \citebutton{Hooker (2007)}{http://www.tandfonline.com/doi/abs/10.1198/106186007X237892} showed the existence of a unique solution for the components under a ``relaxed vanishing condition'' which leads to a ``hierarchical orthogonality''
    % $$\mathbb{E}_{\Xv} (g_{V}(\xv_V) g_{S}(\xv_S)) = 0, \forall V \subset S$$
    % $\leadsto$ Only components are orthogonal where features involved in $g_{V}(\xv_V)$ also appear in $g_{S}(\xv_S)$
    % Only components where all features involved in one component $g_{V}(\xv_V)$ also appear in the other component $g_{S}(\xv_S)$ are orthogonal

    % \pause

    % \textit{[Also talk about constraints corresponding to generalized fANOVA ?]}
    
\end{frame}

% \begin{frame}{Generalized fANOVA: Example}

%     Example from above ??
    
% \end{frame}

\begin{frame}{Revisiting ALE Plots}

    Recap: ALE PLots [...]
    
\end{frame}

\begin{frame}{ALE Decomposition}
    
    ALE Plots also can lead to a full functional decomposition: [...]
    
\end{frame}

% \begin{frame}{Sobol-Hoeffding decomposition}
    
% \end{frame}

% \begin{frame}{if enough time: Constraints (theory) for these other methods}

%     NB: Even more possible constraints (leading to ever more different decompositions) have been developed in research papers.
    
% \end{frame}

\begin{frame}{Conclusion: How useful are functional decompositions?}

    Obwohl fDecompositions eigtl (aus Interpretability-Sicht) die Lösung für alles wären / theoretisch die Endlösung sind, sind alle anderen Methoden trotzdem nötig, weil fDecompositions sehr schwierig \& kompliziert zu berechnen sind
    - Computation time skaliert exponentiell mit der Dimension / Anzahl features  =>  i.a. ist Erzwingen einer sparsen decomposition (s. GAMs / RPFs) die einzige realisitische Möglichkeit

    Nevertheless, functional decompositions are a very important concept in machine learning interpretability because they explain the idea behind many other methods and enable a much better understanding of other methods.
    
\end{frame}










\endlecture
\end{document}
