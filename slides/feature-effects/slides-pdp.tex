\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/bike-sharing-dataset01}
\newcommand{\learninggoals}{
\item Understand PD plots and their relation to ICE plots
\item Understand how to interpret ICE curves and PD plots
}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\lecturechapter{Partial Dependence (PD) plot}
\lecture{Interpretable Machine Learning}


\begin{vbframe}{Partial Dependence}

The partial dependence (PD) function is the expectation of ICE curves w.r.t. the marginal distribution of complementary features $\xv_C$,
$$f_{S, PD}(x_S) = \E_{\xv_C} \left( \fh(x_S, \xv_C) \right) = \int_{-\infty}^{\infty} \fh(x_S, \xv_C) \, d\P(\xv_C)$$

For each grid value $x_S^*$, the PD function is estimated by the point-wise average of the ICE curves at $x_S^*$:
$$\fh_{S, PD}(x_S^*) = \frac{1}{n} \sum_{i=1}^n \fh(x_S^*, \xv_C^{(i)})$$

%Within the SIPA framework, the partial dependence builds the \textbf{Aggregation} step.

\footnote[frame]{Friedman, Jerome H. (2001). Greedy Function Approximation: A Gradient Boosting Machine. Annals of Statistics: 1189-1232.}
%\footnote[frame]{Scholbeck, C. A., Molnar, C., Heumann, C., Bischl, B., and Casalicchio, G. (2019). Sampling, Intervention, Prediction, Aggregation: A Generalized Framework for Model Agnostic Interpretations. ECML PKDD 2019. (pp. 205-216).}
\end{vbframe}


\frame{
\frametitle{Example: Partial Dependence for LM}

Assume a linear regression model with two features:

$$\fh(\xv) = \fh(\xv_1, \xv_2) = \hat\beta_1 \xv_1 + \hat\beta_2 \xv_2 + \hat\beta_0$$

The PD function for the feature of interest $S = \{1\}$ (with $C = \{2\}$) is:

$$
\begin{aligned}
f_{1, PD}(\xv_1) = \E_{\xv_2} \left( \fh(\xv_1, \xv_2) \right) &= \int_{-\infty}^{\infty} \left( \hat\beta_1 \xv_1 + \hat\beta_2 \xv_2 + \hat\beta_0 \right) \, d\P(\xv_2) \\
&= \hat\beta_1 \xv_1 + \hat\beta_2 \cdot \int_{-\infty}^{\infty} \xv_2 \, d\P(\xv_2) + \hat\beta_0 \\
&= \hat\beta_1 \xv_1 + \underbrace{\hat\beta_2 \cdot \E_{\xv_2} (\xv_2) + \hat\beta_0}_{:= const}
\end{aligned}
$$

Here, the PD plot visualizes the function $f_{1, PD}(\xv_1) = \hat\beta_1 \xv_1 + const$, which is the feature effect of $\xv_1$.

}

\frame{
\frametitle{Partial Dependence}
%\begin{onlyenv}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\centering
\only<1>{
\includegraphics[page=8, width=\textwidth]{figure_man/ice_pd_plot_demo}
\end{column}
\begin{column}{0.5\textwidth}

\begin{center}
\includegraphics[page=1, width=\textwidth]{figure/PD}
\end{center}
}

\only<2>{
\includegraphics[page=9, width=\textwidth]{figure_man/ice_pd_plot_demo}
\end{column}
\begin{column}{0.5\textwidth}

\begin{center}
\includegraphics[page=2, width=\textwidth]{figure/PD}
\end{center}
}

\only<3>{
\includegraphics[page=10, width=\textwidth]{figure_man/ice_pd_plot_demo}
\end{column}
\begin{column}{0.5\textwidth}

\begin{center}
\includegraphics[page=3, width=\textwidth]{figure/PD}
\end{center}
}
\end{column}
\end{columns}
%\end{onlyenv}

\only<1>{
Estimate partial dependence by the \textbf{point-wise} average of the ICE curves at gridpoint \fcolorbox{red}{white}{$x_S^* = x_1^* = 1$}:
$$\fh_{1, PD}(x_1) = \frac{1}{n} \sum_{i=1}^n \fh(x_1, \xv_{2, 3}^{(i)})$$
}
\only<2>{
Estimate partial dependence by the \textbf{point-wise} average of the ICE curves at gridpoint \fcolorbox{red}{white}{$x_S^* = x_1^* = 2$}:
$$\fh_{1, PD}(x_1) = \frac{1}{n} \sum_{i=1}^n \fh(x_1, \xv_{2, 3}^{(i)})$$
}
\only<3>{
Estimate partial dependence by the \textbf{point-wise} average of the ICE curves at gridpoint \fcolorbox{red}{white}{$x_S^* = x_1^* = 3$}:
$$\fh_{1, PD}(x_1) = \frac{1}{n} \sum_{i=1}^n \fh(x_1, \xv_{2, 3}^{(i)})$$
}
}










\frame{
\frametitle{Interpretation: PD and ICE}
\begin{center}
\includegraphics[width=0.7\textwidth]{figure/bike-sharing-dataset01}
\end{center}

\begin{onlyenv}
\only<1>{
\begin{itemize}
\item
  \textbf{ICE curve:} Visualize how the prediction of an
  \textbf{individual observation} changes if the feature value is
  changed.\\
  \(\Rightarrow\) ICE is a local interpretation method (black curves).
\item
  \textbf{PD plot:} Visualizes the \textbf{average effect of a feature},
  i.e., how the expected prediction changes if the feature value is changed.\\
  \(\Rightarrow\) PD plot is a global interpretation method (yellow curve).
\end{itemize}
}

\only<2>{
Insights from the Bike Sharing Dataset:
\begin{itemize}
  \item Averaging ICE curves yields the PD plot (yellow curve).
  \item Parallel ICE curves across all observations indicate a very homogeneous effect.
  \item Higher temperature $\Rightarrow$ higher number of bike rentals.
  \item Bike rentals decrease at temperatures of more than $\approx 27^\circ$C.
\end{itemize}
}
\end{onlyenv}
}


\frame{
\frametitle{Categorical Features}

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/pdp_ice_cat}
\end{center}

\begin{itemize}
\item The PD plot visualizes the expected prediction (and the distribution of predictions, e.g., in a box plot) if all observations would belong to a certain category level.
\item The ICE plot connects the predictions of individual observations to identify heterogenous effects between categories (here: lines between SPRING and SUMMER are heterogenous).\\
\textbf{Note:} For nominal categories, the different categories do not have an ordering but could be compared pairwise.
\end{itemize}

}

\begin{vbframe}{2D Partial Dependence}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure/bike-sharing-dataset02.png}
\end{center}

\begin{itemize}
 \item Humidity and temperature interact with each other.
 \item The number of bike rentals is especially high when humidity is below 75 percent and temperature is between 15 $^{\circ}$C and 27 $^{\circ}$C
\end{itemize}


% \framebreak
%
% add datapoints to previous figure and mention convex hull (see pdp package)

\end{vbframe}

\endlecture
\end{document}
