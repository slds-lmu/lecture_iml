\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/ale_plot.pdf}
\newcommand{\learninggoals}{
\item Understand ALE plots
}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\lecturechapter{Accumulated Local Effect (ALE) plot}
\lecture{Interpretable Machine Learning}
%
% \frame{
% \frametitle{Motivation}
%
% \begin{columns}[T]
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figure_man/ale_scatter}
% \end{column}
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figure_man/ale_pdplot}
% \end{column}
% \end{columns}
%
% \only<1>{
% \textbf{Recall:} In case of strongly correlated features $x_1$ and $x_2$, partial dependence (PD) plots \textbf{average predictions} of artificial data instances that are unlikely in reality (green).
% This can lead to biased estimates.
%
% \textbf{Question:} Can we avoid the extrapolation issue by averaging only over points in an appropriate neighborhood? % of a specific point $x_1$? %, i.e., using the conditional distribution at
% }
%
% \only<2>{
%
% \textbf{Answer:} Yes, we could. Marginal plots (M plots) do this by averaging (i.e., marginalizing) over the conditional distribution $\P(\xv_2|x_1)$.
%
% \textbf{But:} M plots introduce the omitted-variable bias (OVB) issue, i.e., an
% M plot always includes the marginal effect of other dependent features.\\
% $\Rightarrow$ M plots are useless to assess the main effect of a feature.
%
% }
% }

\frame{
\frametitle{Motivation}

% \begin{center}
% \includegraphics[width=0.7\textwidth]{figure_man/PD_M.jpg}\\
% \tiny{Source: Figure taken from Apley et al. (2020)}
% \end{center}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\vspace*{-1em}
\centering
\textbf{a) PD plot}
\includegraphics[width=\textwidth]{figure/ale_pdplot}
\end{column}
\begin{column}{0.5\textwidth}
\vspace*{-1em}
\textbf{b) M plot}
\centering
\includegraphics[width=\textwidth]{figure/ale_mplot}
\end{column}
\end{columns}

\begin{enumerate}
%\item[a)] PD plot $\fh_{1, PD}(x_1) = \hat{\E}_{\xv_2} \left( \fh(x_1, \xv_2) \right) = \frac{1}{n} \sum_{i=1}^n \fh(x_1, \xv_2^{(i)})$
%\item[b)] M plot $\fh_{1, M}(x_1) = \hat{\E}_{\xv_2|\xv_1} \left( \fh(x_1, \xv_2) \middle| \xv_1\right) = \frac{1}{|N(x_1)|} \sum\limits_{i \in N(x_1)} \fh(x_1, \xv_2^{(i)})$, where $N(x_1) = \{i: x_1^{(i)} \in [x_1 - \epsilon, x_1 + \epsilon]\}$ is an index set referring to observations in an appropriate neighborhood of feature value $x_1$.
\item[a)] PD plot $\E_{\xv_2} \left( \fh(x_1, \xv_2) \right)$ is estimated by $ \fh_{1, PD}(x_1) = \frac{1}{n} \sum\limits_{i=1}^n \fh(x_1, \xv_2^{(i)})$
\item[b)] M plot $\E_{\xv_2|\xv_1} \left( \fh(x_1, \xv_2) \middle| \xv_1\right)$ is
estimated by
\centerline{$\fh_{1, M}(x_1) = \displaystyle\frac{1}{|N(x_1)|} \sum\limits_{i \in N(x_1)} \fh(x_1, \xv_2^{(i)}),$}
where $N(x_1) = \{i: x_1^{(i)} \in [x_1 - \epsilon, x_1 + \epsilon]\}$ is an index set referring to observations in an appropriate neighborhood of feature value $x_1$.
\end{enumerate}

}

\begin{vbframe}{Motivation}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figure/ale_scatter_grid}
\end{column}
\begin{column}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figure/ale_pdplot}
\end{column}
\end{columns}

%\begin{center}
%\includegraphics[width=0.8\textwidth]{figure_man/pd_grid}
%\tiny{Source: Figure taken from Apley et al. (2020)}
%\end{center}

In case of strongly correlated features $x_1$ and $x_2$, partial dependence (PD) plots \textbf{average predictions} of artificial data instances that are unlikely in reality (red).

\lz

\textbf{But:} This can be an undesired property if the model contains interactions and one is interested in interpreting effects w.r.t. the data distribution.

%\lz

%\textbf{Question:} Can we avoid the extrapolation issue by averaging only over points in an appropriate neighborhood? % of a specific point $x_1$? %, i.e., using the conditional distribution at

\framebreak

Example what can go wrong with PDPs and NNs

\framebreak

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figure/ale_scatter}
\end{column}
\begin{column}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figure/ale_mplot}
\end{column}
\end{columns}

%\textbf{Answer:} Yes, we could.
%\textbf{Recall:} Marginal plots (M plots) do this by averaging (i.e., marginalizing) over the conditional distribution $\P(\xv_2|x_1)$.
%\textbf{Recall:}
Marginal plots (M plots) average (i.e., marginalize) predictions over the conditional distribution $\P(\xv_2|x_1)$, i.e., by definition, they do not average predictions of observations outside the data distribution.

\lz

\textbf{But:} M plots introduce the omitted-variable bias (OVB) issue, i.e., an
M plot always includes the marginal effect of other dependent features.\\
$\Rightarrow$ M plots are useless to assess the main effect of a feature.
% in case of dependent features.

%M plots average over the conditional distribution $x_2|x_1$. However, the M plot of $x_1$ also includes the marginal effect of other dependent features $\Rightarrow$ We don't want this.
% do not include the marginal effect of the considered feature but
% mixture of its marginal effect and the marginal effects of all dependent features
\end{vbframe}



\frame{
\frametitle{Motivation: PD vs. M plot}
%\vspace*{-\topsep}
%\vspace*{-3\lineskip}

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/pd_vs_mplot}
\end{center}

\textbf{Illustration:}
We fitted a linear model on a simulated dataset with 500 i.i.d. observations, features $x_1, x_2 \sim N(0,1)$ with $Cor(x_1, x_2) = 0.9$ and $$y = -x_1 + 2 \cdot x_2 + \epsilon, \; \epsilon \sim N(0,1).$$

\textbf{Results:} The M plot of $x_1$ is a mixture of the marginal effect of $x_1$ and the marginal effect of all other dependent features (here: $x_2$).
}

\begin{vbframe}{Idea: Integrating Partial Derivatives}

\textbf{Idea:} To remove unwanted effects of other features, we take the partial derivative (local effect) of the prediction function with respect to a feature and integrate (accumulate) it with respect to the same feature.

\lz

\textbf{Example:}
\begin{itemize}
\item Consider an additive prediction function: $$\fh(x_1, x_2) = 2x_1 + 2x_2 - 4x_1 x_2$$
\item Partial derivative of $\fh$ with respect to $x_1$:
$\frac{\partial \fh(x_1, x_2)}{\partial x_1} = 2 - 4x_2$
\item Integral of the partial derivative:
$$\int_{z_0}^{x} \frac{\partial \fh(x_1, x_2)}{\partial x_1} dx_1 = \left[2x_1 - 4x_1 x_2\right]_{z_0}^{x}$$
\item We removed the main effect of $x_2$, which was our goal.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Accumulated Local Effects (ALE)}
%\begin{columns}[T]
%\begin{column}{0.65\textwidth}
ALE plots use the idea of integrating partial derivatives and were developed to overcome the extrapolation issue of PD plots and the OVB issue of M plots when features are dependent (Apley, 2020).
%\end{column}
%\begin{column}{0.35\textwidth}
%\centering
%\includegraphics[width=\textwidth]{figure_man/ale.jpg}\\
%\tiny{Source: Figure taken from Apley et al. (2020)}
%\end{column}
%\end{columns}

%Let $z_0$ denote the minimum of observed values of feature $\xv_S$, i.e., $z_0 = \min(\xv_S)$. All complementary features are denoted by $\xv_C$.
\lz

Let $z_0 = \min(\xv_S)$ be the minimum of observed values of feature $\xv_S$. All complementary features are denoted by $\xv_C$. ALE plots are based on
\begin{enumerate}
\item estimating the local effect $\frac{\partial \fh(x_S, \xv_C)}{\partial x_S}$ at $(x_S = z_S, \xv_C)$.
\item averaging the local effects across all values of $\xv_C$, i.e., over the conditional distribution $\P(\xv_C|x_S)$ similar to M plots.\\
$\Rightarrow$ Avoids the extraploation issue.
\item accumulating/integrating the averaged local effects over all values of $z_S$ from $z_0 := \min(\xv_S)$ up to a specific value $x \sim \P(x_S)$.\\
$\Rightarrow$ Estimates the main effect of $\xv_S$ and avoids the OVB issue.
\end{enumerate}

\footnote[frame]{Apley, Daniel W., and Jingyu Zhu (2020). Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4: 1059-1086.}
\end{vbframe}
%\begin{itemize}
  %\item Accumulated Local Effect (ALE) plots were developed by Apley (2020) to overcome the extrapolation issue of PD plots and the OVB issue of M plots when features are dependent.
  %. \item ALEs are an alternative to PD that do not suffer from extrapolation
  %\item PD and ALE reduce the complex prediction function $\hat{f}$ to a function that depends on only one (or two) features.
  %\item They both do that by averaging the effects of the other features.
  %\item We can see the main differences of PD vs. ALE if we look on their formulas.
  %\item Before, we explain the main idea of ALE.
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{First Order ALE}
The uncentered first order ALE $\tilde{f}_{S, ALE}(x)$ at point $x$ is defined as:
$$
\begin{aligned}
\tilde{f}_{S, ALE}(x) &= \int_{z_{0}}^{x} \E_{\xv_C \vert x_S} \left(\frac{\partial \fh(x_S, \xv_C)}{\partial x_S} \bigg \vert x_S = z_S \right) dz_S \\
&= \int_{z_{0}}^{x} \left( \int_{-\infty}^{\infty}  \frac{\partial \fh(z_S, \xv_C)}{\partial z_S} d\P(\xv_C | z_S) \,   \right) dz_S
\end{aligned}
$$

A constant, i.e., the average of the uncentered ALE curve is substracted such that the centered ALE curve $f_{S, ALE}(x)$ has a mean of zero with respect to the marginal distribution of $x_S$:

$$
\begin{aligned}
f_{S, ALE}(x) = \tilde{f}_{S, ALE}(x) - \underbrace{\int_{-\infty}^{\infty}\tilde{f}_{S, ALE}(x_S) \, d\P(x_S)}_{:= const}
\end{aligned}
$$

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
  \frametitle{PD vs. ALE}
    \textbf{PD:}
    \centerline{$
    \begin{aligned}
      \hspace{2cm} f_{S, PD}(x_S) &= \E_{\xv_C} \left( \fh(x_S, \xv_C) \right) = \int_{-\infty}^{\infty} \fh(x_S, \xv_C) \, d\P(\xv_C)
    \end{aligned}
    $} \\
    \textbf{ALE:}
    \centerline{$
    \begin{aligned}
      \only<1>{\hspace{1cm}
       f_{S, ALE}(x)
%&= \int_{z_{0}}^{x} \E_{\xv_C \vert x_S} \left(\textcolor{purple}{\frac{\partial \fh(x_S, \xv_C)}{\partial x_S}} \textcolor{blue}{\bigg \vert x_S = z_S} \right) dz_S - const%\\
 &= \int_{z_{0}}^{x} \left( \int_{-\infty}^{\infty}  \textcolor{purple}{\frac{\partial \fh(x_S, \xv_C)}{\partial x_S}} d \textcolor{blue}{\P(\xv_C | z_S) } \,   \right) dz_S - const
}
    \only<2>{\hspace{1cm}
     f_{S, ALE}(x)
%&= \textcolor{blue}{\int_{z_{0}}^{x}} \E_{\xv_C \vert x_S} \left(\frac{\partial \fh(x_S, \xv_C)}{\partial x_S} \bigg \vert x_S = z_S \right) \textcolor{blue}{dz_S} - const%\\
  &= \textcolor{blue}{\int_{z_{0}}^{x}} \left( \int_{-\infty}^{\infty}\frac{\partial \fh(x_S, \xv_C)}{\partial x_S} d \P(\xv_C | z_S)  \,   \right) \textcolor{blue}{dz_S} - const
}
\only<3>{\hspace{1cm}
     f_{S, ALE}(x)
%&= \textcolor{blue}{\int_{z_{0}}^{x}} \E_{\xv_C \vert x_S} \left(\frac{\partial \fh(x_S, \xv_C)}{\partial x_S} \bigg \vert x_S = z_S \right) \textcolor{blue}{dz_S} - const%\\
  &= \int_{z_{0}}^{x} \left( \int_{-\infty}^{\infty}\frac{\partial \fh(x_S, \xv_C)}{\partial x_S} d \P(\xv_C | z_S)  \,   \right) dz_S - \textcolor{blue}{const}
}
  \end{aligned}
    $}
    \lz
    \begin{itemize}
    \item<1-> PD directly averages predictions over the marginal distribution.
    \only<1>{
    \item Difference 1: ALE averages the \textcolor{purple}{change of predictions} (via the partial derivative) over the \textcolor{blue}{conditional distribution at $x_S = z_S$}.
    }
    \only<2-3>{
    \item Difference 1: ALE averages the change of predictions (via the partial derivative) over the conditional distribution at $x_S = z_S$.
    }
    \only<2>{
    \item Difference 2: ALE \textcolor{blue}{integrates over $z_S$} the partial derivatives of feature $S$ to isolate the effect of the feature of interest and to remove the main effect of dependent features.
    }
    \only<3>{
    \item Difference 2: ALE integrates over $z_S$ the partial derivatives of feature $S$ to isolate the effect of the feature of interest and to remove the main effect of dependent features.
    }
    %\item Difference 2: ALE has an additional \textcolor{blue}{integral over $z_S$}. Instead of directly averaging the predictions, the ALE method calculates the prediction differences conditional on features S and integrates the derivative over features $S$ to estimate the effect. The derivative (or interval difference) isolates the effect of the feature of interest and blocks the effect of correlated features.
    %}
    \item<3-> Difference 3: ALE is \textcolor{blue}{centered} so that $\E_{x_S} \left( f_{S, ALE}(x) \right) = 0.$
    \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{ALE Estimation}

\begin{itemize}
  \item The partial derivative can not be computed for all models, e.g., for tree-based methods such as the random forests.
  \item The partial derivative of the prediction function is approximated by computing finite differences of predictions within a set of $K$ intervals across the range of observed feature values $\xv_S$:
  $$
  \begin{aligned}
  x \in [\min(\xv_S), \max(\xv_S)] \iff &x \in [z_{0, S}, z_{1, S}] \\
  \lor &x \in \; ]z_{1, S}, z_{2, S}] \\
  &\dots \\
  \lor &x \in \; ]z_{K-1, S}, z_{K, S}]
  \end{aligned}
  $$
  \item A simple way to create $K$ intervals for feature $\xv_S$ is to use its quantile distribution with $K-1$ quantiles as interval bounds $z_{1,S}, \dots, z_{K-1,S}$ (not counting the 0\% and 100\% quantiles).
  %A simple way of creating $K$ intervals for the value range of $\xv_S$ is to use $K-1$ quantiles as interval bounds $z_1, \dots, z_{K-1}$ (not counting the 0\% and 100\% quantiles).
\end{itemize}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{2-D Illustration}

\centerline{\includegraphics[width=0.9\textwidth]{figure/ale_interval}}

%Compared to PD, ALE avoids averaging predictions of unlikely data instances and blocks the effect of other correlated features.
\begin{itemize}
\item We divide the feature of interest into intervals (vertical lines).
\item For all data instances within an interval, we compute the \textbf{difference} \textbf{in the prediction} when we replace the feature value with the upper and lower interval bound while keeping other feature values unchanged (blue lines/points).
\item These \textbf{finite differences} approximate the local effect and are later accumulated and centered to produce the ALE plot.
\end{itemize}

\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{2-D Illustration}
\centerline{\includegraphics[width=0.9\textwidth]{figure/ale_interval}}

 \begin{itemize}
  %\item Now let us define it more formalized.
  \item Consider the $i$-th observation $\xi = (x_S^{(i)}, \xi_C)$ for which $x_S^{(i)}$ is located within the $k$-th interval of $\xv_S$, i.e., $x_S^{(i)} \in \; ]z_{k-1, S}, z_{k, S}]$.
  \item We substitute $x_S^{(i)}$ by the right and left interval bounds while all other feature values in $\xi_C$ are kept constant.
  \item The finite differences correspond to $\fh(z_{k, S}, \xi_C) - \fh(z_{k-1, S}, \xi_C)$. %and approximates the local effect of the $i$-th observation.
\end{itemize}

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{2-D Illustration}
\centerline{\includegraphics[width=0.9\textwidth]{figure/ale_interval}}

 \begin{itemize}
  \item The local effect of feature $S$ within each interval is estimated by averaging all observation-wise finite differences. This is an approximation of the inner integral, which averages the local effect with respect to the conditional distribution of $\xv_C$ on $\xv_S$.
  \item Summing up the local effects of all intervals up to the point of interest estimates the outer integral.
\end{itemize}

\end{vbframe}
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% \begin{vbframe}{3-D Illustration}
%
% Consider the following data generating process:
%
% $$
% \begin{gathered}
% u_1, u_2 =  \{-10, -9.9, -9.8, \dots, 9.8, 9.9, 10\} \\
% n_1, n_2 \stackrel{iid}{\sim}  N(0, 1) \\
% \varepsilon \stackrel{iid}{\sim} N(0, 3) \\
% \\
% x_1 = u_1 + n_1 \\
% x_2 = u_2 + n_2 \\
% y = 100  \left[ \frac{\partial^2 \left[ \frac{1}{1 + exp(-x_1)} \right] }{\partial x_1 \partial x_1} \right] + 2 x_2 + \varepsilon
% \end{gathered}
% $$
%
% \framebreak
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/3D01.png}
% \end{center}
%
% 3D visualization of the data, the predictions made by the predictive model (blue net) and 10 intervals for the value range of $x_1$.
%
% \framebreak
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/3D02.png}
% \end{center}
%
% Zooming in on the interval $x_1 \in [4.198, 6.249]$.
%
% \framebreak
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/3D03.png}
% \end{center}
%
%
% We substitute each observation`s $x_1$ value by the right and left interval boundaries, predict and take the difference.
%
% \framebreak
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/3D04.png}
% \end{center}
%
% For each observation, we receive a change in prediction when traversing the interval from the left to the right boundary.
%
% \end{vbframe}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{vbframe}{3-D Illustration}
% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/3D05.png}
% \end{center}
%
% We integrate over the conditional distribution of $x_2$ on $x_1$ by averaging all observation-wise finite differences inside each interval.
%
% \framebreak
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/3D06.png}
% \end{center}
%
%
% We repeat the same procedure for every interval. The first order ALE corresponds to adding up all average interval-wise finite differences and substracting the centering constant.
%
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{ALE Estimation: Formula}

Let $k_S(x)$ denote the interval index a feature value $x \in \xv_S$ falls in. $n_S(k)$ denotes the number of observations inside the $k$-th interval of $\xv_S$. The estimated uncentered first order ALE $\hat{\tilde{f}}_{S, ALE}(x)$ at point $x$ corresponds to:
$$
\begin{aligned}
\hat{\tilde{f}}_{S, ALE}(x) = \sum_{k = 1}^{k_S(x)}\frac{1}{n_S(k)}\sum_{i: \; x_S^{(i)} \in \; ]z_{k-1, S}, z_{k, S}]}\left[\fh(z_{k, S}, \xi_C) -\fh(z_{k-1, S}, \xi_C)\right]
\end{aligned}
$$

In order to receive the centered ALE estimate $\hat{f}_{S, ALE}(x)$ at point $x$, we substract the average of the estimated uncentered ALE curve:

$$
\begin{aligned}
\hat{f}_{S, ALE}(x) = \hat{\tilde{f}}_{S, ALE}(x) - \frac{1}{n}\sum_{i = 1}^n \hat{\tilde{f}}_{S, ALE}(x_S^{(i)})
\end{aligned}
$$

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{ALE Estimation: Algorithm}

\begin{enumerate}
	\item \textbf{Sampling:} Create $K$ intervals for the value range of $\xv_S$.
	\item For each interval $k \in 1, \dots, k_S(x)$:
	  \begin{itemize}
	  \item Select the subset of observations inside the $k$-th interval.
	  \item \textbf{Intervention:} Replace the observation's feature value $x_S$ with the value of the lower and upper interval bound for each observation inside the $k$-th interval.
	  \item \textbf{Prediction:} Compute the observation-wise finite difference inside $k$-th interval and average them to estimate the interval-wise local effects.
	  \end{itemize}
  \item \textbf{Aggregation:} Accumulate the interval-wise local effects up to the point of interest $x$ to estimate the uncentered ALE and center it.
\end{enumerate}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Extrapolation and the ALE}
\begin{itemize}
\item Remember that there are two sources of extrapolation:
  \begin{enumerate}
  \item the model predicts in regions where it was not trained, and
  \item the Monte Carlo integral corresponds to an integration w.r.t. a uniform distribution, instead of the data distribution.
  \end{enumerate}
\item Assuming that all interval bounds are close to the corresponding observed values, ALEs are robust to both sources of extrapolation
  \begin{enumerate}
    \item The trained model does not predict in regions that are far away from the training data. %where it was not trained.
    \item We integrate w.r.t. the conditional distribution of $\xv_C$ on $x_S$ instead of the marginal distribution of $\xv_C$, because we only average the finite differences inside a single interval.
  \end{enumerate}
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{Bike Sharing Dataset: First Order ALE}

The PD (left) often looks very similar to the (centered) first order ALE (right) but on a different scale. In the case of correlated features, the ALE is a better option due to the PD's extrapolation issues.


\begin{center}
\includegraphics[width=0.9\textwidth]{figure/ale1d}
\end{center}


\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Bike Sharing Dataset: Second Order ALE}

It is possible to estimate ALEs of higher order, e.g., second order ALEs. As opposed to the bivariate PD, the second order ALE corresponds to an estimate of the interaction between two features only, i.e., first order effects have been substracted.

\vspace{0.1cm}

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/ale2d}
\end{center}

\end{vbframe}


\endlecture
\end{document}
