\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/feature-effect}
\newcommand{\learninggoals}{
\item Introduction to feature effects
\item Understand the steps involved in constructing ICE plots
%\item Understand how to interpret ICE curves and PD plots
}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\lecturechapter{Individual Conditional Expectation (ICE) Plot}
\lecture{Interpretable Machine Learning}



\begin{vbframe}{Feature Effects}

For a LM without interactions, the effect of a feature $x_j$ is quantified by its associated coefficient $\hat\beta_j$ and does not vary across observations $\xi$:

%the prediction of any observation $\xi$ is %can be expressed by %explained by its marginal feature effects
%prediction of an observation $\xi$ can be explained by the individual main effects, e.g.:
$$\fh(\xi) = \hat\beta_0 + x^{(i)}_1\hat\beta_1 + \dots + x^{(i)}_p\hat\beta_p.$$

%Here,

%Here, the marginal effect of a feature $x_j$ does not vary across observations and is quantified by its associated coefficient $\hat\beta_j$. %, which explains how a feature affects the model prediction.

%The $\hat\beta$-coefficients are constant across different observations.
%It is sufficient to consider a feature's $\hat\beta$ coefficient as marginal effect as it provides an understanding how a feature affects the model prediction. % on average.

%\lz
\textbf{Example}: %Visualizing the marginal effect of a LM (left) and a GAM (right) with a single feature (temperature) to predict the number of bike rentals.
The feature effect of a LM (left) or GAM (right) visualizes the relationship (i.e., marginal effect) of a single feature (here: temperature) on the prediction (here: number of bike rentals) while ignoring all other features.
For the GAM, the linear terms $x_j\hat\beta_j$ are replaced by more flexible non-linear functions $f_j(x_j)$ estimated via splines.
%Marginal effects of a LM with features temperature (\texttt{temp}) and \texttt{season} to predict the number of bike rentals (\texttt{cnt}).

\centering
%\includegraphics[width=0.75\textwidth, trim=0cm 0.56cm 0cm 0.08cm, clip]{figure_man/lm_main_effects}
\includegraphics[width=0.75\textwidth, trim=0cm 0.1cm 0cm 0cm, clip]{figure/lm_main_effects}

\end{vbframe}

%
% \begin{vbframe}{Feature Effects}
%
% %If the model contains interactions, the global effect is not enough as the
% %If the model contains interactions, there is a modifying effect for certain observations that changes the slope of the regression line (in case of LMs) or the shape of the partial effect curve (in case of GAMs).
% %(in case of LMs) or the shape of the partial effect curve (in case of GAMs).
% %If the model contains interactions, the slope of the regression line (in case of LMs) or the shape of the partial effect curve (in case of GAMs) will be different for certain observations.
% If the model contains interactions, the functional shape of the estimated feature effect will usually be different for certain observations.
% \lz
%
% \textbf{Example}: If we include the interaction \texttt{temp*season},
% %the marginal effect of \texttt{temp} will depend on \texttt{season}. That is,
% observations belonging to a certain category of \texttt{season} will have different marginal effects (slopes) for \texttt{temp}.
% %, which results in a different slope for the feature temp at each category of the feature season.
% %so that each category of season yields a different slope for temp.
%
% {\centering\includegraphics[width=0.9\textwidth, trim=0cm 0.1cm 0cm 0cm, clip]{figure_man/lm_interaction}}
%
% %\textbf{Note}: In case of interactions between two continuous features, we would obtain multiple regression lines with different slopes.
% %The marginal effect can then be visualized in a 2d effect plot or
%
% \end{vbframe}



\begin{vbframe}{Feature Effects}

In ML, \textbf{feature effect methods} visualize or quantify the marginal contribution of a feature of interest towards the model predictions. %marginal contribution of a feature to the model \textbf{prediction}. % $\hat{y} = \fh(\xi[])$. effect (e.g., the average relationship) between a
\begin{itemize}
%\tightlist
\item Methods: PD Plots, ICE curves, ALE plots
\item Similar to regression coefficients (LMs) or Splines (GAMs).
\end{itemize}

\centerline{\includegraphics[width=\textwidth]{figure/feature-effect.pdf}}

\hspace{8px} \small Individual (curves) \hspace{2px}
$\xrightarrow[]{\text{aggregate}}$ \hspace{2px} Global (curve) \hspace{2px}
$\xrightarrow[]{\text{aggregate}}$ \hspace{2px} Global (number)


\end{vbframe}


% \begin{vbframe}{Marginal Plots}
% Marginal plots (also known as M plots, bivariate plots or conditional plots)
%
% %which types of questions can be answered
%
% %Why visualizations of feature effects are meaningful in ML.
%
% %\lz
%
% \footnote[frame]{Liu, Xiaoyu, et al. "Model interpretation: A unified derivative-based framework for nonparametric regression and supervised machine learning." arXiv preprint arXiv:1808.07216 (2018).}
% \end{vbframe}


\begin{vbframe}{Individual Conditional Expectation (ICE)}

%Consider an index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$.
%\lz
\begin{columns}[T]
\begin{column}{0.18\textwidth} % 0.2 4.26cm
\includegraphics[page=1, trim=0cm 0.35cm 4.53cm 0.35cm, clip, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.82\textwidth}
%Assume each observation $\xi$ can be partitioned into $\xi_S$ and $\xi_C$ containing only feature values from the index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$, respectively.
%Assume each observation $\xi$ can be partitioned into $\xi_S$ and $\xi_C$ containing only feature values addressed by the feature's index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$, respectively.
%Assume each observation $\xi$ can be partitioned into $\xi_S$ and $\xi_C$ containing values from features addressed by the index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$, respectively.
Partition each observation $\xi$ into $\xi_S$ and $\xi_C$ containing only feature values addressed by the feature's index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$, respectively.
\lz

Individual conditional expectation (ICE) curves visualize how the model prediction of individual observations $\xi$ change by varying the feature values indexed by $S$ while keeping all other features indexed by $C$ fixed:
%by replacing the feature values $\xv_S^{(i)}$ with other values $\xv_S$ while keeping all other features in $\xi_C$ fixed:
%by varying the feature values in $\xv_S$ while keeping all other features in $\xi_C$ fixed:
$$\fh_{S}^{(i)}(\xv_S) = \fh(\xv_S, \xi_C)$$

Here, $\fh_{S}^{(i)}(\xv_S)$ refers to the prediction of the $i$-th observation in which feature values indexed by $S$ were replaced with $\xv_S$.

\textbf{Note:} In practice, $\xv_S$ consists of one or two features.

\end{column}
\end{columns}

% \begin{columns}[T, totalwidth=\textwidth]
% \begin{column}{0.6\textwidth}


% \end{column}
% \begin{column}{0.4\textwidth}
% %\begin{center}
% \centerline{\includegraphics[width=\textwidth]{figure_man/ice_bike10obs}}
% %\end{center}
% %\vspace{-0.3cm}
% \scriptsize{\textbf{Figure:} ICE Curves of 10 observations of the bike sharing dataset. Each line displays the change in prediction for a single observation due to varying the feature temperature.\par}
%
% \end{column}
% \end{columns}
\footnote[frame]{Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. (2013). Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation, 1-22. https://doi.org/10.1080/10618600.2014.907095}
\end{vbframe}

% \begin{vbframe}{Individual Conditional Expectation (ICE)}
%
% Steps to create an ICE curve of an observation regarding a single feature $x_S$ according to the \textbf{SIPA} framework:
%
% \begin{enumerate}
% \item \textbf{Sampling:} Choose grid points along $x_S$.
% \item For each grid point:
% \begin{itemize}
% \item \textbf{Intervention:} Replace the original feature value $x_S$ with the current grid value.
% \item \textbf{Prediction:} Get the model prediction with replaced feature value $x_S$.
% \end{itemize}
% \item \textbf{Aggregation:} none.
% \item \textbf{Visualization:} Draw a curve per observation with the grid points on the x-axis and the prediction on the y-axis.
% \end{enumerate}
%
% \footnote[frame]{Scholbeck, C. A., Molnar, C., Heumann, C., Bischl, B., and Casalicchio, G. (2019). Sampling, Intervention, Prediction, Aggregation: A Generalized Framework for Model Agnostic Interpretations. ECML PKDD 2019. (pp. 205-216).}
% \end{vbframe}

% \begin{vbframe}{Individual Conditional Expectation (ICE)}
%
% \begin{columns}[T]
% \begin{column}{0.45\textwidth}
% \includegraphics[page=1, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
% \end{column}
% \begin{column}{0.55\textwidth}
%
% \end{column}
% \end{columns}
% \vspace*{\topsep}
%
% aa
% \end{vbframe}


\begin{vbframe}{Individual Conditional Expectation (ICE)}



\end{vbframe}

\begin{vbframe}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\includegraphics[page=2, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}

\end{column}
\end{columns}
\vspace*{\topsep}

\textbf{Grid points:}

For the $i$-th observation, we sample grid values $x_S^*$ (here: $x_S^* \in \{1, 2, 3\}$) along the feature of interest $x_S$ (here: $S = 1$) and replace the original feature value of the $i$-th observation by these grid values. \newline
$\Rightarrow$ We obtain new artificial data points for the $i$-th observation.


\framebreak

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\includegraphics[page=3, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=1, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
\vspace*{\topsep}

\textbf{Predict and visualize:}

For each artificially created data point of the $i$-th observation, we plot its prediction $\fh_{S}^{(i)}(x_S^*)$ vs. the corresponding grid value $x_S^*$, here $$\fh_{1}^{(i)}(x_1^*) = \fh(x_1^*, \xi_{2, 3}) \text{ vs. } x_1^* \in \{1, 2, 3\}.$$


\framebreak

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\includegraphics[page=4, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=2, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
\vspace*{\topsep}

\textbf{Predict and visualize:}

For each artificially created data point of the $i$-th observation, we plot its prediction $\fh_{S}^{(i)}(x_S^*)$ vs. the corresponding grid value $x_S^*$, here $$\fh_{1}^{(i)}(x_1^*) = \fh(x_1^*, \xi_{2, 3}) \text{ vs. } x_1^* \in \{1, 2, 3\}.$$

\framebreak

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\includegraphics[page=5, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=3, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
\vspace*{\topsep}

\textbf{Predict and visualize:}

For each artificially created data point of the $i$-th observation, we plot its prediction $\fh_{S}^{(i)}(x_S^*)$ vs. the corresponding grid value $x_S^*$, here $$\fh_{1}^{(i)}(x_1^*) = \fh(x_1^*, \xi_{2, 3}) \text{ vs. } x_1^* \in \{1, 2, 3\}.$$

\framebreak

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\includegraphics[page=5, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=3, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
\vspace*{\topsep}

\textbf{Interpretation:}

The resulting \textbf{ICE} curve visualizes for an \textbf{I}ndividual observation and \textbf{C}onditional on all other features (by keeping all other features constant) how the \textbf{E}xpected prediction changes.

\framebreak




\begin{columns}[T]
\begin{column}{0.45\textwidth}
\includegraphics[page=6, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=4, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
\vspace*{\topsep}

\textbf{Repeat for other observation:}

The ICE curve of observation $i=2$ connects all predictions at the corresponding grid values associated to the $i$-th observation.

\framebreak

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\includegraphics[page=7, trim=0cm 0.35cm 0.85cm 0.35cm, width=\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
%\begin{center}
\includegraphics[page=5, width=0.95\textwidth]{figure/ICE}
%\end{center}
\end{column}
\end{columns}
\vspace*{\topsep}

\textbf{Repeat for other observation:}

The ICE curve of observation $i=3$ connects all predictions at the corresponding grid values associated to the $i$-th observation.

\textbf{Definition:}

ICE curves involve plotting the pairs $ \{(x_S^{*^{(k)}}, \fh_{S}^{(i)}(x_S^*{^{(k)}})) \}_{k=1}^g $ for
\end{vbframe}


\begin{vbframe}{Comments}
\begin{itemize}
\item ICE curves show how different feature values of an observation affect its prediction $\Rightarrow$ local interpretation method.
\item Plotting ICE curves involves generating grid values $x_S^*$ that are visualized on the x-axis.
Common choices for grid values are
\begin{itemize}
\item equidistant values within the range of the considered feature,
\item randomly sampled values or quantile values from the observed feature values.
\end{itemize}
\item In contrast to an equidistant grid, the other two options preserve (approximately) the marginal distribution of the feature of interest.
$\Rightarrow$ Avoids unrealistic feature values for distributions with outliers.

\vspace{3pt}
\centering
\includegraphics[width=0.75\textwidth, trim=0cm 0cm 0cm 0cm, clip]{figure/sampling}

%Preferable for skewed distributions (with outliers) to avoid using unrealistic feature values.
% \begin{itemize}
% \item equidistant grid values:
% \item sub-sampled grid values:
% \item quantile grid values:
% \end{itemize}
\end{itemize}
\end{vbframe}

\endlecture
\end{document}
