\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/feature-effect}
\newcommand{\learninggoals}{
\item Introduction to feature effects
\item Understand the steps involved in constructing ICE plots
%\item Understand how to interpret ICE curves and PD plots
}

\lecturechapter{Individual Conditional Expectation (ICE) Plot}
\lecture{Interpretable Machine Learning}

\begin{frame}{Feature Effects}

LM without interaction quantifies effect of $x_j$ via coefficient $\hat\beta_j$ (constant across all observations $\xi$):

%the prediction of any observation $\xi$ is %can be expressed by %explained by its marginal feature effects
%prediction of an observation $\xi$ can be explained by the individual main effects, e.g.:
$$\fh(\xi) = \hat\beta_0 + x^{(i)}_1\hat\beta_1 + \dots + x^{(i)}_p\hat\beta_p.$$

%Here,

%Here, the marginal effect of a feature $x_j$ does not vary across observations and is quantified by its associated coefficient $\hat\beta_j$. %, which explains how a feature affects the model prediction.

%The $\hat\beta$-coefficients are constant across different observations.
%It is sufficient to consider a feature's $\hat\beta$ coefficient as marginal effect as it provides an understanding how a feature affects the model prediction. % on average.

%\lz
\textbf{Example}: %Visualizing the marginal effect of a LM (left) and a GAM (right) with a single feature (temperature) to predict the number of bike rentals.
Feature effect of LM (left) visualizes relationship of a single feature (here: temperature) on prediction (here: number of bike rentals) while ignoring all other features.
GAM (right) replaces linear terms $x_j\hat\beta_j$ of LM by non-linear functions $f_j(x_j)$ estimated via splines.
%Marginal effects of a LM with features temperature (\texttt{temp}) and \texttt{season} to predict the number of bike rentals (\texttt{cnt}).

\centering
%\includegraphics[width=0.75\textwidth, trim=0cm 0.56cm 0cm 0.08cm, clip]{figure_man/lm_main_effects}
\includegraphics[width=0.75\textwidth, trim=0cm 0.1cm 0cm 0cm, clip]{figure/lm_main_effects}

\end{frame}

%
% \begin{frame}{Feature Effects}
%
% %If the model contains interactions, the global effect is not enough as the
% %If the model contains interactions, there is a modifying effect for certain observations that changes the slope of the regression line (in case of LMs) or the shape of the partial effect curve (in case of GAMs).
% %(in case of LMs) or the shape of the partial effect curve (in case of GAMs).
% %If the model contains interactions, the slope of the regression line (in case of LMs) or the shape of the partial effect curve (in case of GAMs) will be different for certain observations.
% If the model contains interactions, the functional shape of the estimated feature effect will usually be different for certain observations.
% \lz
%
% \textbf{Example}: If we include the interaction \texttt{temp*season},
% %the marginal effect of \texttt{temp} will depend on \texttt{season}. That is,
% observations belonging to a certain category of \texttt{season} will have different marginal effects (slopes) for \texttt{temp}.
% %, which results in a different slope for the feature temp at each category of the feature season.
% %so that each category of season yields a different slope for temp.
%
% {\centering\includegraphics[width=0.9\textwidth, trim=0cm 0.1cm 0cm 0cm, clip]{figure_man/lm_interaction}}
%
% %\textbf{Note}: In case of interactions between two continuous features, we would obtain multiple regression lines with different slopes.
% %The marginal effect can then be visualized in a 2d effect plot or
%
% \end{frame}



\begin{frame}{Feature Effects}

In ML, \textbf{feature effect methods} visualize or quantify the marginal contribution of a feature of interest towards the model predictions. %marginal contribution of a feature to the model \textbf{prediction}. % $\hat{y} = \fh(\xi[])$. effect (e.g., the average relationship) between a
\begin{itemize}
%\tightlist
\item Methods: PD Plots, ICE curves, ALE plots
\item Similar to regression coefficients (LMs) or Splines (GAMs).
\end{itemize}

\centerline{\includegraphics[width=\textwidth]{figure/feature-effect.pdf}}

\hspace{8px} \small Individual (curves) \hspace{2px}
$\xrightarrow[]{\text{aggregate}}$ \hspace{2px} Global (curve) \hspace{2px}
$\xrightarrow[]{\text{aggregate}}$ \hspace{2px} Global (number)


\end{frame}


% \begin{frame}{Marginal Plots}
% Marginal plots (also known as M plots, bivariate plots or conditional plots)
%
% %which types of questions can be answered
%
% %Why visualizations of feature effects are meaningful in ML.
%
% %\lz
%
% \footnote[frame]{Liu, Xiaoyu, et al. "Model interpretation: A unified derivative-based framework for nonparametric regression and supervised machine learning." arXiv preprint arXiv:1808.07216 (2018).}
% \end{frame}


\begin{frame}{Individual Conditional Expectation (ICE)}

%Consider an index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$.
%\lz
\begin{columns}[T]
\begin{column}{0.18\textwidth} % 0.2 4.26cm
\includegraphics[page=1, trim=0cm 0.35cm 4.53cm 0.35cm, clip, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.82\textwidth}
%Assume each observation $\xi$ can be partitioned into $\xi_S$ and $\xi_C$ containing only feature values from the index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$, respectively.
%Assume each observation $\xi$ can be partitioned into $\xi_S$ and $\xi_C$ containing only feature values addressed by the feature's index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$, respectively.
%Assume each observation $\xi$ can be partitioned into $\xi_S$ and $\xi_C$ containing values from features addressed by the index set $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$, respectively.
Partition $\xi$ into $\xi_S$ and $\xi_C$ containing only feature values indexed by $S \subseteq \{1, \dots, p\}$ and its complement $C = S^\complement$.
\lz

ICE curves visualize how prediction of an observation $\xi$ changes by varying its feature values in $S$ while keeping all other features in $C$ fixed:
%by replacing the feature values $\xv_S^{(i)}$ with other values $\xv_S$ while keeping all other features in $\xi_C$ fixed:
%by varying the feature values in $\xv_S$ while keeping all other features in $\xi_C$ fixed:
$$\fh_{S}^{(i)}(\xv_S) = \fh(\xv_S, \xi_C)$$

$\fh_{S}^{(i)}(\xv_S)$ is prediction of $i$-th observation where $\xi_S$ was replaced by $\xv_S$.

\textbf{Note:} In practice, $\xv_S$ consists of one or two features.

\end{column}
\end{columns}

% \begin{columns}[T, totalwidth=\textwidth]
% \begin{column}{0.55\textwidth}


% \end{column}
% \begin{column}{0.4\textwidth}
% %\begin{center}
% \centerline{\includegraphics[width=\textwidth]{figure_man/ice_bike10observation}}
% %\end{center}
% %\vspace{-0.3cm}
% \scriptsize{\textbf{Figure:} ICE Curves of 10 observations of the bike sharing dataset. Each line displays the change in prediction for a single observation due to varying the feature temperature.\par}
%
% \end{column}
% \end{columns}
\footnote[frame]{Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. (2013). Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation, 1-22. https://doi.org/10.1080/10618600.2014.907095}
\end{frame}

% \begin{frame}{Individual Conditional Expectation (ICE)}
%
% Steps to create an ICE curve of an observation regarding a single feature $x_S$ according to the \textbf{SIPA} framework:
%
% \begin{enumerate}
% \item \textbf{Sampling:} Choose grid points along $x_S$.
% \item For each grid point:
% \begin{itemize}
% \item \textbf{Intervention:} Replace the original feature value $x_S$ with the current grid value.
% \item \textbf{Prediction:} Get the model prediction with replaced feature value $x_S$.
% \end{itemize}
% \item \textbf{Aggregation:} none.
% \item \textbf{Visualization:} Draw a curve per observation with the grid points on the x-axis and the prediction on the y-axis.
% \end{enumerate}
%
% \footnote[frame]{Scholbeck, C. A., Molnar, C., Heumann, C., Bischl, B., and Casalicchio, G. (2019). Sampling, Intervention, Prediction, Aggregation: A Generalized Framework for Model Agnostic Interpretations. ECML PKDD 2019. (pp. 205-216).}
% \end{frame}

% \begin{frame}{Individual Conditional Expectation (ICE)}
%
% \begin{columns}[T]
% \begin{column}{0.4\textwidth}
% \includegraphics[page=1, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
% \end{column}
% \begin{column}{0.55\textwidth}
%
% \end{column}
% \end{columns}
% %\vspace*{\topsep}
%
% aa
% \end{frame}

\begin{frame}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.4\textwidth}
\includegraphics[page=2, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}

\end{column}
\end{columns}
%\vspace*{\topsep}

\textbf{Grid points:}

For $i$-th observation, sample grid values $x_S^*$ (here: $x_S^* \in \{1, 2, 3\}$) along feature of interest $x_S$ (here: $S = 1$) and replace original feature value of $i$-th observation with these grid values. \newline
$\Rightarrow$ We obtain new artificial data points for the $i$-th observation.

\end{frame}

\begin{frame}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.4\textwidth}
\includegraphics[page=3, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=1, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
%\vspace*{\topsep}

\textbf{Predict and visualize:}

For each artificially created data point of observation $i$, plot prediction $\fh_{S}^{(i)}(x_S^*)$ vs. grid value $x_S^*$: $$\fh_{1}^{(i)}(x_1^*) = \fh(x_1^*, \xi_{2, 3}) \text{ vs. } x_1^* \in \{1, 2, 3\}.$$

\end{frame}

\begin{frame}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.4\textwidth}
\includegraphics[page=4, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=2, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
%\vspace*{\topsep}

\textbf{Predict and visualize:}

For each artificially created data point of observation $i$, plot prediction $\fh_{S}^{(i)}(x_S^*)$ vs. grid value $x_S^*$: $$\fh_{1}^{(i)}(x_1^*) = \fh(x_1^*, \xi_{2, 3}) \text{ vs. } x_1^* \in \{1, 2, 3\}.$$
\end{frame}

\begin{frame}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.4\textwidth}
\includegraphics[page=5, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=3, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
%\vspace*{\topsep}

\textbf{Predict and visualize:}

For each artificially created data point of observation $i$, plot prediction $\fh_{S}^{(i)}(x_S^*)$ vs. grid value $x_S^*$: $$\fh_{1}^{(i)}(x_1^*) = \fh(x_1^*, \xi_{2, 3}) \text{ vs. } x_1^* \in \{1, 2, 3\}.$$
\end{frame}

\begin{frame}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.4\textwidth}
\includegraphics[page=5, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=3, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
%\vspace*{\topsep}

\textbf{Interpretation:}

Resulting \textbf{ICE} curve visualizes for \textbf{I}ndividual observations and \textbf{C}onditional on all other features (by keeping all other features constant) how \textbf{E}xpected prediction changes.
\end{frame}

\begin{frame}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.4\textwidth}
\includegraphics[page=6, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics[page=4, width=0.95\textwidth]{figure/ICE}
\end{column}
\end{columns}
%\vspace*{\topsep}

\textbf{Repeat for other observation:}

ICE curve $i=2$ connects all predictions at grid values associated to $i$-th observation.
\end{frame}

\begin{frame}{Individual Conditional Expectation (ICE)}

\begin{columns}[T]
\begin{column}{0.4\textwidth}
\includegraphics[page=7, trim=0cm 0.35cm 0.85cm 0.35cm, width=0.9\textwidth]{figure_man/ice_plot_demo}
\end{column}
\begin{column}{0.55\textwidth}
%\begin{center}
\includegraphics[page=5, width=0.95\textwidth]{figure/ICE}
%\end{center}
\end{column}
\end{columns}
%\vspace*{\topsep}

\textbf{Repeat for other observation:}

ICE curve $i=3$ connects all predictions at grid values associated to $i$-th observation.

\textbf{Definition:}

ICE curves involve plotting the pairs $ \{(x_S^{*^{(k)}}, \fh_{S}^{(i)}(x_S^*{^{(k)}})) \}_{k=1}^g $ for grid points $x_S^{*^{(k)}}$.
\end{frame}


\begin{frame}{Comments}
\begin{itemize}
\item ICE curves show how different feature values of an observation affect its prediction \\ $\Rightarrow$ local interpretation method.
\item Plotting ICE curves involves generating grid values $x_S^*$ that are visualized on the x-axis.
Common choices for grid values are
\begin{itemize}
\item equidistant values within the range of considered feature,
\item randomly sampled values or quantile values from observed feature values.
\end{itemize}
\item Except equidistant grid, the other options (approximately) preserve marginal distribution of feature of interest.
$\Rightarrow$ Avoids unrealistic feature values for distributions with outliers.

\vspace{3pt}
\centering
\includegraphics[width=0.75\textwidth, trim=0cm 0cm 0cm 0cm, clip]{figure/sampling}

%Preferable for skewed distributions (with outliers) to avoid using unrealistic feature values.
% \begin{itemize}
% \item equidistant grid values:
% \item sub-sampled grid values:
% \item quantile grid values:
% \end{itemize}
\end{itemize}
\end{frame}

\endlecture
\end{document}
