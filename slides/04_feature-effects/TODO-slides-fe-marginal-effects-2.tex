% See https://chatgpt.com/share/673b30a9-1820-8000-90b6-c7f6e955bd6e
\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath, amsfonts}
\usepackage{bm} % For bold math symbols
\usepackage{xcolor}
\usepackage{enumerate} % For numbered lists
\usepackage{hyperref}


\newcommand{\boldx}{\bm{x}}
\newcommand{\boldX}{\bm{X}}
\newcommand{\boldxS}{\bm{x}_{S}}
\newcommand{\boldxMinusS}{\bm{x}_{-S}}
\newcommand{\boldxMinusj}{\bm{x}_{-j}}

\newcommand{\boldxi}{\bm{x}^{(i)}}
\newcommand{\boldxMinusSi}{\bm{x}_{-S}^{(i)}}
\newcommand{\boldxMinusji}{\bm{x}_{-j}^{(i)}}

\newcommand{\boldhS}{\bm{h}_{S}}
\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}


\begin{document}

% Title Slide
% \begin{frame}
%   \titlepage
% \end{frame}

\lecturechapter{Marginal Effects}
\lecture{Interpretable Machine Learning}

% Learning Goals
\begin{frame}{Learning Goals}
\begin{itemize}
\item Understand why parameter-based interpretations are not always possible for parametric models.
\item Learn how marginal effects can be used to interpret such models.
\item Recognize the drawbacks of traditional marginal effects.
\item Discover model-agnostic applicability of forward marginal effects (FMEs).
\item Introduce unified FMEs for all feature types.
\item Learn about the Non-Linearity Measure (NLM) to assess deviation from linearity.
\item Explore Conditional Average Marginal Effects (cAMEs) for regional interpretations.
\end{itemize}
\end{frame}

%\section{Introduction}

% Introduction
\begin{frame}{Introduction}
\begin{itemize}
\item \textbf{Motivation}:
\begin{itemize}
  \item Machine learning models are powerful but often lack interpretability.
  \item Need for methods that provide interpretable insights similar to traditional statistical models.
\end{itemize}
\item \textbf{Challenges}:
\begin{itemize}
  \item Complex, non-linear prediction functions.
  \item Diverse feature types (continuous, categorical, mixed).
  \item Interactions and higher-order terms complicate interpretations.
\end{itemize}
\item \textbf{Objective}:
\begin{itemize}
  \item Develop model-agnostic methods to interpret machine learning models using Marginal Effects.
  \item Introduce a unified approach for both continuous and categorical features.
\end{itemize}
\end{itemize}
\end{frame}

% Interpretation of Simple Models
\begin{frame}{Interpretation of Simple Models}
\begin{itemize}
\item \textbf{Linear Models}:
\begin{itemize}
\item Change in $x_j$ by $\Delta x_j$ results in change in $y$ by $\Delta y = \Delta x_j \cdot \theta_j$
\item Model equation:
\[
y = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p + \epsilon
\]
\item Default interpretations correspond to $\Delta x_j = 1$, i.e., $\Delta y = \theta_j$
\item Assumes "ceteris paribus" (all other features held constant)
\end{itemize}
\item \textbf{Non-Linear Models with Interactions}:
\begin{itemize}
\item For models with higher-order or interaction terms, single coefficients are not sufficient:
\[
y = \theta_0 + \theta_{1} x_1^2 + \theta_{2} x_2^2 + \theta_{1,2} x_1 x_2 + \epsilon
\]
\item Marginal effect of $x_1$ varies with different values of $x_2$ (and vice versa)
\item Interactions depend on the values of other features
\end{itemize}
\end{itemize}
\end{frame}

%\section{Marginal Effects}

% Background on Marginal Effects
\begin{frame}{Marginal Effects}
\begin{itemize}
\item \textbf{Definition}:
\begin{itemize}
  \item Marginal Effects (MEs) measure the change in the predicted outcome due to a change in a feature.
\end{itemize}
\item \textbf{Types of MEs}:
\begin{itemize}
  \item \textbf{Derivative Marginal Effects (DMEs)}: Use derivatives; suitable for continuous, differentiable functions.
  \item \textbf{Forward Marginal Effects (FMEs)}: Use finite differences; applicable to non-differentiable functions.
\end{itemize}
\item \textbf{Limitations of DMEs}:
\begin{itemize}
  \item Not suitable for non-differentiable or piecewise constant functions (e.g., decision trees).
  \item Interpretation can be misleading for non-linear models.
\end{itemize}
\end{itemize}
\end{frame}

% Marginal Effects for Continuous Features
\begin{frame}{Marginal Effects for Continuous Features}
\begin{itemize}
\item \textbf{Derivative Marginal Effect (DME)}:
\[
\text{DME}_j(\xv) = \frac{\partial \widehat{f}(\xv)}{\partial x_j} \approx \frac{\widehat{f}(\xv + h_j \mathbf{e}_j) - \widehat{f}(\xv - h_j \mathbf{e}_j)}{2h_j}
\]
where $\mathbf{e}_j$ is the unit vector in the $j$-th direction.
\item \textbf{Forward Marginal Effect (FME)}:
\[
\text{FME}_j(\xv, h_j) = \widehat{f}(\xv_1, \dots, x_j + h_j, \dots, \xv_p) - \widehat{f}(\xv)
\]
\item \textbf{Example Model}:
\[
y = \theta_0 + \theta_{1} x_1^2 + \theta_{2} x_2^2 + \theta_{1,2} x_1 x_2 + \epsilon
\]
\begin{itemize}
\item Derivative ME of $x_1$:
\[
\text{DME}_1(\xv) = 2\theta_1 x_1 + \theta_{1,2} x_2
\]
\item Forward ME of $x_1$ with step size $h_1$:
\[
\text{FME}_1(\xv, h_1) = \theta_1 \left( (x_1 + h_1)^2 - x_1^2 \right) + \theta_{1,2} x_2 h_1
\]
\end{itemize}
\end{itemize}
\end{frame}

% Marginal Effects for Categorical Features
\begin{frame}{Marginal Effects for Categorical Features}
\begin{itemize}
\item \textbf{Traditional Approach}:
\begin{itemize}
\item Compute the change in prediction when the feature value changes from a reference category to another category.
\item For each observation, set the categorical feature to the reference category and record the change in prediction when changing it to every other category.
\end{itemize}
\item \textbf{Unified FME Definition for Categorical Features}:
\[
\text{FME}_{\xv, c_j} = \widehat{f}(c_j, \xv_{-j}) - \widehat{f}(\xv)
\]
\item \textbf{Advantages}:
\begin{itemize}
\item Removes ambiguity by treating categorical features similarly to continuous features.
\item Provides an exact change in prediction for a change in category.
\end{itemize}
\end{itemize}
\end{frame}


% NEW SLIDE: Limitations of AMEs in Non-Parametric Models
\begin{frame}{Limitations of AMEs in Non-Parametric Models}
\begin{itemize}
\item \textbf{Average Marginal Effects (AMEs)}:
\begin{itemize}
\item Widely used in logistic regression to interpret effects on the probability scale.
\item Provide an average effect across all observations.
\end{itemize}
\item \textbf{Variations}:
\begin{itemize}
\item Marginal Effects at the Mean (MEM).
\item Marginal Effects at Representative Values (MER).
\end{itemize}
\item \textbf{Issue with Non-Parametric Models}:
\begin{itemize}
\item AMEs assume a consistent effect across the feature space.
\item Non-parametric models (e.g., random forests, neural networks) can model complex, non-linear relationships.
\item Averaging effects can obscure important heterogeneities.
\end{itemize}
\end{itemize}
\end{frame}

% NEW SLIDE: Concrete Example Illustrating Limitations
\begin{frame}{Example: AMEs in Logistic Regression vs. Non-Parametric Model}
\begin{itemize}
\item \textbf{Logistic Regression Model}:
\begin{itemize}
\item Model: $\text{logit}(P(Y=1)) = \beta_0 + \beta_1 X$
\item AME provides average change in probability for a unit change in $X$.
\item Transformation is monotonic, so averaging makes sense.
\end{itemize}
\item \textbf{Non-Parametric Model (e.g., Decision Tree)}:
\begin{itemize}
\item Model can capture non-linear and interaction effects.
\item Effect of $X$ on $Y$ can vary greatly across different regions.
\item \textbf{AME may not reflect any individual's experience}.
\end{itemize}
% \item \textbf{Illustration}:
% \begin{figure}
% \centering
% \includegraphics[width=0.8\textwidth]{figure_man/derivative_me_error.png}
% \includegraphics[width=0.8\textwidth]{figure_man/forward_me_deviation.png}
% \caption{AME fails to capture the varying effects of $X$ on $Y$ in different regions of the feature space.}
% \end{figure}
\item \textbf{Conclusion}:
\begin{itemize}
\item Need for local or regional methods like FMEs and cAMEs to capture true effects.
\end{itemize}
\end{itemize}
\end{frame}

% Forward Difference vs. Derivative
\begin{frame}{Forward Difference vs. Derivative}
\begin{columns}[T]
\begin{column}{0.65\textwidth}
\begin{itemize}
\item Derivatives of non-linear functions vary significantly at different points $\rightarrow$ DME may not be reliable.
\item FME captures the actual change in the prediction due to a finite change in the feature.
\item FME is more suitable for non-linear and non-differentiable models.
\item Both methods may lose information about the function's behavior along the change.
\item \textbf{Visualization}:
\begin{itemize}
\item DME corresponds to the slope of the tangent at a point.
\item FME corresponds to the slope of the secant between two points.
\end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.35\textwidth}
\includegraphics[width=\textwidth]{figure_man/derivative_me_error.png}
\includegraphics[width=\textwidth]{figure_man/forward_me_deviation.png}
\end{column}
\end{columns}
\end{frame}

% Additive Recovery Property
\begin{frame}{Additive Recovery Property}
\begin{itemize}
\item Both DME and FME recover only terms involving the feature(s) of interest.
\item \textbf{Example}:
\[
\widehat{f}(\xv) = a x_1 + b x_2
\]
Then:
\begin{align*}
\text{DME}_1(\xv) &= a \\
\text{FME}_1(\xv, h_1) &= a h_1
\end{align*}
\item Effects of other features linked additively are removed.
\item This property allows focusing on the direct effect of the feature.
\end{itemize}
\end{frame}

% Unified Definition of FMEs
\begin{frame}{Unified Definition of FMEs}
\begin{itemize}
\item \textbf{For Continuous Features}:
\[
\text{FME}_{\xv, \boldhS} = \widehat{f}(\xv_S + \boldhS, \xv_{-S}) - \widehat{f}(\xv)
\]
\item \textbf{For Categorical Features}:
\[
\text{FME}_{\xv, c_j} = \widehat{f}(c_j, \xv_{-j}) - \widehat{f}(\xv)
\]
\item \textbf{Mixed-Type Features}:
\begin{itemize}
\item Combine changes in both continuous and categorical features.
\end{itemize}
\item \textbf{Advantages}:
\begin{itemize}
\item Provides exact change in prediction.
\item Applicable to any model type.
\item Removes ambiguity from marginal effects definitions.
\end{itemize}
\end{itemize}
\end{frame}

%\section{Non-Linearity Measure (NLM)}

% Non-Linearity Measure (NLM)
\begin{frame}{Non-Linearity Measure (NLM)}
\begin{itemize}
\item \textbf{Purpose}:
\begin{itemize}
  \item Quantify deviation from linearity along the path of the FME.
\end{itemize}
\item \textbf{Computation}:
\begin{itemize}
  \item Compare the prediction function to the linear secant.
  \item Use the normalized integral of squared deviations.
\end{itemize}
\item \textbf{Definition}:
\[
\text{NLM}_{\xv, \boldhS} = 1 - \frac{\int_0^1 [\widehat{f}(\gamma(t)) - g_{\xv, \boldhS}(t)]^2 \Vert \gamma'(t) \Vert_2 dt}{\int_0^1 [\widehat{f}(\gamma(t)) - \overline{\widehat{f}}]^2 \Vert \gamma'(t) \Vert_2 dt}
\]
\item \textbf{Interpretation}:
\begin{itemize}
  \item NLM $\in (-\infty, 1]$.
  \item NLM $=1$: Perfect linearity.
  \item NLM $\leq 0$: Indicates strong non-linearity.
\end{itemize}
\end{itemize}
\end{frame}

% Computation and Interpretation of NLM
\begin{frame}{Computation and Interpretation of NLM}
\begin{itemize}
\item \textbf{Linear Reference Function}:
\[
g_{\xv, \boldhS}(t) = \widehat{f}(\xv) + t \cdot \text{FME}_{\xv, \boldhS}
\]
\item \textbf{Path Parameterization}:
\[
\gamma(t) = \xv + t \cdot \boldhS
\]
\item \textbf{Line Integrals}:
\begin{align*}
\text{(I)} &= \int_0^1 [\widehat{f}(\gamma(t)) - g_{\xv, \boldhS}(t)]^2 \Vert \boldhS \Vert_2 dt \\
\text{(II)} &= \int_0^1 [\widehat{f}(\gamma(t)) - \overline{\widehat{f}}]^2 \Vert \boldhS \Vert_2 dt
\end{align*}
\item \textbf{Advantages}:
\begin{itemize}
\item Provides a quantitative measure of non-linearity.
\item Applicable to univariate and multivariate feature changes.
\item Helps assess the reliability of linear approximations.
\end{itemize}
\end{itemize}
\end{frame}

% Visualization of NLM
\begin{frame}{Visualization of NLM}
\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figure_man/forward_me_deviation.png}
  \caption{Comparison between the prediction function and the linear secant. The purple area represents the deviation quantified by the NLM.}
\end{figure}
\end{frame}

%\section{Selecting Step Sizes and Trust Regions}

% Selecting Step Sizes and Trust Regions
\begin{frame}{Selecting Step Sizes and Trust Regions}
\begin{itemize}
\item \textbf{Step Size Selection}:
\begin{itemize}
  \item Based on domain knowledge or natural units.
  \item Use statistical measures (e.g., standard deviation, IQR).
  \item Contextual information dictates meaningful step sizes.
\end{itemize}
\item \textbf{Local Linear Trust Region (LLTR)}:
\begin{itemize}
  \item Identify regions where NLM indicates approximate linearity (e.g., NLM $\geq 0.9$).
  \item Allows for reliable interpretation of FMEs.
  \item Step sizes within LLTR ensure linear approximation is valid.
\end{itemize}
\end{itemize}
\end{frame}

% LLTR Visualization
\begin{frame}{Local Linear Trust Region (LLTR) Visualization}
\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figure_man/fme_trust_region_lime.png}
  \caption{LLTR using FMEs compared to LIME explanations. The orange arrows represent steps where NLM indicates linearity.}
\end{figure}
\end{frame}

% Model Extrapolation and Step Sizes
\begin{frame}{Model Extrapolation and Step Sizes}
\begin{itemize}
\item \textbf{Model Extrapolation Risks}:
\begin{itemize}
\item Large step sizes may lead to extrapolation beyond the training data.
\item Predictions in extrapolated regions may be unreliable.
\end{itemize}
\item \textbf{Mitigation}:
\begin{itemize}
\item Limit step sizes to ranges within the training data.
\item Identify and exclude extrapolation points (EPs) from analysis.
\end{itemize}
\item \textbf{Visualization}:
\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figure_man/uniform_extrapolation_data.png}
  \caption{Model extrapolation occurs when moving beyond the training data range.}
\end{figure}
\end{itemize}
\end{frame}

%\section{Conditional Average Marginal Effects (cAMEs)}

% Conditional Average Marginal Effects (cAMEs)
\begin{frame}{Conditional Average Marginal Effects (cAMEs)}
\begin{itemize}
\item \textbf{Purpose}:
\begin{itemize}
\item Estimate expected FMEs within specific subgroups or regions.
\item Capture regional effects in non-linear models.
\end{itemize}
\item \textbf{Definition}:
\[
\text{cAME}_{\mathcal{D}_{[\;]}, \boldhS} = \frac{1}{n_{[\;]}} \sum_{i \,:\, \xv^{(i)} \in \mathcal{D}_{[\;]}} \left[ \widehat{f}\left(\xv_S^{(i)} + \boldhS, \xv_{-S}^{(i)}\right) - \widehat{f}\left(\xv^{(i)}\right) \right]
\]
\item \textbf{Subgroups}:
\begin{itemize}
\item Defined based on feature values or characteristics.
\item Aim to have homogeneous effects within subgroups.
\end{itemize}
\end{itemize}
\end{frame}

% Desiderata for Finding Subgroups
\begin{frame}{Desiderata for Finding Subgroups}
\begin{itemize}
\item \textbf{Within-Group Effect Homogeneity}:
\begin{itemize}
\item Minimize FME variance within subgroups.
\end{itemize}
\item \textbf{Between-Group Effect Heterogeneity}:
\begin{itemize}
\item Maximize differences in cAMEs between subgroups.
\end{itemize}
\item \textbf{Full Segmentation}:
\begin{itemize}
\item Entire dataset is partitioned into subgroups.
\end{itemize}
\item \textbf{Non-Congruence}:
\begin{itemize}
\item Subgroups do not overlap.
\end{itemize}
\item \textbf{Confidence and Stability}:
\begin{itemize}
\item Prefer larger, stable subgroups.
\item Subgroups should be consistent across data variations.
\end{itemize}
\end{itemize}
\end{frame}

% Estimation Using Decision Trees
\begin{frame}{Estimation Using Decision Trees}
\begin{itemize}
\item \textbf{Recursive Partitioning (RP)}:
\begin{itemize}
\item Use decision trees to partition the feature space.
\item Find subgroups with homogeneous FMEs.
\end{itemize}
\item \textbf{Conditional Inference Trees (CTREE)}:
\begin{itemize}
\item Use statistical tests to find significant splits.
\item Help stabilize the splits and improve subgroup reliability.
\end{itemize}
\item \textbf{Advantages}:
\begin{itemize}
\item Automatically identify meaningful subgroups.
\item Handle complex interactions and non-linearities.
\end{itemize}
\end{itemize}
\end{frame}

% Confidence Intervals for cAME and cANLM
\begin{frame}{Confidence Intervals for cAME and cANLM}
\begin{itemize}
\item \textbf{Purpose}:
\begin{itemize}
\item Provide uncertainty estimates for cAMEs and cANLMs.
\end{itemize}
\item \textbf{Computation}:
\begin{align*}
\text{CI}_{\text{cAME}, \; 1 - \alpha} &= \left[\text{cAME}_{\mathcal{D}_{[\;]}, \boldhS} \pm t_{1-\frac{\alpha}{2}, n_{[\;]}-1} \frac{\text{SD}_{\text{FME}, [\;]}}{\sqrt{n_{[\;]}}} \right] \\
\text{CI}_{\text{cANLM}, \; 1 - \alpha} &= \left[\text{cANLM}_{\mathcal{D}_{[\;]}, \boldhS} \pm t_{1-\frac{\alpha}{2}, n_{[\;]}-1} \frac{\text{SD}_{\text{NLM}, [\;]}}{\sqrt{n_{[\;]}}} \right]
\end{align*}
\item \textbf{Interpretation}:
\begin{itemize}
\item Narrow intervals indicate higher confidence.
\item Larger subgroups provide more reliable estimates.
\end{itemize}
\end{itemize}
\end{frame}

%\section{Related Work}

% Relation to ICE and PD
\begin{frame}{Relation to ICE and PD}
\begin{itemize}
\item \textbf{Individual Conditional Expectation (ICE)}:
\begin{itemize}
\item Visualizes predictions for an observation across a range of feature values.
\item FME corresponds to vertical differences between points on an ICE curve.
\end{itemize}
\item \textbf{Partial Dependence (PD)}:
\begin{itemize}
\item Shows average predictions across a range of feature values.
\item AME is equivalent to vertical differences on PD for linear models.
\end{itemize}
\item \textbf{Advantages of FMEs}:
\begin{itemize}
\item Provide exact change in prediction.
\item Applicable to high-dimensional feature changes.
\item Quantifiable and not limited to visual interpretation.
\end{itemize}
\end{itemize}
\end{frame}

% Comparison to LIME
\begin{frame}{Comparison to LIME}
\begin{itemize}
\item \textbf{LIME (Local Interpretable Model-Agnostic Explanations)}:
\begin{itemize}
\item Trains a local surrogate model around an observation.
\item Interpretation depends on surrogate model parameters.
\end{itemize}
\item \textbf{Differences with FMEs}:
\begin{itemize}
\item FMEs directly compute changes without surrogates.
\item FMEs are deterministic with fixed parameters.
\item LIME explanations can vary due to randomness and parameter choices.
\end{itemize}
\item \textbf{Advantages of FMEs}:
\begin{itemize}
\item More stable and interpretable.
\item NLM provides measure of linearity and trust in the explanation.
\end{itemize}
\end{itemize}
\end{frame}

% Sensitivity Analysis
\begin{frame}{Relation to Sensitivity Analysis}
\begin{itemize}
\item \textbf{Purpose of Sensitivity Analysis (SA)}:
\begin{itemize}
\item Determine how uncertainty in model output can be attributed to inputs.
\end{itemize}
\item \textbf{Methods in SA}:
\begin{itemize}
\item Use finite differences to estimate local sensitivities.
\item Elementary effects correspond to univariate forward difference quotients.
\end{itemize}
\item \textbf{Relation to FMEs}:
\begin{itemize}
\item FMEs can be seen as a form of sensitivity analysis.
\item Provide insight into feature importance and interactions.
\end{itemize}
\end{itemize}
\end{frame}

%\section{Conclusion}

% Conclusion and Future Work
\begin{frame}{Conclusion and Future Work}
\begin{itemize}
\item \textbf{Key Takeaways}:
\begin{itemize}
  \item FMEs provide exact changes in predictions for any model type.
  \item NLM helps assess the linearity of effects along feature changes.
  \item cAMEs enable regional interpretations, capturing heterogeneity.
  \item Unified approach removes ambiguity from marginal effects.
\end{itemize}
\item \textbf{Future Directions}:
\begin{itemize}
  \item Enhance methods for detecting model extrapolations.
  \item Explore stabilization techniques for subgroup discovery.
  \item Investigate applications of FMEs in feature importance and other domains.
  \item Integrate FMEs with other interpretability methods.
\end{itemize}
\end{itemize}
\end{frame}


% References (if needed)
% \begin{frame}[allowframebreaks]{References}
% \bibliographystyle{apalike}
% \bibliography{references}
% \end{frame}

\endlecture
\end{document}
