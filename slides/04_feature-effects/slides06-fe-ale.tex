\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/ale_plot.pdf}
\newcommand{\learninggoals}{
\item Understand ALE plots
\item Difference between ALE and PD plots
}

\lecturechapter{Accumulated Local Effect (ALE) plot}
\lecture{Interpretable Machine Learning}
%
% \frame{
% \frametitle{Motivation}
%
% \begin{columns}[T, totalwidth=\textwidth]
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figure_man/ale_scatter}
% \end{column}
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figure_man/ale_pdplot}
% \end{column}
% \end{columns}
%
% \only<1>{
% \textbf{Recall:} In case of strongly correlated features $x_1$ and $x_2$, partial dependence (PD) plots \textbf{average predictions} of artificial data instances that are unlikely in reality (green).
% This can lead to biased estimates.
%
% \textbf{Question:} Can we avoid the extrapolation issue by averaging only over points in an appropriate neighborhood? % of a specific point $x_1$? %, i.e., using the conditional distribution at
% }
%
% \only<2>{
%
% \textbf{Answer:} Yes, we could. Marginal plots (M plots) do this by averaging (i.e., marginalizing) over the conditional distribution $\P(\xv_2|x_1)$.
%
% \textbf{But:} M plots introduce the omitted-variable bias (OVB) issue, i.e., an
% M plot always includes the marginal effect of other dependent features.\\
% $\Rightarrow$ M plots are useless to assess the main effect of a feature.
%
% }
% }



% \begin{frame}{Motivation - Correlated Features}

% \begin{columns}[T, totalwidth=\textwidth]
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=0.9\textwidth]{figure/ale_scatter_grid}
% \end{column}
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=0.9\textwidth]{figure/ale_pdplot}
% \end{column}
% \end{columns}

% %\begin{center}
% %\includegraphics[width=0.8\textwidth]{figure_man/pd_grid}
% %\tiny{Source: Figure taken from Apley et al. (2020)}
% %\end{center}

% %In case of strongly correlated features $x_1$ and $x_2$, PD plots \textbf{average predictions} of artificial data points that are unlikely in reality (red).

% \begin{itemize}
%     \item PD plots \textbf{average over predictions} of artificial points that are out of distribution / unlikely (red)\\
%     $\Rightarrow$ Can lead to misleading / biased interpretations, especially if model also contains interactions
%     \item Not wanted if interest is to interpret effects within data distribution
% \end{itemize}
% %If features are correlated, PD plots \textbf{average predictions} of artificial points that are unlikely (red).
% %\textbf{But:} This might be undesired if interest is to interpret effects within data distribution.
% %This can be an undesired property if the model contains interactions and one is interested in interpreting effects w.r.t. the data distribution.

% %\lz

% %\textbf{Question:} Can we avoid the extrapolation issue by averaging only over points in an appropriate neighborhood? % of a specific point $x_1$? %, i.e., using the conditional distribution at

% \end{frame}


% \begin{frame}{Motivation - Correlated Features}

% %TODO: Example what can go wrong with PDPs and NNs

% Example: Fit a NN to $5000$ simulated data points with $x \sim Unif(0,1)$, $\epsilon \sim N(0, 0.2)$ and

% \centerline{$y = x_1 + x_2^2 + \epsilon$, where
% $x_1 = x + \epsilon_1$, 
% $x_2 = x + \epsilon_2$ and $\epsilon_1, \epsilon_2 \sim N(0, 0.05)$.}

% \begin{columns}[T, totalwidth=\textwidth]
% \begin{column}{0.65\textwidth}
% \centering
% \only<1>{\includegraphics[width=\textwidth]{figure/ale_vs_pdp_surf}}
% \only<2>{\includegraphics[width=\textwidth]{figure/ale_vs_pdp_nn}}
% \end{column}
% \begin{column}{0.35\textwidth}
% %What went wrong here?

% \begin{itemize}
% \item Test error (MSE) of NN is comparable to other models
% \item NN contains interactions (see complex pred. surface)
% %\item ALE shows a linear effect for $x_1$ and quadratic effect for $x_2$\\
% %$\Rightarrow$ In line with ground truth %Match true effects of DGP
% %\item PDP shows expected model behaviour but does not recover effects of DGP \\
% %$\Rightarrow$ Due to averaging of artificial points outside data distribution
% \item<2> ALE in line with ground truth
% \item<2> PDP does not reflect ground truth effects of DGP well \\
% $\Rightarrow$ Due to interactions and averaging of points outside data distribution
% \end{itemize}

% \end{column}
% \end{columns}

% \end{frame}

% \begin{frame}{M Plot vs. PD plot}

% % \begin{center}
% % \includegraphics[width=0.7\textwidth]{figure_man/PD_M.jpg}\\
% % \tiny{Source: Figure taken from Apley et al. (2020)}
% % \end{center}

% \begin{columns}[T, totalwidth=\textwidth]
% \visible<1-2>{\begin{column}{0.5\textwidth}
% \vspace*{-1em}
% \centering
% \textbf{a) PD plot}
% \includegraphics[width=0.9\textwidth]{figure/ale_pdplot}
% \end{column}
% }
% \visible<2>{\begin{column}{0.5\textwidth}
% \vspace*{-1em}
% \textbf{b) M plot}
% \centering
% \includegraphics[width=0.9\textwidth]{figure/ale_mplot}
% \end{column}
% }
% \end{columns}

% \begin{enumerate}[<+->]
% %\item[a)] PD plot $\fh_{1, PD}(x_1) = \hat{\E}_{\xv_2} \left( \fh(x_1, \xv_2) \right) = \frac{1}{n} \sum_{i=1}^n \fh(x_1, \xv_2^{(i)})$
% %\item[b)] M plot $\fh_{1, M}(x_1) = \hat{\E}_{\xv_2|\xv_1} \left( \fh(x_1, \xv_2) \middle| \xv_1\right) = \frac{1}{|N(x_1)|} \sum\limits_{i \in N(x_1)} \fh(x_1, \xv_2^{(i)})$, where $N(x_1) = \{i: x_1^{(i)} \in [x_1 - \epsilon, x_1 + \epsilon]\}$ is an index set referring to observations in an appropriate neighborhood of feature value $x_1$.
% \item[a)] PD plot $\E_{\xv_2} \left( \fh(x_1, \xv_2) \right)$ is estimated by $ \fh_{1, PD}(x_1) = \frac{1}{n} \sum\limits_{i=1}^n \fh(x_1, \xv_2^{(i)})$
% \item[b)] M plot $\E_{\xv_2|\xv_1} \left( \fh(x_1, \xv_2) \middle| \xv_1\right)$ is
% estimated by $\fh_{1, M}(x_1) = \textstyle\frac{1}{|N(x_1)|} \sum_{i \in N(x_1)} \fh(x_1, \xv_2^{(i)}),$
% where index set $N(x_1) = \{i: x_1^{(i)} \in [x_1 - \epsilon, x_1 + \epsilon]\}$ refers to observations with feature value close to $x_1$. %in an appropriate neighborhood of feature value $x_1$.
% \end{enumerate}
% \end{frame}

% \begin{frame}{M Plot vs. PD plot}

% \begin{columns}[T, totalwidth=\textwidth]
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=0.9\textwidth]{figure/ale_scatter}
% \end{column}
% \begin{column}{0.5\textwidth}
% \centering
% \includegraphics[width=0.9\textwidth]{figure/ale_mplot}
% \end{column}
% \end{columns}

% %\textbf{Answer:} Yes, we could.
% %\textbf{Recall:} Marginal plots (M plots) do this by averaging (i.e., marginalizing) over the conditional distribution $\P(\xv_2|x_1)$.
% %\textbf{Recall:}

% \begin{itemize}
%     \item M plots average predictions over conditional distribution (e.g., $\P(\xv_2|x_1)$)\\
%     $\Rightarrow$ Averaging predictions close to data distribution avoid extrapolation issues
%     \item \textbf{But:} M plots suffer from omitted-variable bias (OVB)
% \begin{itemize}
% \item They contain effects of other dependent features
% \item Useless in assessing a feature's marginal effect if feature dependencies are present
% \end{itemize}
% \end{itemize}

% % in case of dependent features.

% %M plots average over the conditional distribution $x_2|x_1$. However, the M plot of $x_1$ also includes the marginal effect of other dependent features $\Rightarrow$ We don't want this.
% % do not include the marginal effect of the considered feature but
% % mixture of its marginal effect and the marginal effects of all dependent features
% \end{frame}

% \begin{frame}{M Plot vs. PD plot - OVB Example}
% %\vspace*{-\topsep}
% %\vspace*{-3\lineskip}

% \begin{center}
% \includegraphics[width=0.9\textwidth]{figure/pd_vs_mplot}
% \end{center}

% \textbf{Illustration:} Fit LM on 500 i.i.d. observations with features $x_1, x_2 \sim N(0,1)$, $Cor(x_1, x_2) = 0.9$ and $$y = -x_1 + 2 \cdot x_2 + \epsilon, \; \epsilon \sim N(0,1).$$

% \textbf{Results:} M plot of $x_1$ also includes marginal effect of all other dependent features (here: $x_2$)
% \end{frame}

% \begin{frame}{Idea: Integrating Partial Derivatives}

% \textbf{Idea:} To remove unwanted effects of other features, take partial derivatives (local effects) of prediction function w.r.t. feature of interest and integrate (accumulate) them w.r.t. the same feature

% \begin{itemize}
% \item[$\Rightarrow$] Computing the partial derivative of $\fh$ w.r.t. $\xv_j$ removes other main effects
% \item[$\Rightarrow$] Integrating again w.r.t. $\xv_j$ recovers the original main effect of $\xv_j$
% \end{itemize}

% \pause %\lz

% \textbf{Example:}
% \begin{itemize}[<+->]
% \item Consider an additive prediction function: $$\fh(x_1, x_2) = 2x_1 + 2x_2 - 4x_1 x_2$$
% \item Partial derivative of $\fh$ w.r.t. $x_1$:
% $\frac{\partial \fh(x_1, x_2)}{\partial x_1} = 2 - 4x_2$
% \item Integral of partial derivative ($z_0 = \min(x_1)$):
% $$\int_{z_0}^{x} \frac{\partial \fh(x_1, x_2)}{\partial x_1} dx_1 = \left[2x_1 - 4x_1 x_2\right]_{z_0}^{x}$$
% \item We removed the main effect of $x_2$, which was our goal % (Note: interaction is still there)
% \end{itemize}
% \end{frame}

\begin{frame}{Accumulated Local Effects (ALE) \citebutton{Apley, Zhu (2020)}{https://arxiv.org/abs/1612.08468}}
%\begin{columns}[T, totalwidth=\textwidth]
%\begin{column}{0.65\textwidth}
%ALE plots use the idea of integrating partial derivatives. They do not suffer from the extrapolation issue of PD plots and the OVB issue of M plots when features are dependent.
%\end{column}
%\begin{column}{0.35\textwidth}
%\centering
%\includegraphics[width=\textwidth]{figure_man/ale.jpg}\\
%\tiny{Source: Figure taken from Apley et al. (2020)}
%\end{column}
%\end{columns}

ALE plots estimate the marginal effect of a feature by accumulating its local effects (integrating partial derivatives), evaluated in regions supported by the data. 

% \medskip
% \textbf{Key advantages:}
% \begin{itemize}
%     \item No extrapolation beyond observed data (unlike PD plots)
%     \item No omitted variable bias (OVB) from dependent features (unlike M plots)
% \end{itemize}

%Let $z_0$ denote the minimum of observed values of feature $\xv_S$, i.e., $z_0 = \min(\xv_S)$. All complementary features are denoted by $\xv_C$.

\medskip
\textbf{Computation Steps:}
\begin{enumerate}[<+->]
\item \textbf{Estimate local effects} $\frac{\partial \fh(x_S, \xv_{-S})}{\partial x_S}$ (via finite differences) \label{ref1}\\ %evaluated at certain points $(x_S = z_S, \xv_{-S})$
$\Rightarrow$ Removes unwanted main effects of other features $\xv_{-S}$ (unlike M plots)
\item \textbf{Average local effects} over conditional distribution $\P(\xv_{-S}|x_S)$ similar to M plots\\ %across all values of $\xv_{-S}$, i.e.,
$\Rightarrow$ Avoids extrapolation (unlike PD plots)\label{ref2}
\item \textbf{Accumulate:} Integrate averaged local effects up to a specific value $x \in \mathcal{X}_S$\\ %all values of $z_S$   from $z_0 := \min(x_S)$ u
$\Rightarrow$ Reconstructs main effect of $x_S$ \label{ref3}\\
%$\Rightarrow$ Avoids OVB issue as other unwanted main effects were removed in \eqref{ref1}
\end{enumerate}

%\footnote[frame]{Apley, Daniel W., and Jingyu Zhu (2020). Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4: 1059-1086.}
\end{frame}
%\begin{itemize}
  %\item Accumulated Local Effect (ALE) plots were developed by Apley (2020) to overcome the extrapolation issue of PD plots and the OVB issue of M plots when features are dependent.
  %. \item ALEs are an alternative to PD that do not suffer from extrapolation
  %\item PD and ALE reduce the complex prediction function $\hat{f}$ to a function that depends on only one (or two) features.
  %\item They both do that by averaging the effects of the other features.
  %\item We can see the main differences of PD vs. ALE if we look on their formulas.
  %\item Before, we explain the main idea of ALE.
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \begin{frame}{First Order ALE Function}

% \begin{itemize}%[<+->]
% %\item Let $z_0 = \min(x_S)$ be minimum of feature of interest $x_S$.
% %\item All complementary features are denoted by $\xv_C$.
% \item Let $x_S$ be feature of interest with $z_0 = \min(x_S)$ and $\xv_{-S}$ all other features  (complement of $S$)
% \item Uncentered first order ALE $\tilde{f}_{S, ALE}(x)$ at feature value $x \sim \P(x_S)$ is defined as:  
% $$
% \begin{aligned}
% \hspace*{-0.7cm} 
% \tilde{f}_{S, ALE}(x) &= \underbrace{\int_{z_{0}}^{x}}_{ \eqref{ref3}} \underbrace{\E_{\xv_{-S} \vert x_S = z}}_{\eqref{ref2}} \bigg(  \underbrace{\frac{\partial \fh(z, \xv_{-S})}{\partial z}}_{\eqref{ref1}} \bigg) dz 
% = \int_{z_0}^{x} \int \frac{\partial \hat{f}(z, \xv_{-S})}{\partial z} \, d\P(\xv_{-S} \mid z) \, dz
% % = \int_{z_{0}}^{x} \left( \int_{-\infty}^{\infty}  \frac{\partial \fh(z_S, \xv_{-S})}{\partial z_S} d\P(\xv_{-S} | z_S) \,   \right) dz_S
% \end{aligned}
% $$
% \pause
% \item Subtract average of uncentered ALE curve (constant) to obtain centered ALE curve $f_{S, ALE}(x)$ with zero mean regarding marginal distribution of feature of interest $x_S$:
% $$
% \begin{aligned}
% f_{S, ALE}(x) = \tilde{f}_{S, ALE}(x) - \underbrace{\int_{-\infty}^{\infty}\tilde{f}_{S, ALE}(x_S) \, d\P(x_S)}_{:= constant}
% \end{aligned}
% $$
% \end{itemize}



% \end{frame}

\begin{frame}{First Order ALE Function}

\textbf{Uncentered ALE Function} evaluated at $x \in \mathcal{X}_S$ (domain of feature $x_S$):
\[
\tilde{f}_{S, \text{ALE}}(x) 
= \underbrace{\int_{z_0}^{x}}_{\text{\eqref{ref3}}} 
\underbrace{\E_{\xv_{-S} \mid x_S = z_S}}_{\substack{\text{\eqref{ref2} average} \\ \text{locally }}} 
\left( \underbrace{\frac{\partial \hat{f}(z_S, \xv_{-S})}{\partial z_S}}_{\text{\eqref{ref1} local effect}} \right) dz_S
= \int\limits_{z_0}^{x} \int \frac{\partial \hat{f}(z_S, \xv_{-S})}{\partial z_S} d\P(\xv_{-S} \mid z_S) dz_S
\]
\begin{itemize}
\tightlist
    \item $x_S$ is feature of interest, with minimum value $z_0 = \min(x_S)$
    \item $z_S$ is integration variable ranging over $\mathcal{X}_S$, used to evaluate local effects
    \item $\xv_{-S}$ denotes all other features (complement of $S$)
\end{itemize}

\pause
\medskip
\textbf{Centering (to ensure identifiability):}
\[
f_{S, \text{ALE}}(x) = \tilde{f}_{S, \text{ALE}}(x) 
- \underbrace{\int \tilde{f}_{S, \text{ALE}}(x_S) \, d\P(x_S)}_{\text{constant shift to mean zero}}
\]

% \medskip
% \textbf{Interpretation:}
% ALE curves accumulate local effects of $x_S$ while averaging out the influence of other features, using only data-supported regions.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{ALE Estimation}

% \begin{itemize}
%   \item Partial derivatives not useful for all models (e.g., tree-based methods)
%   \item Approximate them by finite differences of predictions within $K$ intervals for $\xv_S$:
%   $$
%   \begin{aligned}
%   x \in [\min(\xv_S), \max(\xv_S)] \iff &x \in [z_{0}, z_{1, S}] \\
%   \lor &x \in \; [z_{1, S}, z_{2, S}] \\
%   &\dots \\
%   \lor &x \in \; [z_{K-1, S}, z_{K, S}]
%   \end{aligned}
%   $$
%   \item Create $K$ intervals for feature $\xv_S$, e.g., using quantiles as interval bounds 
%   %A simple way of creating $K$ intervals for the value range of $\xv_S$ is to use $K-1$ quantiles as interval bounds $z_1, \dots, z_{K-1}$ (not counting the 0\% and 100\% quantiles).
% \end{itemize}

% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\begin{frame}{ALE Estimation: Illustration}

%\centerline{\includegraphics[width=0.8\textwidth]{figure/ale_interval}}

\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.6\textwidth}
\centering
\includegraphics<1>[width=0.7\textwidth, trim=0 0 0 30, clip]{figure/ale_step1_2D.pdf}
\end{column}
\begin{column}{0.4\textwidth}
\centering
\includegraphics<1>[width=0.8\textwidth, trim=150 40 150 130, clip]{figure/ale_step1_3D_plotly.pdf}
\end{column}
\end{columns}

%Compared to PD, ALE avoids averaging predictions of unlikely data instances and blocks the effect of other correlated features.
% \begin{itemize}
% \item Divide feature of interest into intervals (vertical lines)
% \item For all points within an interval, compute \textbf{prediction difference} when we replace feature value with upper/lower interval bound (blue points) while keeping other feature values unchanged
% \item These \textbf{finite differences} (approximate local effect) are accumulated \& centered $\Rightarrow$ ALE plot %to produce
% \end{itemize}

\begin{itemize}
\tightlist
\item \textbf{Motivation:} Partial derivatives are not well-defined for all models (e.g., tree-based methods).  
$\Rightarrow$ Use finite differences within intervals instead.
\item<1-> Partition the feature range of $x_S$ into $K$ intervals (vertical lines)
    \begin{itemize}
      \item Define intervals:
      $$x_S \in [\min(x_S), \max(x_S)] \Rightarrow 
  x_S \in [z_{0}, z_{1, S}] \cup [z_{1, S}, z_{2, S}] \cup \dots \cup [z_{K-1, S}, z_{K, S}]$$
      \item \textit{Equidistant}: preserves resolution
      \item \textit{Quantile-based}: balances sample size per interval
    \end{itemize}
\end{itemize}


\end{frame}



\begin{frame}{ALE Estimation: Illustration}

%\centerline{\includegraphics[width=0.8\textwidth]{figure/ale_interval}}

\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.6\textwidth}
\centering
%\includegraphics<1>[width=0.7\textwidth, trim=0 0 0 30, clip]{figure/ale_step1_2D.pdf}
\includegraphics<1>[width=0.7\textwidth, trim=0 0 0 30, clip]{figure/ale_step2_2D.pdf}
\includegraphics<2>[width=0.7\textwidth, trim=0 0 0 30, clip]{figure/ale_step3_2D.pdf}
%\includegraphics<4>[width=0.7\textwidth, trim=0 0 0 30, clip]{figure/ale_step4_2D.pdf}
\end{column}
\begin{column}{0.4\textwidth}
\centering
%\includegraphics<1>[width=0.8\textwidth, trim=150 40 150 130, clip]{figure/ale_step1_3D_plotly.pdf}
\includegraphics<1>[width=0.8\textwidth, trim=150 40 150 130, clip]{figure/ale_step2_3D_plotly.pdf}
\includegraphics<2>[width=0.8\textwidth, trim=150 40 150 130, clip]{figure/ale_step3_3D_plotly.pdf}
%\includegraphics<4>[width=0.7\textwidth, trim=150 40 150 100, clip]{figure/ale_step4_3D_plotly.pdf}
\end{column}
\end{columns}

%Compared to PD, ALE avoids averaging predictions of unlikely data instances and blocks the effect of other correlated features.
% \begin{itemize}
% \item Divide feature of interest into intervals (vertical lines)
% \item For all points within an interval, compute \textbf{prediction difference} when we replace feature value with upper/lower interval bound (blue points) while keeping other feature values unchanged
% \item These \textbf{finite differences} (approximate local effect) are accumulated \& centered $\Rightarrow$ ALE plot %to produce
% \end{itemize}

\begin{itemize}
\tightlist
%\item<1-> Partition the feature range of $x_S$ into $K$ intervals (vertical lines)
\item<1-> For each observation in $k$-th interval, i.e., $\{i: x_S^{(i)} \in \; [z_{k-1, S}, z_{k, S}]\}$:
  \begin{itemize}
    \item Replace $x_S^{(i)}$ with \textcolor{blue}{upper/lower interval bounds}, keeping $\xv_{-S}^{(i)}$ fixed
    \item Compute observation-wise finite difference of $i$-th obs. in $k$-th interval \\
    %Compute \textbf{prediction difference} (local effect) for $i$-th dobservation \\
    $\leadsto$ $\fh(z_{k, S}, \xi_{-S}) - \fh(z_{k-1, S}, \xi_{-S})$  \; (approximates local effect)  %\\
    %$\leadsto$ Approximates local effect of $x_S$ for $i$-th data point
  \end{itemize}
\item<2-> Average these finite differences over all observations in each interval \\
$\leadsto$ Approximates \textbf{inner integral} \textcolor{red}{\( \E_{\xv_{-S} \mid x_S = z_S} \left[ \partial \hat{f}/\partial z_S \right] \)}
\item<2-> Accumulate these averages from \( z_0 \) to the point of interest \( x \in \mathcal{X}_S \) \\
$\leadsto$ Approximates \textbf{outer integral} over \( z_S \in [z_{0}, x] \) $\Rightarrow$ uncentered ALE function

%\item Accumulate and center differences over all data points $\Rightarrow$ ALE curve
\end{itemize}

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{2-D Illustration}
% \centerline{\includegraphics[width=0.9\textwidth]{figure/ale_interval}}

%  \begin{itemize}
%   %\item Now let us define it more formalized
%   %\item Consider the $i$-th observation $\xi = (x_S^{(i)}, \xi_C)$ for which $x_S^{(i)}$ is located within the $k$-th interval of $\xv_S$, i.e., $x_S^{(i)} \in \; ]z_{k-1, S}, z_{k, S}]$
%   \item For $\xi = (x_S^{(i)}, \xi_{-S})$, value $x_S^{(i)}$ is located within $k$-th interval of $\xv_S$ ($x_S^{(i)} \in \; [z_{k-1, S}, z_{k, S}]$)
%   \item Replace $x_S^{(i)}$ by upper/lower interval bound while all other feature values $\xi_{-S}$ are kept constant
%   \item Finite differences correspond to $\fh(z_{k, S}, \xi_{-S}) - \fh(z_{k-1, S}, \xi_{-S})$
%   %and approximates the local effect of the $i$-th observation
% \end{itemize}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{2-D Illustration}
% \centerline{\includegraphics[width=0.9\textwidth]{figure/ale_interval}}

%  \begin{itemize}
%   \item Estimate local effect of $\xv_S$ within each interval by averaging all observation-wise finite differences $\hat = $ Approximation of inner integral that integrates over local effects w.r.t. $\P(\xv_{-S} | z_S)$. %conditional distribution of $\xv_C$ on $\xv_S$
%   \item Sum up local effects of all intervals up to point of interest $\hat = $ Estimates outer integral
% \end{itemize}

% \end{frame}
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% \begin{frame}{3-D Illustration}
%
% Consider the following data generating process:
%
% $$
% \begin{gathered}
% u_1, u_2 =  \{-10, -9.9, -9.8, \dots, 9.8, 9.9, 10\} \\
% n_1, n_2 \stackrel{iid}{\sim}  N(0, 1) \\
% \varepsilon \stackrel{iid}{\sim} N(0, 3) \\
% \\
% x_1 = u_1 + n_1 \\
% x_2 = u_2 + n_2 \\
% y = 100  \left[ \frac{\partial^2 \left[ \frac{1}{1 + exp(-x_1)} \right] }{\partial x_1 \partial x_1} \right] + 2 x_2 + \varepsilon
% \end{gathered}
% $$
%
% \framebreak
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/3D01.png}
% \end{center}
%
% 3D visualization of the data, the predictions made by the predictive model (blue net) and 10 intervals for the value range of $x_1$.
%
% \framebreak
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/3D02.png}
% \end{center}
%
% Zooming in on the interval $x_1 \in [4.198, 6.249]$.
%
% \framebreak
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/3D03.png}
% \end{center}
%
%
% We substitute each observation`s $x_1$ value by the right and left interval boundaries, predict and take the difference.
%
% \framebreak
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/3D04.png}
% \end{center}
%
% For each observation, we receive a change in prediction when traversing the interval from the left to the right boundary.
%
% \end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{frame}{3-D Illustration}
% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/3D05.png}
% \end{center}
%
% We integrate over the conditional distribution of $x_2$ on $x_1$ by averaging all observation-wise finite differences inside each interval.
%
% \framebreak
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/3D06.png}
% \end{center}
%
%
% We repeat the same procedure for every interval. The first order ALE corresponds to adding up all average interval-wise finite differences and substracting the centering constant.
%
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{ALE Estimation: Formula}

% \begin{itemize}
% \item Estimated uncentered first order ALE $\hat{\tilde{f}}_{S, ALE}(x)$ at point $x$:
% $$
% \begin{aligned}
% \hat{\tilde{f}}_{S, ALE}(x) = \sum_{k = 1}^{k_S(x)}\frac{1}{n_S(k)}\sum_{i: \; x_S^{(i)} \in \; [z_{k-1, S}, z_{k, S}]}\left[\fh(z_{k, S}, \xi_{-S}) -\fh(z_{k-1, S}, \xi_{-S})\right]
% \end{aligned}
% $$
% \item $k_S(x)$ denotes the interval index a feature value $x \in \xv_S$ falls in
% \item $n_S(k)$ denotes the number of observations inside the $k$-th interval of $\xv_S$
% \item Subtract average of estimated uncentered ALE to obtain centered ALE estimate: %  $\hat{f}_{S, ALE}(x)$ at point $x$
% $$
% \begin{aligned}
% \hat{f}_{S, ALE}(x) = \hat{\tilde{f}}_{S, ALE}(x) - \frac{1}{n}\sum_{i = 1}^n \hat{\tilde{f}}_{S, ALE}(x_S^{(i)})
% \end{aligned}
% $$

% \end{itemize}
% \end{frame}

\begin{frame}{ALE Estimation: Formula}

\textbf{Estimated uncentered ALE:}  
For a point $x \in \mathcal{X}_S$, define:
\[
\hat{\tilde{f}}_{S, ALE}(x) = \sum_{k = 1}^{k_S(x)}\frac{1}{n_S(k)}\sum_{i: \; x_S^{(i)} \in \; [z_{k-1, S}, z_{k, S}]}\left[\fh(z_{k, S}, \xi_{-S}) -\fh(z_{k-1, S}, \xi_{-S})\right]
\]

\begin{itemize}
\tightlist
  \item $[z_{k-1, S}, z_{k,S}]$: $k$-th interval of feature $x_S$
  \item $k_S(x)$: index of the interval in which $x$ lies
  \item $n_S(k)$: number of observations in interval $k$
  \item $\xv_{-S}^{(i)}$: all other features held fixed for $i$-th observation
\end{itemize}

\pause
\medskip
\textbf{Centering:}  
Ensure identifiability by subtracting the average uncentered ALE:
\[
\hat{f}_{S, \text{ALE}}(x) =
\hat{\tilde{f}}_{S, \text{ALE}}(x)
- \frac{1}{n} \sum_{i = 1}^n \hat{\tilde{f}}_{S, \text{ALE}}(x_S^{(i)})
\]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{ALE Estimation: Algorithm}

% \begin{enumerate}
% 	\item Create $K$ intervals for value range of $\xv_S$
% 	\item Repeat for each interval: %$k \in 1, \dots, k_S(x)$:
% 	  \begin{itemize}
% 	  %\item Select the subset of observations inside the $k$-th interval
% 	  \item Replace observation's feature value $x_S^{(i)}$ with upper/lower interval bound for each observation inside $k$-th interval
% 	  \item Compute observation-wise finite difference inside $k$-th interval and average them to estimate interval-wise local effects
% 	  \end{itemize}
%   \item Accumulate interval-wise local effects up to value of interest $x$ to estimate uncentered ALE and then center it
% \end{enumerate}

% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{Second Order ALE Function}

% \textbf{Goal:} Estimate interaction effects between two features $x_j$ and $x_k$  
% $\Rightarrow$ Remove both marginal effects and isolate their combined influence

% \medskip
% \textbf{Uncentered second-order ALE:}
% \[
% \tilde{f}_{jk, \text{ALE}}(x_j, x_k) =
% \int_{z_{0,j}}^{x_j} \int_{z_{0,k}}^{x_k}
% \E_{\xv_{-\{j,k\}} \mid x_j = z_j, x_k = z_k}
% \left( \frac{\partial^2 \hat{f}(z_j, z_k, \xv_{-\{j,k\}})}{\partial z_j \partial z_k} \right)
% dz_k \, dz_j
% \]

% \begin{itemize}
% \tightlist
%   \item $x_j, x_k$: features of interest; $z_j, z_k$ are integration variables
%   \item $\xv_{-\{j,k\}}$: all remaining features held fixed
%   \item \textbf{Interpretation:} measures the effect of changing $x_j$ \emph{depending on} the value of $x_k$
% \end{itemize}

% \pause
% \textbf{Centering:}
% \[
% f_{jk, \text{ALE}}(x_j, x_k) =
% \tilde{f}_{jk, \text{ALE}}(x_j, x_k)
% - f_{j, \text{ALE}}(x_j)
% - f_{k, \text{ALE}}(x_k)
% \]

% \begin{itemize}
% \tightlist
%   \item Removes main effects to isolate interaction
%   \item Ensures $\E[f_{jk, \text{ALE}}(x_j, x_k)] = 0$
% \end{itemize}

% \end{frame}


% \begin{frame}{Extrapolation and ALE Plot}
% \begin{itemize}
% \item Remember that there are two sources of extrapolation in PD plots:
%   \begin{enumerate}
%   \item Model predicts in regions where it was not trained, and
%   \item Monte Carlo integral corresponds to an integration w.r.t. a uniform distribution, instead of the data distribution
%   \end{enumerate}
% \item Assuming that all interval bounds are close to the corresponding observed values, ALEs are robust to both sources of extrapolation
%   \begin{enumerate}
%     \item Trained model does not predict in regions that are far away from the training data
%     %where it was not trained
%     \item We integrate w.r.t. the conditional distribution of $\xv_{-S}$ on $x_S$ instead of the marginal distribution of $\xv_{-S}$, because we only average the finite differences inside a single interval
%   \end{enumerate}
% \end{itemize}
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Bike Sharing Dataset: First Order ALE}

Shape of PD plot (top) often looks similar to (centered) first order ALE plot (bottom) but on different $y$-axis scale.
In case of correlated features, ALE might be better due to PD's extrapolation issue.


\begin{center}
\includegraphics[width=0.8\textwidth]{figure/ale1d}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Bike Sharing Dataset: Second Order ALE}

%It is possible to estimate higher-order ALEs, e.g., 2nd-order ALEs.
Unlike bivariate PD plots, 2nd-order ALE plots only estimate pure interaction between two features (1st-order effects are not included).

\vspace{0.1cm}

\begin{center}
\includegraphics[width=0.8\textwidth]{figure/ale2d}
\end{center}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{PD vs. ALE}
%$$\int_{z_{0}}^{x} \E_{\xv_{-S} \vert x_S} \Bigg(  \frac{\partial \fh(x_S, \xv_{-S})}{\partial x_S} \bigg \vert x_S = z_S \Bigg) dz_S - const$$

    \textbf{PD:}
    \centerline{$
    \begin{aligned}
      %\hspace{2cm} 
      f_{S, PD}(x_S) &= \E_{\xv_{-S}} \left( \fh(x_S, \xv_{-S}) \right) %= \int_{-\infty}^{\infty} \fh(x_S, \xv_{-S}) \, d\P(\xv_{-S})
    \end{aligned}
    $} \\
    \lz
    \textbf{ALE:}
    \centerline{$
\begin{aligned}
\only<1>{
f_{S, \text{ALE}}(x)
&= \int_{z_0}^{x} \E_{\textcolor{blue}{\xv_{-S} \mid x_S = z_S}} 
\left( \textcolor{purple}{\frac{\partial \hat{f}(z_S, \xv_{-S})}{\partial z_S}} \right) dz - \text{const}
}
\only<2>{
f_{S, \text{ALE}}(x)
&= \textcolor{blue}{\int_{z_0}^{x}} 
\E_{\xv_{-S} \mid x_S = z_S} 
\left( {\frac{\partial \hat{f}(z_S, \xv_{-S})}{\partial z_S}} \right) 
\textcolor{blue}{dz} - {\text{const}}
}
\only<3>{
f_{S, \text{ALE}}(x)
&= \int_{z_0}^{x} 
\E_{\xv_{-S} \mid x_S = z_S} 
\left( \frac{\partial \hat{f}(z_S, \xv_{-S})}{\partial z_S} \right) dz 
- \textcolor{blue}{\int \tilde{f}_{S, \text{ALE}}(x_S) \, d\P(x_S)}
}
\end{aligned}
$}

%     \centerline{$
%     \begin{aligned}
%       \only<1>{%\hspace{1cm}
%        f_{S, ALE}(x)
% %&= \int_{z_{0}}^{x} \E_{\xv_C \vert x_S} \left(\textcolor{purple}{\frac{\partial \fh(x_S, \xv_C)}{\partial x_S}} \textcolor{blue}{\bigg \vert x_S = z_S} \right) dz_S - const%\\
%  &= 
%  \int_{z_{0}}^{x} \E_{\textcolor{blue}{\xv_{-S} \vert x_S}} \Bigg(  \textcolor{purple}{\frac{\partial \fh(x_S, \xv_{-S})}{\partial x_S}} \bigg \vert x_S = z_S \Bigg) dz_S - const
%  %\int_{z_{0}}^{x} \left( \int_{-\infty}^{\infty}  \textcolor{purple}{\frac{\partial \fh(z_S, \xv_{-S})}{\partial z_S}} d \textcolor{blue}{\P(\xv_{-S} | z_S) } \,   \right) dz_S - const
% }
%     \only<2>{%\hspace{1cm}
%      f_{S, ALE}(x)
% %&= \textcolor{blue}{\int_{z_{0}}^{x}} \E_{\xv_C \vert x_S} \left(\frac{\partial \fh(x_S, \xv_C)}{\partial x_S} \bigg \vert x_S = z_S \right) \textcolor{blue}{dz_S} - const%\\
%   &= 
%    \textcolor{blue}{\int_{z_{0}}^{x}} \E_{\xv_{-S} \vert x_S} \Bigg( \frac{\partial \fh(x_S, \xv_{-S})}{\partial x_S} \bigg \vert x_S = z_S \Bigg) \textcolor{blue}{dz_S} - const
%   %\textcolor{blue}{\int_{z_{0}}^{x}} \left( \int_{-\infty}^{\infty}\frac{\partial \fh(z_S, \xv_{-S})}{\partial z_S} d \P(\xv_{-S} | z_S)  \,   \right) \textcolor{blue}{dz_S} - const
% }
% \only<3>{%\hspace{1cm}
%      f_{S, ALE}(x)
% %&= \textcolor{blue}{\int_{z_{0}}^{x}} \E_{\xv_C \vert x_S} \left(\frac{\partial \fh(x_S, \xv_C)}{\partial x_S} \bigg \vert x_S = z_S \right) \textcolor{blue}{dz_S} - const%\\
%   &= 
%      \int_{z_{0}}^{x} \E_{\xv_{-S} \vert x_S} \Bigg( \frac{\partial \fh(x_S, \xv_{-S})}{\partial x_S} \bigg \vert x_S = z_S \Bigg) dz_S - \textcolor{blue}{const}
%   %\int_{z_{0}}^{x} \left( \int_{-\infty}^{\infty}\frac{\partial \fh(z_S, \xv_{-S})}{\partial z_S} d \P(\xv_{-S} | z_S)  \,   \right) dz_S - \textcolor{blue}{const}
% }
%  \end{aligned}
%    $}
    \lz
    \begin{itemize}
    \item<1-> Recall: PD directly averages predictions over marginal distribution of $\xv_{-S}$
    \item<1-> ALE is faster: Needs $O(2 \cdot n)$ model calls vs. $O(n \cdot g)$ for PD with $g$ grid points
    \only<1>{
    \item Difference 1: ALE averages 
    \begin{itemize}
        \item \textcolor{purple}{prediction changes} (via partial derivatives, estimated by finite differences) 
        \item over \textcolor{blue}{conditional distribution $\P(\xv_{-S}| x_S = z_S)$}
    \end{itemize}
    }
    \only<2-3>{
    \item Difference 1: ALE averages the
        \begin{itemize}
        \item prediction changes (via partial derivatives, estimated by finite differences) 
        \item over conditional distribution $\P(\xv_{-S}| x_S = z_S)$
        \end{itemize}
    }
    \only<2>{
    \item Difference 2: ALE \textcolor{blue}{integrates these partial derivatives over $z_S \in [z_0, x] \subseteq \mathcal{X}_S$ }\\
    $\leadsto$ isolates effect of \( x_S \) and removes main effect of other dependent features
    }
    \only<3>{
    \item Difference 2: ALE integrates these partial derivatives over $z_S \in [z_0, x] \subseteq \mathcal{X}_S$\\
    %integrates partial derivatives of feature $S$ over $z_S$\\
    $\leadsto$ isolates effect of \( x_S \) and removes main effect of other dependent features
    }
    %\item Difference 2: ALE has an additional \textcolor{blue}{integral over $z_S$}. Instead of directly averaging the predictions, the ALE method calculates the prediction differences conditional on features S and integrates the derivative over features $S$ to estimate the effect. The derivative (or interval difference) isolates the effect of the feature of interest and blocks the effect of correlated features.
    %}
    \item<3-> Difference 3: ALE is \textcolor{blue}{centered} so that $\E_{x_S} \left( f_{S, ALE}(x) \right) = 0$

    \end{itemize}
\end{frame}


\endlecture
\end{document}
