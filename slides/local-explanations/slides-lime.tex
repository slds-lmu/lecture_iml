
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
	\ifdim\Gin@nat@width>\linewidth
	\linewidth
	\else
	\Gin@nat@width
	\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
	\def\at@end@of@kframe{}%
	\ifinner\ifhmode%
	\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
	\fi\fi%
	\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
		\colorbox{shadecolor}{##1}\hskip-\fboxsep
		% There is no \\@totalrightmargin, so:
		\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
	\MakeFramed {\advance\hsize-\width
		\@totalleftmargin\z@ \linewidth\hsize
		\@setminipage}}%
{\par\unskip\endMakeFramed%
	\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
	%Define standard arrow tip
	>=stealth',
	%Define style for boxes
	punkt/.style={
		rectangle,
		rounded corners,
		draw=black, very thick,
		text width=6.5em,
		minimum height=2em,
		text centered},
	% Define arrow style
	pil/.style={
		->,
		thick,
		shorten <=2pt,
		shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
% \newcommand{\titlefigure}{figure/sample-dgp-2d.pdf}
\newcommand{\learninggoals}{
	\item }
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Local Explanations}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\input{../../latex-math/basic-math.tex}
	\input{../../latex-math/basic-ml.tex}
	
	\lecturechapter{LIME}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------


\begin{vbframe}{LIME}
\begin{itemize}
        \item  Local Interpretable Model-agnostic Explanations (LIME) fits local, interpretable models that can explain individual predictions of any black-box model.
        \item  These local surrogate models are interpretable like a linear model or tree  and are learned on predictions of the original model. 
\end{itemize}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure/lime-effect.png}
\end{center}   

\textbf{Figure:} LIME for an example instances of the bike sharing dataset.
\end{vbframe}

\begin{vbframe}{LIME}
\normalsize
Steps to fit a local surrogate model according to the SIPA framework:
\begin{enumerate}
  \item Choose an instance of interest $\xv$. 
  \item Perturb your dataset and get black box predictions for these new points. 
  \item Weight the new samples by their proximity to $\xv$.
  \item Train a weighted, sparse, interpretable model on this new dataset (e.g. LASSO) with the obtained predictions as the target.
  \item Explain the prediction by interpreting the local model. 
\end{enumerate}

\framebreak

\begin{center}
 \includegraphics[width=0.5\textwidth]{figure/lime}
\end{center}
\begin{itemize}
  \item The blue/pink background represents the complex decision function $\fh$.
  \item The red bold cross is our instance of interest $\xv$.
  \item Other crosses and points represent the sampled instances. The size of the symbols corrresponds to the proximity to $\xv$.
  \item The dashed line corresponds to the learned model used as a local explanation.
\end{itemize}
{\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier}}
\end{vbframe}

\begin{vbframe}{Bike Sharing Dataset}
\vspace{-.3cm}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure/bike-figure.png}
\end{center} 

\footnotesize \textbf{Figure:} LIME for two example instances of the bike sharing dataset.

\normalsize
\vspace{0.2cm}
The plots show the feature effect of the sparse linear model, i.e. the model coefficients times the feature value of the instance.
Warmer temperature has a positive effect on the prediction, 
while the year 2011 has a large negative effect as well as the springtime.
\end{vbframe}

\begin{vbframe}{Remarks}
  \begin{itemize}
      \item LIME creates new samples by perturbing each feature individually. For numerical features, samples are drawn from a normal distribution with mean and standard deviation based on the observed feature values.   
      \item LIME samples are not taken around the instance of interest $\xv$, but from the training data's mass center.
      \item Any interpretable model (e.g. LASSO, decision tree) can be used as a local model. 
      \item LIME assumes that even if a machine learning model is very complex, the local prediction can be described with a simpler model.
     \item It is difficult to define locality (= how samples are weighted locally). It has a huge influence on the local model, but there is no automatic procedure for choosing the neighbourhood. 
     \item Current implementations use an exponential smoothing kernel to define the neighborhood. 
     Until now there is not a good way to find a good kernel and kernel width. 
     \item A small kernel width means that an instance must be very close to influence the local model, a larger kernel width means that instances that are farther away also influence the model. 
   \end{itemize}
\end{vbframe}


\frame{
  \frametitle{Remarks}
  \begin{onlyenv}
  \begin{columns}
  \begin{column}{0.5\textwidth}
  \begin{itemize}
  \only<1>{
    \item In certain scenarios, you can easily turn your explanation around by changing the kernel width. 
    \item In the figure, the predictions of the black box model depending on a single feature $x$ is shown as a thick line. The distribution of the data is shown with rugs. 
  }
  \only<2>{
    \item Three local surrogate models with different kernel widths are computed. 
    \item The resulting linear regression model depends on the kernel width. 
    \item The instability of the explanations is a really big issue. 
    It implies that explanations should be taken with a grain of salt.
  }
  \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}  
        \begin{center}
        \includegraphics[width=1\textwidth]{figure/lime-fail-1}
      \end{center}
  \end{column}
\end{columns}
\end{onlyenv}
\vspace{1cm}
\tiny{Molnar, C, (2019). Interpretable machine learning. A Guide for Making Black Box Models Explainable. \url{https://christophm.github.io/interpretable-ml-book/.}\par}
}

\begin{vbframe}{LIME for Images and Text}
LIME for images and texts differs from LIME for tabular data. \\[0.2cm]
Text classification: 
\begin{itemize}
    \item Each instance is represented as a binary vector indicating the presence or absence of a word.
  \item Starting from the original text, new texts are created by randomly removing words from the original text. 
\end{itemize}

\begin{figure}
\begin{center}
%\captionsetup{font = scriptsize, labelfont = {bf, scriptsize}}
 \includegraphics[width=0.9\textwidth]{figure/lime_movier}
\end{center}
\end{figure}


 \scriptsize{\textbf{Figure:} LIME for two instances of a labeled movie review dataset. One half is labeled as positive reviews (\textcolor{orange}{1}), 
the other halfs as negative reviews (\textcolor{blue}{0}).Words like ``worst`` or ``waste`` indicate a negative review while words like ``best`` or ``great`` indicate a positive review.}

\vspace{0.3cm}
{\tiny{Shen, Ian, (2019, March). Explain sentiment prediction with LIME.
\url{https://medium.com/just-another-data-scientist/explain-sentiment-prediction-with-lime-f90ae83da2da}}\par}

\framebreak

\normalsize
LIME for images and texts differs from LIME for tabular data. \\[0.2cm]
Image classification: 
  \begin{itemize}
  \item Variations of the images are created by segmenting the image into $"$superpixels$"$ and turning superpixels off or on. 
  \item Superpixels are interconnected pixels with similar colors. They are used because a single pixel would probably not change a prediction by much.
  \item A binary vector indicates the presence or absence of these superpixels.
\end{itemize}
\vspace{-0.3cm}
\begin{center}
 \includegraphics[width=0.8\textwidth]{figure/lime-images}
\end{center}
\vspace{-0.3cm}
\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier. Retrieved from \url{https://github.com/marcotcr/lime}\par}
\end{vbframe}

\endlecture
\end{document}

