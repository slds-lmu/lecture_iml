\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}
	
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\newcommand{\titlefigure}{figure/counterfactuals_heat.png}
    \newcommand{\learninggoals}{
    	\item See two strategies to generate CEs
    	\item Know problems and limitations of CEs}
	
	\lecturechapter{Methods \& Discussion of CEs}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------


\begin{vbframe}{Overview of Methods}
	Currently, multiple methods exist to calculate counterfactuals. They differ greatly in: 
	\begin{itemize}
		\item \textbf{Targets:} Most methods focus on classification models, only few cover regression models. So far, all remain in the supervised learning paradigm.
		\item \textbf{Data Types:} Mainly for tabular data, few methods focus on visual/text data, none on audio data.
		\item \textbf{Feature space:} Some methods can only handle numerical features, few can process discrete and continuous feature spaces. 
		\item \textbf{Objectives:} Many methods focus on action guidance, plausibility and sparsity, few on other objectives like fairness or individual preferences.
		\framebreak
		\item \textbf{Model access:} Methods could require access to complete model internals, access to gradients or only to prediction functions. Therefore, both model-agnostic and model-specific methods exist.
		\item \textbf{Optimization tool:} Gradient-based algorithms (only for differential models), mixed-integer programming (only linear) or gradient-free algorithms e.g. Nelder-Mead, genetic algorithm. 
		\item \textbf{Rashomon Effect:} Many methods return a single counterfactual per run, some multiple counterfactuals, others prioritize CEs or let the user choose.
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Method: Wachter et. al (2018)}

	Wachter et al. were the first to introduce counterfactual explanations in the context of machine learning predictions. They suggest to solve
		\begin{equation}
			\argmin_{\xv'} \max_{\lambda} \lambda \underbrace{(\fh(\xv') - y')^2}_{o_1(\fh(\xv'), y')} + \underbrace{\sum\nolimits_{j = 1}^p |x'_j - x_j|/MAD_j}_{o_2(\xv', \xv)}.
			\label{eq:wachter}
		\end{equation}
	$MAD_j$ is the median absolute deviation of feature $j$. In each iteration, optimisers like ADAM or Nelder-Mead solve Eq.~(\ref{eq:wachter}) for $\xv'$ and then $\lambda$ is increased until a sufficiently close solution is found. \\[0.2cm]
	
	This optimization problem has several shortcomings: 	
	\begin{itemize}
		\item We do not know how to choose $\lambda$ a priori. 
		\item Due to the maximization of $\lambda$ we focus primarily on the minimization of $o_1$. Only if $\fh(\xv') = y'$, we focus on minimizing $o_2$. 
		\item The definition of $o_2$ only covers numerical features. 
		\item Sparsity and plausibility of counterfactuals are neglected. 
	\end{itemize}
	


	\footnote[frame]{Wachter S, Mittelstadt B, Russel C (2017). Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR. Harvard Journal of Law \& Technology, 31 (2), 2018. \url{http://dx.doi.org/10.2139/ssrn.3063289}}
\end{vbframe}

\begin{vbframe}{Method: Dandl et al. (2020)}
	\begin{itemize}
		\item Instead of collapsing objectives into a single objective, we could optimize all four objectives simultaneously
	$$	\argmin_{\xv'} \left(o_1(\fh(\xv'), y'), o_2(\xv', \xv), o_3(\xv', \xv), o_4(\xv', \Xmat) \right). $$
		
		\item Note that weighting parameters like $\lambda$ are not necessary anymore. 
		\item This approach is called Multi-Objective Counterfactual Explanations (MOC) and was developed by Dandl et al. in 2020. 
		\item They adjusted the multi-objective genetic algorithm (NSGA-II) to produce a set of diverse counterfactuals for mixed discrete and continuous feature spaces.
		\item Instead of one, the algorithm returns a set of counterfactuals that represents different trade-offs between the objectives and are constructed to be diverse in feature space.
	\end{itemize}

	\footnote[frame]{Dandl et al. (2020) Multi-Objective Counterfactual Explanations. In: Bäck T. et al. (eds) Parallel Problem Solving from Nature – PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham.}
	
	 
\end{vbframe}

\begin{vbframe}[allowframebreaks]{Example: Credit Data}
	\begin{itemize}
		\item Model: SVM with RBF kernel
		\item $\xv$ is the first data point of the dataset with $\P(y = good)  = 0.34$ of being a ``good" customer.  
		\item Our desired goal is to increase the probability to $[0.5, 1]$.
		\item MOC (with default parameters) found 82 counterfactuals after 200 iterations that met the target.
		\item All counterfactuals proposed changes to the credit duration and many of them to the credit amount.  
		\item We can visualize feature changes with a parallel plot and 2-dim surface plot. 
		\item The parallel plot reveals that all counterfactuals had values equal to or smaller than the values of $\xv$.
		\item The surface plot illustrates why these feature changes are recommended. 
		\item Counterfactuals in the lower left corner seem to be in a less favorable region far from $\xv$, but they are in high density areas close to training samples (indicated by histograms).
	\end{itemize}
	\framebreak
	\begin{columns}
				\begin{column}{0.5\textwidth}  
			\begin{center}
				\includegraphics[width=0.75\textwidth]{figure/counterfactuals_credit_parallel}
			\end{center}
		\vspace{-0.2cm}
			\scriptsize{\textbf{Figure:} Parallel plot. 
				The grey lines show the feature values of the counterfactuals $\xv'$, the blue line corresponds to the values of $\xv$. Features without proposed changes are omitted. The bold numbers give minima and maxima of numeric features.} 
			
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{center}
				\includegraphics[width=1\textwidth]{figure/counterfactuals_credit_heat}
			\end{center}
		\vspace{-0.2cm}
			\scriptsize{\textbf{Figure:} Response surface plot. 
				The white dot is $\xv$, black dots are $\xv'$. The histograms display the marginal distribution of the training data $\Xmat$.} 
				
		\end{column}
	\end{columns}
\footnote[frame]{Dandl S., Molnar C., Binder M., Bischl B. (2020) Multi-Objective Counterfactual Explanations. In: Bäck T. et al. (eds) Parallel Problem Solving from Nature – PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham.}
\end{vbframe}


%\begin{vbframe}{Example: Bike Sharing Dataset}
%	\begin{itemize}
%		\item Model: Random Forest with 500 trees
%		\item $\xv$ is the first data point of the dataset with $\fh(\xv) = 1767.93$ rental bikes. 
%		\item Our desired goal is to increase the count of total rental bikes to $y' = [3000, \infty[$
%		\item MOC (with default parameters) found 56 counterfactuals after 200 iterations that met the target.
%		\item Most counterfactuals proposed to decrease the humidity (94.6 \%) and more than half to increase the temperature (55.4\%). 
%		\item Some counterfactuals proposed additional changes to the year (2012 instead of 2011) and month (December instead of Januar).
%		\framebreak 
%		\item We can visualize feature changes with a parallel plot. 
%		\item For humidity and temperature, we can additionally show a 2-dim surface plot. 
%	\end{itemize}
%	\vspace{-0.5cm}
%	\begin{columns}
%		\begin{column}{0.5\textwidth}
%			\begin{center}
%				\includegraphics[width=1\textwidth]{figure/counterfactuals_bike_sp}
%			\end{center}
%		
%			\scriptsize{\textbf{Figure:} Response surface plot. 
%				The white dot is $\xv$, black dots are $\xv'$. The histograms display the marginal distribution of the training data $\Xmat$.} 
%				
%		\end{column}
%		\begin{column}{0.5\textwidth}  
%			\begin{center}
%				\includegraphics[width=1\textwidth]{figure/counterfactuals_bike_para}
%			\end{center}
%		
%		\scriptsize{\textbf{Figure:} Parallel plot. 
%			The grey lines show the feature values of the counterfactuals $\xv'$, the blue line corresponds to the values of $\xv$. Features without proposed changes are omitted. The bold numbers give minima and minima of numeric features while character strings indicate categories of character features.} 
%		
%		\end{column}
%	\end{columns}
%\end{vbframe}

\begin{vbframe}[allowframebreaks]{Problems, Pitfalls, \& Limitations}
\begin{itemize}
    \item \textbf{Illusion of model understanding:} 
    CEs explain ML decisions by pointing to few specific alternatives. This reduces complexity, but is limited in explanatory power. Psychologists showed that even though the perceived model-understanding of end-users increases, the objective model-understanding remains unchanged.
    
    \item \textbf{Finding the right metric:} Similarity is the crucial concept for finding good CEs. However, our concept of similarity is context and domain dependent. E.g. while L1 can be a reasonable notion for tabular data, it is counterintuitive for image data. Sparsity is often desirable for end-users but not for data scientists searching for biases in the model.
    
    \item \textbf{Confusing Model and Real-World:} As pointed out before, explanations of the model do not easily transfer to the process in which a model is applied. This information should be conveyed to the end-user.
    
    \framebreak
    \item \textbf{Disclosing too much information:} CEs can reveal too much information about the model and help potential attackers.
    \item \textbf{Rashomon effect:} One, few, all? Which CEs should be shown to the end-user? There is no universal solutions, the right way depends on the end-users temporal resources and knowledge. 
    \item \textbf{Actionability vs. fairness:} Some authors suggest to focus only on the actionability of CEs. However, this counteract functions like contestability. E.g., if ethnicity is not permuted in a CE since it is not actionable this could lead to hiding racial biases in the model.
    \item \textbf{Attacking CEs:} Researchers can create models with great performance, which generate arbitrary explanations specified by the ML developer. Thus, the question is how faithful CEs are to the models underlying mechanism.
    \item \textbf{Assumption of constant model:} To provide guidance for the future, CEs assume that their underlying model would not change in the future. However, in reality this assumption is often violated and CEs are not reliable anymore. 
\end{itemize}

%	\textbf{Pitfall 3:} Rashomon Effect
%	\begin{itemize}
%		\item Due to the Rashomon Effect, multiple counterfactual explanations could be found for an instance. 
%		\item If all counterfactuals are reported the user could be overwhelmed. Instead of a comprehensible explanations for a prediction, users received an even more complex explanations.
%		\item Another option is to only report the ``best" ones. But this requires a notion for ``superiority".  
%		\item Furthermore, users might not be interested in the ``best" but most ``diverse" counterfactuals.
%		\item The best option might be to report all counterfactuals but let the user decide which one to select, e.g., based on their previous knowledge. 
%	\end{itemize}
%	\textbf{Pitfall 4:} 
%	\begin{itemize}
%		\item 
%	\end{itemize}
%	\textbf{Pitfall 5:} Confusing model explanation with real data process explanations
%	\begin{itemize}
%		\item Causal dependencies
%		\item Fixed model at time $t$ 
%		\item Wrong input by user
%	\end{itemize}
\end{vbframe}

\endlecture
\end{document}