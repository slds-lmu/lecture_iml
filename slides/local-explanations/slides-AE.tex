\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
 \newcommand{\titlefigure}{figure/AEturtle.jpg}
\newcommand{\learninggoals}{
\item Understand the definition of AEs and their relation to Counterfactual Explanations
\item Understand different methods that generate AEs
\item Know the debate of why AEs arise and standard defenses against them}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\lecturechapter{\Large{Local Explanations: Adversarial Examples}}
\lecture{Interpretable Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Adversarial Machine Learning}
Computer systems must be \textbf{robust}, that means they have to be able to cope with errors during execution.
\begin{itemize}
\item \textbf{Adversarial ML} is the field that studies the robustness of Machine Learning (ML) algorithms to malicious input.
\item We differentiate between three different kind of attacks: Evasion, Poisoning, and Model Stealing.
\item \textbf{Evasion attacks} allow to mislead an employed ML model with manipulated inputs. These attacks are focused in this lecture.
\item In \textbf{Poisoning}, an attackers enters malicious inputs in the training dataset.
\item \textbf{Model Stealing} encompass methods by which an attacker can recover a model or the data it was trained on.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Adversarial Examples}
The inputs by which evasion attacks can be conducted are called \textbf{Adversarial Examples (AEs)}
\begin{itemize}
\item An AE is an input to a model that is deliberately designed to "fool" the model into misclassifying it.
\item AEs occur even for ML algorithms with very high accuracy scores on a test set.
\item Deep Learning (DL) algorithms like Convolutional Neural Networks (CNNs) are particular vulnerable to such attacks, but, classical ML models are often too [cite Dalvi].
\item AEs that are created from a real data instance $\xv$ are often indistinguishable from $\xv$ by a human observer. Such cases suggest that even models with a very good test set performance have no deep understanding of the underlying concepts.% that determine the correct output label.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Examples: Model-Attacks}
\begin{figure}[h]
\centering
  \includegraphics[width=0.6\linewidth]{figure/AEduckSound.png}
  \caption{On the left side, we see real instances of image and audio data  correctly classified. The middle column, depicts a noisy-input in the same respective spaces. Column three, shows the summation of the original inputs plus the noisy input multiplied by a small scalar. The resulting data-points are AEs as they are assigned the wrong label.}
  \label{fig:mnist}
\end{figure} 


%eykholt stop sign
%brown turtle
%Gong duck and sound
%@inproceedings{inproceedings,
%author = {Gong, Yuan and Poellabauer, Christian},
%year = {2018},
%month = {03},
%pages = {},
%title = {An Overview of Vulnerabilities of Voice Controlled Systems}
%}
\end{vbframe}

\begin{vbframe}{Examples: Real-World-Attacks}
\begin{figure}[h]
\centering
\includegraphics[width=0.46\linewidth]{figure/AEstop.png}\quad \includegraphics[width=0.45\linewidth]{figure/AEturtle.jpg}
  \caption{The images show AEs employed in a real-world application. The stop sign on the right is an input designed to resemble a normal stop sign with graffiti on it like the one on the right. The right stop sign is misclassified as a right of way sign. In the second image we see a 3D-print turtle. It is designed in a way that it is misclassified as a rifle from every angle of presentation.}
  \label{fig:mnist}
\end{figure} 


%eykholt stop sign
%brown turtle
%Gong duck and sound
%@inproceedings{inproceedings,
%author = {Gong, Yuan and Poellabauer, Christian},
%year = {2018},
%month = {03},
%pages = {},
%title = {An Overview of Vulnerabilities of Voice Controlled Systems}
%}
\end{vbframe}

\begin{vbframe}{Example: Tabular Data Setup}
For tabular data, it is difficult to define e.g. the ground-truth or imperceptibility. Expert knowledge is required in evaluation.
\begin{itemize}
    \item Experts focus on the most important features in their judgment.
%    \item Tabular features have specific natural ranges that must be respected.
    \item An AE arises from manipulating features the model deems important but the expert does not.
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{figure/AEloanApplication.png}
  \caption{The image depicts the decision boundary of a classifier that decides about loan applications.}
  \label{fig:mnist}
\end{figure} 
\end{vbframe}

\begin{vbframe}{AEs and Interpretability}
From the examples we can see that AEs pose security threats on the use of ML algorithms in real-world applications. But what do they have to do with interpretability?

\begin{itemize}
    \item AEs show where models fail and can therefore increase the understanding of the constructor of the model.
    \item AEs motivate the need for interpretability.
    \item Interpretation techniques can provide insight of how to improve ML algorithms and make them more robust against AEs.
    \item Explanations to end-users may enclose too much information about the used model. This information can be used to build adversarial attacks.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Formal Definition}
\begin{block}{Adversarial Input}
Let $\epsilon>0$, $f:\Xspace \rightarrow \Yspace$ be an ML model, $y'\in \Yspace$ be a desired output, and $\xv \in \Xspace$ be an input correctly classified i.e. $f(\xv)=y_{\xv,true}$. We call $a_{\xv}$ an \textbf{adversarial input} to $\xv$ if:
\begin{equation*}
    \| a_{\xv}- \xv\|<\epsilon\text{ and } f(a_{\xv})\neq y_{a_{\xv},true}=f(\xv).
\end{equation*}
Moreover, we call $a_{\xv}$ \textbf{targeted} if $f(a_{\xv})=y'$.
\end{block}
\begin{itemize}
    \item Intuitively speaking an adversarial is a data-point close to a real, correctly classified input that is misclassified.
    \item It is called targeted if the class it is assigned to is determined.
    \item An AE is the depiction of such an input in visual, textual or any other form. 
\end{itemize}
\end{vbframe}


\begin{vbframe}{AEs and Counterfactual Explanations}
It seems as if AEs are defined very similarly to Counterfactual Explanations (CEs). Both, adversarials and counterfactuals describe inputs close to a given input $\xv$ that gets a different assignment. What are the differences?
\begin{itemize}
    \item Counterfactuals do not have to be misclassified.
    \item Counterfactuals should be maximally close to $\xv$.
    \item Different notions of distance $\|\cdot\|$ are applied e.g. $p_{2,\infty}$-norm for AEs or $p_{0,1}$-norm for CEs.
    \item Informal Difference I: AEs are largely considered for high-dimensional data, while CEs mostly in the context of low-dimensional data.
    \item Informal Difference II: AEs hide changes while CEs highlight them.
    \item \textbf{Joint Example:} If you had two more pets, your loan application would have been granted.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Why Exist AEs?}
There are many hypotheses about why AEs occur. We present a non-exhaustive list in historic order:
\begin{itemize}
    \item \textbf{Low-Probability Spaces Hypotheses:} AEs live in low-probability yet dense spaces in the data manifold that are not represented in the training samples.
    %@article{szegedy2013intriguing,
 % title={Intriguing properties of neural networks},
 % author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
 % journal={arXiv preprint arXiv:1312.6199},
 % year={2013}
%}
    \item \textbf{Linearity Hypotheses (most popular):} Adversarial examples are omnipresent in the data manifold. They occur, because of the very linear behavior of CNNs (using ReLU as it's easy to optimize). Small changes of $\epsilon$ in every feature make a resultant change of $\epsilon\|w\|_1$ in prediction (for $w$ the weights in the linear function).
    %@article{goodfellow2014explaining,
 % title={Explaining and harnessing adversarial examples},
 % author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
 % journal={arXiv preprint arXiv:1412.6572},
 % year={2014}
%}
    \item \textbf{The Boundary Tilting Hypothesis:} Linearity is neither necessary nor sufficient to explain AEs. Instead, AEs mostly result from overfitting the sampled manifold.
 %   @article{tanay2016boundary,
 % title={A boundary tilting persepective on the phenomenon of adversarial examples},
 % author={Tanay, Thomas and Griffin, Lewis},
 % journal={arXiv preprint arXiv:1608.07690},
 % year={2016}
%}
    \item \textbf{Human-Centric Hypotheses:} ML models make use of predictive but non-robust features. These are features that are highly correlated with the prediction target, but not used by humans.
    %cite: @article{ilyas2019adversarial,
%  title={Adversarial Examples Are Not Bugs, They Are Features},
%  author={Ilyas, Andrew and Santurkar, Shibani and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
%  journal={Advances in neural information processing systems},
%  volume={32},
%  year={2019}
%}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Ways to Generate AEs}
There exist various ways in the literature to generate such AEs for a given model in feasible time. We can...
\begin{itemize}
    \item formulate the search for AEs as an \textbf{optimization problem}, e.g. 
    \begin{equation*}
        \label{eq:optimization}
        \underset{\xv'\in \Xspace}{\text{argmin}}\; \| \xv-\xv' \|+\lambda\;    \vertiii{f(\xv')-y'}.
    \end{equation*}
    \item use \textbf{sensitivity analysis} to find out about features that influence the target class.
    \item train a generative network to generate adversarials for a given model.
\end{itemize}
Moreover, dependent on the model access of the attacker we can distinguish between...
\begin{itemize}
    \item \textbf{White-Box Attacks}: the attacker has full access to the internals of the model.
    \item \textbf{Black-Box Attacks}: the attacker can only query the model on some inputs and receives the model's outputs.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Fast-Gradient-Sign-Method (FGSM)}
We have seen optimization problem approaches already for CEs. Therefore, we focus now on a method using sensitivity analysis, namely FGSM.
\begin{itemize}
    \item FGSM is based on the Linearity Hypotheses.
    \item FGSM finds AEs from:
    \begin{equation*}
        a_{\xv}=\xv+\epsilon\cdot\text{sign}(\nabla_{\xv} J(\theta,\xv,y_{\xv,true}))
    \end{equation*}
    where $\text{sign}(\nabla_{\xv} J(\theta,x,y_{\xv,true}))$ describes the component-wise signum of the Gradient of cost function $J$ in $\xv$ with true label $y_{\xv,true}$.
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figure/AEpanda.png}
  \caption{An AE generated with the FGSM}
  \label{fig:mnist}
\end{figure} 
\end{vbframe}

\begin{vbframe}{Fast-Gradient-Sign-Method (FGSM)}
\begin{itemize}
    \item FGSM works particularly well for linear-like models in high-dimensional spaces e.g. LSTMs, logistic regression, CNNs with ReLU activations.
    \item Not every vector generated by FGSM must be an AE, especially if $\epsilon$ is too small.
    \item FGSM-attacks can be also generated without model-access by approximating the gradient i.e. with finite difference methods.
    \item The notion of similarity in FGSM is based on $\|\cdot\|_{\infty}$. However, there exist generalizations of FGSM to other norms.
\end{itemize}
%@inproceedings{lyu2015unified,
%  title={A unified gradient regularization family for adversarial examples},
%  author={Lyu, Chunchuan and Huang, Kaizhu and Liang, Hai-Ning},
%  booktitle={2015 IEEE international conference on data mining},
%  pages={301--309},
%  year={2015},
%  organization={IEEE}
%}
\end{vbframe}


\begin{vbframe}{Black-Box Attacks with Surrogates}
The biggest threat comes from AEs that can be generated in a black-box scenario. Among these, solution-approaches through surrogates are conceptually most interesting.
\begin{itemize}
    \item Query the model you aim to attack on data similar to the training data.
    \item Use the labeled data you received to train a surrogate model by supervised learning e.g. a CNN.
    \item Use white-box methods (e.g. Gradient-Based-Methods) to generate AEs for the surrogate model for the given optimisation problem.
    \item Use these AEs to attack the original model.
\end{itemize}
Several experiments showed that such attacks are often successful. Papernot et al. call this surprising phenomenon the \textbf{Transferability} of AEs.
%
%
%@article{papernot2016transferability,
  %title={Transferability in machine learning: from phenomena to %black-box attacks using adversarial samples},
  %author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow,
  %Ian},
%  journal={arXiv preprint arXiv:1605.07277},
%  year={2016}
%}
\end{vbframe}

\begin{vbframe}{Defenses Against AEs}
There are several ways to protect your network against such attacks. We distinguish between two broad types of defenses, differing in the position in which they act.
\begin{itemize}
    \item \textbf{Guards:} Act on the inputs a model receives. Includes methods to \textbf{detect anomalies} (e.g. statistical testing, or discriminator networks from GANs), or \textbf{conduct transformations} on inputs (e.g. PCA).
    \item \textbf{Defense by Design:} Act on the employed model itself. Includes methods like \textbf{adversarial training} (train model on adversarials) or \textbf{architectural defenses} (e.g. removing low-predictive features from the model or imposing layer-wise contraints on output-variance) to make models more robust against AEs.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Regularization Against AEs}
The use of regularization techniques against AEs is motivated by most hypotheses that explain AEs. As an example we look at a technique based on the FGSM and the linearity hypotheses.
\begin{itemize}
    \item Goodfellow et al. suggest to specfiy the cost function s.t.
    \begin{equation*}
        \tilde{J}(\theta,\xv,y):=\alpha J(\theta,\xv,y)+(1-\alpha) J(\theta,\xv+\epsilon\cdot\text{sign}(\nabla_{\xv} J(\theta,\xv,y)),y)
    \end{equation*}
    with $\alpha=0.5$.
    \item This increased the model-robustness against FGSM generated AEs.
    \item However, model are not only still prone to other AEs but also to many FGSM-AEs.
    \item Applying such regularization terms can increase test-error.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Summary}
\begin{itemize}
    \item AEs are not explanations themselves, but are strongly connected to them conceptually.
    \item AEs can be generated in white-box and black-box settings. Crucial modelling-decisions are the distance measure, the local-environment, and the target-level (model or process).
    \item There are various hypotheses on the existence of AEs which also motivate different defense strategies.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Outlook}
\begin{itemize}
    \item Even in highly non-linear models AEs occur. As long as their existence is not well-understood, defenses against them will have limited power.
    \item More and more different distance measures are considered, e.g. $p_0$ for one pixel attacks, the Wasserstein-metric or psychologically motivated measures like the Perceptual Adversarial Similarity Score (PASS).
    \item Recent work considered AEs that fool both, humans and ML models. AEs may be a case where research on human and machine perception can profit from each other.
\end{itemize}
%@misc{rozsa2016adversarial,
%      title={Adversarial Diversity and Hard Positive Generation}, 
%      author={Andras Rozsa and Ethan M. Rudd and Terrance E. Boult},
%      year={2016},
%      eprint={1605.01775},
%      archivePrefix={arXiv},
%      primaryClass={cs.CV}
%}
\end{vbframe}




\endlecture
\end{document}
