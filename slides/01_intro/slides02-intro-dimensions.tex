\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\definecolor{imllightblue}{RGB}{184, 198, 210}
\definecolor{imlmedblue}{RGB}{90, 122, 151}
\definecolor{imldarkblue}{RGB}{7, 55, 99}
\definecolor{imlhuegreen}{RGB}{0, 191, 196}
\definecolor{imlhuered}{RGB}{248, 118, 109}

\tikzset{main node/.style={rectangle,draw,minimum size=1cm,inner sep=4pt},}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
%\item What is interpretable machine learning (IML) and Explainable Artificial Intelligence (XAI)?
%: feature attribution vs. data attribution vs. counterfactual explanations
\item Intrinsic vs. model-agnostic methods
\item Different types of explanations
\item Local vs. global methods
\item Model or learner explanations -- with or without refits
}

\lecturechapter{Dimensions of Interpretability}
\lecture{Interpretable Machine Learning}
%
% \begin{frame}{Interpretable ML}
% \begin{itemize}
% %\itemsep2em
% \item ML algorithms algorithmically train predictive models with no or little pre-specifications and assumptions about the data.
% \item Several algorithms such as decision tree learning create interpretable models. However, most algorithms create models which can be considered a black box.
% \item We use the term black box, although the internal workings of the model are in fact accessible, but too complex for the human mind to comprehend.
% \end{itemize}
% \end{frame}
%
% \begin{frame}{Explainable AI}
% \begin{itemize}
% %\itemsep1em
% \item IML is often used synonymously with Explainable AI (XAI).
% \item There is no unified standard for these terminologies. We find that XAI often is specifically concerned with the interpretation of neural networks, whereas IML is used as an encompassing term for everything related to model interpretability.
% \item The nature of (deep) neural networks allows for powerful model-specific interpretation techniques, e.g., layer-wise relevance propagation (LRP) and saliency maps.
% \item Also covering model-specific NN methods would exceed the timeframe of this lecture. This lecture will concentrate on model-agnostic techniques, as they are both versatile, and receive a lot of attention in industry and academia.
% \end{itemize}
% \end{frame}
%
% \begin{frame}{XAI - Saliency Maps}
%
% A saliency map is a heatmap indicating pixel influence on the prediction (e.g., a classification of an image): \footnote[frame]{Mundhenk, T., Chen, B.Y., Friedland, G. (2019). Efficient Saliency Maps for Explainable AI. ArXiv, abs/1911.11293.
% }
% \medskip
% \begin{figure}
% \includegraphics[width = 0.8 \textwidth]{figure/saliencymap}
% \end{figure}
% \end{frame}
%
% \begin{frame}{What is Interpretability?}
% \begin{itemize}
% %\itemsep1em
% \item There is no scientific consensus on the definition of interpretability.
% \item We need to differentiate between interpretations of a model or reality. The latter is distorted by all modeling fallacies involved in predictive modeling, e.g., data quality, under- and overfitting, or model extrapolations.
% \item We use a practical definition of interpretability.
% Think back to the foundations of statistical modeling:  the linear regression model (LM). The LM, with its known equation of beta coefficients, represents a paradigm for statistical interpretability.
% \item It follows that it would be beneficial to create techniques that give us an interpretation similar to the one of an LM.
%
% \end{itemize}
% \end{frame}

% \begin{frame}{Dimensions of Interpretability}

% Interpretation methods can be organized in different dimensions, e.g.:
% \medskip
% 	\begin{itemize}
% 		\itemsep1em
% 		\item Intrinsic vs. model-agnostic interpretability.
% 		\item Type or style of explanations: \\
% 		feature attribution vs. data attribution vs. counterfactual explanations.
%     \item Global vs. local interpretations.
% 		%\item Feature effects vs. feature importance.
% 		\item Fixed model analysis vs. model refits.
% 	\end{itemize}
% \end{frame}


% \begin{frame}{Intrinsic vs. Model-Agnostic}
% %	\begin{center}
% %		\includegraphics[width=0.8\textwidth]{figure/overview}
% %	\end{center}
% 	\begin{center}
	    
% 		\begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
%                         every label/.append style={align=left, font=\footnotesize, text width=4cm}]
%         \node[main node] (1) { Model Interpretation };
%         \node[main node,
%             label={below: 
%             \tab[0.25cm] - Decision trees \\ 
%             \tab[0.25cm] - Decision rules \\ 
%             \tab[0.25cm] - GLMs}
%             ] (2) [below left = 1cm and 0cm of 1]  { Interpretable Models };
%         \node[main node] (3) [below right = 1cm and -0cm of 1] { Black Box Models };
%         \node[main node,
%             label={below: 
%              - \textbf{Advantage:} Can exploit model structure\\
%             - Random forest explainer \\ - Visualize activations of NNs}
%             ] (4) [below left = 1cm and -1cm of 3] { Model-specific Methods  };
%         \node[main node, fill=lightgray,
%             label={below:
%             - \textbf{Advantage:} Applicable to any model\\ 
%             - Feature effect methods\\
%             - Feature importance methods}
%             ] (5) [below right = 1cm and -1cm of 3] { Model-agnostic Methods };
%         \draw (1) -- (2);
%         \draw (1) -- (3);
%         \draw (3) -- (4);
%         \draw (3) -- (5);

%     \end{tikzpicture}
% 	\end{center}
% \end{frame}

\begin{frame}{Intrinsic vs. Model-Agnostic}
%	\begin{center}
%		\includegraphics[width=0.8\textwidth]{figure/overview}
%	\end{center}
	\begin{center}
	    
	    \resizebox{9.5cm}{3.5cm}{%
		\begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
                        every label/.append style={align=left, font=\footnotesize, text width=4cm}]
        \node[main node] (1) { Model Interpretation };
        \only<1>{\node[main node, fill=lightgray] (2) [below left = 1cm and 0cm of 1]  { Interpretable Models };}
        \only<2->{\node[main node] (2) [below left = 1cm and 0cm of 1]  { Interpretable Models };}
        \node[main node] (3) [below right = 1cm and -0cm of 1] { Black Box Models };
        \only<1,3->{\node[main node] (4) [below left = 1cm and -1cm of 3] { Model-specific Methods  };}
        \only<2>{\node[main node, fill=lightgray] (4) [below left = 1cm and -1cm of 3] { Model-specific Methods  };}
        \only<-2>{\node[main node] (5) [below right = 1cm and -1cm of 3] { Model-agnostic Methods };}
        \only<3->{\node[main node, fill=lightgray] (5) [below right = 1cm and -1cm of 3] { Model-agnostic Methods };}
        \draw (1) -- (2);
        \draw (1) -- (3);
        \draw (3) -- (4);
        \draw (3) -- (5);

    \end{tikzpicture}}
	\end{center}
	
	\only<1>{\begin{columns}
		\begin{column}{0.75\textwidth}
	\begin{itemize}
		\item<1-> Intrinsically interpretable models:
		\medskip
		
		\begin{itemize}
		%\itemsep1em
			\item Examples: linear model, decision tree, decision rule, GLMs
			\item Interpretable because of simple model structure, \\
			e.g., weighted combination of feature values or tree structure
			\item Difficult to interpret with many features / complex interactions
		\end{itemize}
	\end{itemize}
	\end{column}
	\begin{column}{0.25\textwidth}
	
	\resizebox{2.75cm}{3cm}{%	
   \begin{tikzpicture}[scale=0.7, transform shape]
   \usetikzlibrary{arrows}
    \usetikzlibrary{shapes}
     \tikzset{treenode/.style={draw, circle, font=\small}}
     \tikzset{line/.style={draw, thick}}
     \node [treenode, draw=red] (a0) {$a_0$};
     \node [treenode, below=0.75cm of a0, xshift=-1cm]  (a1) {$a_1$};
     \node [treenode, draw=red, below=0.75cm of a0, xshift=1cm]  (a2) {$a_2$};
     
     \node [treenode, draw=red, below=0.75cm of a2, xshift=-1cm] (a3) {$a_3$};
     \node [treenode, below=0.75cm of a2, xshift=1cm]  (a4) {$a_4$};
     
     \node [treenode, below=0.75cm of a3, xshift=-1cm] (a5) {$a_5$};
     \node [treenode, below=0.75cm of a3, xshift=1cm]  (a6) {$a_6$};
     
     \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<0.3$};
     \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq0.3$};
     
     \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_1<0.6$};;
     \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_1\geq0.6$};
     
          
     \path [line] (a3.south) -- + (0,-0.4cm) -| (a5.north) node [midway, above] {$x_2<0.2$};;
     \path [line] (a3.south) -- +(0,-0.4cm) -|  (a6.north) node [midway, above] {$x_2\geq0.2$};
     
   \end{tikzpicture}}
	\end{column}
	\end{columns}}

	
	\only<2>{\begin{columns}
	\begin{column}{0.8\linewidth}
	    \begin{itemize}
	        \item Model-specific methods:
		    \medskip
			\begin{itemize}
		    %\itemsep1em
		        \item Interpretation method being applicable to a particular ML model
		        \item Example: implicitly integrated feature interpretation methods in tree based models, e.g., Gini Importance
    			\item Advantage: Can exploit model structure
                \item Visualize activations of NNs
		    \end{itemize}
		\end{itemize}
	\end{column}  
	\begin{column}{0.2\linewidth}
	
    \centering \includegraphics[page=1, width=\textwidth]{figure/nn_model}
    \end{column}
	\end{columns}}
		
	
	
	\only<3>{\begin{columns}
	\begin{column}{0.7\linewidth}
	    \begin{itemize}
		\item Model-agnostic methods:
		\medskip
		\begin{itemize}
		%\itemsep1em
			\item In ML: Tune over many model classes \\ 
            $\leadsto$ Unknown which model is best / deployed \\
            $\leadsto$  Need for interpretation methods applicable to any model
		    \item Applied after training (post-hoc)
      \item Applicable to intrinsically interpretable models\\
       $\leadsto$ provides insights into other types of explanations \\
		      %\item Can also be applied to intrinsically interpretable models to additionally gain insights into other types of explanations 
		      %$\leadsto$ feature or data attribution, counterfactual explanations
   \end{itemize}
	\end{itemize}
	\end{column}
	\begin{column}{0.3\linewidth}
	\quad \vspace{0.15cm} \quad \\
    \centering \includegraphics[page=1, width=\textwidth]{figure/counterfactuals_obj}
    \end{column}
	\end{columns}}


 %    \only<4>{\begin{columns}
	% \begin{column}{0.7\linewidth}
 %        \begin{itemize}
 %        \item Model-agnostic methods:
 %        \medskip
	%     \begin{itemize}
	% 	      \item Model-agnostic methods work for any model \\ $\leadsto$ Only access to data and model required 
 %             \item Also applicable to intrinsically interpretable models to additionally gain insights into other types of explanations \\
	% 	      %\item Can also be applied to intrinsically interpretable models to additionally gain insights into other types of explanations 
	% 	      $\leadsto$ feature or data attribution, counterfactual explanations
 %        \end{itemize}
	% \end{itemize}
	% \end{column}
	% \begin{column}{0.3\linewidth}
	% \quad \vspace{0.15cm} \quad \\
 %    \centering \includegraphics[page=1, width=\textwidth]{figure/counterfactuals_obj}
 %    \end{column}
	% \end{columns}}
\end{frame}



% \begin{frame}{Model-Agnostic Interpretability}
% 	\begin{itemize}
% 		\itemsep1em
% 		\item Model-agnostic interpretation methods work for \textbf{any} kind of machine learning model.
% 		\item Explanation type is not tied to the underlying model type.
% 		\item Only access to data and trained model is required.\\
% 		 No further knowledge about the model itself is necessary.
% 		\item There are multiple types of explanations:\\
% 		feature attribution, data attribution, or counterfactual explanations.
% 	\end{itemize}
% \end{frame}


\begin{frame}{Types of Explanations}
% 	\begin{center}
% 		\includegraphics[width=\textwidth]{figure/1-attributions.png}
%     \end{center}
    \begin{center}
        \begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
                            every label/.append style={align=left, font=\footnotesize, text width=4cm},
                            scale=0.7, transform shape]
            \node[main node] (1) { Model Interpretation };
            \only<-3>{\node[main node, fill=lightgray] (2) [below left = 1.3cm and 1.5cm of 1]  { Feature Attribution };}
            \only<4->{\node[main node] (2) [below left = 1.3cm and 1.5cm of 1]  { Feature Attribution };}
            \only<1,2,3,7,8>{\node[main node, text width=3cm, align=center] (3) [below = 1.3cm of 1] { Data Attribution };}
            \only<4,5,6>{\node[main node, fill=lightgray, text width=3cm, align=center] (3) [below = 1.3cm of 1] { Data Attribution };}
            \only<-6>{\node[main node] (4) [below right = 1.3cm and 1.5cm of 1] { Counterfactual Explanations };}
            \only<7->{\node[main node, fill=lightgray] (4) [below right = 1.3cm and 1.5cm of 1] { Counterfactual Explanations };}
            \draw (1) -- (2);
            \draw (1) -- (3);
            \draw (1) -- (4);
    
        \end{tikzpicture} 
    \end{center}
    
    \only<1>{\begin{itemize}
	\item Feature Attribution:
	\medskip
		\begin{itemize}
		%\itemsep1em
		    \item Produce explanations on a per-feature level, 
		        e.g., feature effects or feature importance \\
		    $\leadsto$ Vary feat. values, inspect change of model prediction, model variance or model error
		\end{itemize}
	\end{itemize}}
	
	\only<2>{%Feature Effects vs. Feature Importance
	\textbf{Feature Effects} indicate the change in prediction due to changes in feature values.
	\medskip
	\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.4\linewidth}
	    \begin{itemize}
	    	\item Model-agnostic methods: \\ ICE curves, PD plots $\hdots$
	    	\item Pendant in linear models: \\Regression coefficient $\beta_j$
	    	\item Further examples: Saliency Maps, model-agnostic methods such as SHAP and LIME
	    \end{itemize}
	\end{column}
	\begin{column}{0.6\linewidth}
    \includegraphics[page=1, width=\textwidth, trim=0 0 215 0, clip]{figure/feature-effect}
    \end{column}
	\end{columns}
	}
	
	\only<3>{%Feature Effects vs. Feature Importance
	\textbf{Feature importance} methods rank features by how much they contribute to the predictive performance or prediction variance of the model.
	\medskip
	\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.55\linewidth}
	    \begin{itemize}
	    	\item Model-agnostic methods: PFI, $\hdots$
		\item Pendant in linear models: t-statistic, p-value (significant effect)
	    \end{itemize}
	\end{column}
	\begin{column}{0.45\linewidth}
    \includegraphics[width=0.7\textwidth]{figure/feature-importance}
    \end{column}
	\end{columns}
	}
	
	\only<4-6>{Data Attribution: Identify training instances most responsible for a decision (e.g. Influence Functions)}
	
	\only<5>{
	\textbf{Example:} Consider a model which should distinguish muffins and dogs
	\begin{center}
		\includegraphics[page=1, width=0.5\textwidth]{figure/Chihuahua_or_muffin_model.png}
	\end{center}
	How does this incorrect prediction come about? }
	
	\only<6>{
	Look at the training data set: Which of the training data points caused the model prediction?
	\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.5\linewidth}
	\begin{center}
		\includegraphics[page=1, width=0.9\textwidth]{figure/Chihuahua_or_muffin.png} 
	\end{center}
	
	\end{column}
	\begin{column}{0.5\linewidth}
	
    Method searches for the most similar images and bases the decision on them
    \begin{itemize}
        \item[$\leadsto$] The training images that look most like the new input represent a muffin 
        \item[$\leadsto$] Wrong output (muffin instead of dog)
    \end{itemize}
    \end{column}
	\end{columns}}
	
	\only<7>{\quad \vspace{0.15cm} \quad \\
    \begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.65\linewidth}
	\begin{itemize}
	\item Counterfactual Explanations:
	\medskip
		\begin{itemize}
		%\itemsep1em
            \item Identify smallest necessary change in feature values so that a desired outcome is predicted
		    \item Contrastive explanations 
            \item Diverse counterfactuals 
            \item Feasible \& actionable explanations
		\end{itemize}
    \end{itemize}
    \end{column}
	\begin{column}{0.3\linewidth}
	
        \centering \includegraphics[page=1, width=\textwidth]{figure/counterfactuals_obj}
    \end{column}
	\end{columns}}
	
	\only<8>{
	\textbf{Example} (loan application):
	\begin{center}
	\includegraphics[page=1, width=0.5\textwidth]{figure/counterfactual.pdf}
	\end{center}}
\end{frame}




% \begin{frame}{Feature Effects vs. Feature Importance}

% 	\textbf{Feature Effects} indicate the change in prediction due to changes in feature values.
% 	\medskip
% 	\begin{center}
% 		\includegraphics[page=1, width=0.7\textwidth, trim=0 0 215 0, clip]{figure/feature-effect}
% 	\end{center}
% 	\begin{itemize}
% 		\item Model-agnostic methods: ICE curves, PD plots $\hdots$
% 		\item Pendant in linear models: Regression coefficient $\beta_j$
% 	\end{itemize}
% \end{frame}

% \begin{frame}{Feature Effects vs. Feature Importance}

% 	\textbf{Feature importance} methods rank features by how much they contribute to the predictive performance or prediction variance of the model.
% 	\begin{center}
% 		\includegraphics[page=1, width=0.4\textwidth]{figure/feature-importance}
% 	\end{center}
% 	\begin{itemize}
% 		%\itemsep1em
% 		\item Model-agnostic methods: PFI, $\hdots$
% 		\item Pendant in linear models: t-statistic, p-value (significant effect)
% 	\end{itemize}

% \end{frame}


% \begin{frame}{Explanation using training instances~\citebutton{Koh et al. 2017}{https://arxiv.org/pdf/1703.04730.pdf}}

% 	\textbf{Data attribution:} Which training instances results in the decision for the instance $x$ of the model?
% 	\begin{center}
% 		\includegraphics[page=1, width=0.7\textwidth]{figure/fish-attribution.pdf}
% 	\end{center}
% \end{frame}

% \begin{frame}{Explanation using training instances~\citebutton{Koh et al. 2017}{https://arxiv.org/pdf/1703.04730.pdf}}
% 	\textbf{Data attribution:} Which training instances results in the decision for the instance $x$ of the model?
% 	\begin{center}
% 		\includegraphics[page=1, width=0.7\textwidth]{figure/prototypes-fish.pdf}
% 	\end{center}
% 	\begin{itemize}
% 		\item Methods:
% 		Influence functions, prototype generation
% 	\end{itemize}
% \end{frame}

% \begin{frame}{Explanation using training instances}
%     \textbf{Data attribution:} Which training instances results in the decision for the instance $x$ of the model?
% 	\vspace{0.4cm}
% 	\begin{center}
% 		\includegraphics[page=1, width=0.8\textwidth]{figure/Chihuahua_or_muffin_model.png}
% 	\end{center}
% \end{frame}

% \begin{frame}{Explanation using training instances}
% 	\textbf{Data attribution:} Which training instances results in the decision for the instance $x$ of the model?
% 	\begin{center}
% 		\includegraphics[page=1, width=0.3\textwidth]{figure/Chihuahua_or_muffin.png} 
% 	\end{center}
% 	\begin{itemize}
% 		\item[$\leadsto$]  Model chose the wrong output
% 	\end{itemize}
% \end{frame}

% \begin{frame}{Explanation using Counterfactuals}
%     A counterfactual is small ``imperceptible'' change in $x$ causing a different prediction.\\
%     $\leadsto$ What if a small difference $ |x - x'| \leq \epsilon$ to $x$ causes a large change in the model output ?
    
%     \textbf{Example} (loan application):
% 	\begin{center}
% 	\includegraphics[page=1, width=0.7\textwidth]{figure/counterfactual.pdf}
% 	\end{center}

% \end{frame}


\begin{frame}{Global vs. Local}
Global interpretation methods explain the model behavior for the entire input space by considering all available observations:
\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.65\linewidth}
	\begin{itemize}
		\item Permutation Feature Importance (PFI)
		\item Partial Dependence (PD) plots
		%\item Functional ANOVA (FANOVA)
		\item Accumulated Local Effect (ALE) plots
		\item ...
	\end{itemize}
\bigskip
Local interpretation methods explain the model behavior for single data instances:
	\begin{itemize}
		\item Individual Conditional Expectation (ICE) curves
		\item Local Interpretable Model-Agnostic Explanations (LIME)
		\item Shapley values, SHAP
		\item ...
	\end{itemize}
	\end{column}
	\begin{column}{0.35\linewidth}
	\begin{center}
	\includegraphics[page=1, width=0.7\textwidth, trim=215 0 215 43, clip]{figure/feature-effect}
	\medskip
	\includegraphics[width=0.7\textwidth]{figure/lime5}
	\end{center}
	\end{column}
	\end{columns}
\end{frame}

% \begin{frame}{Local vs. Global Explanations}
% % 	\begin{center}
% % 		\includegraphics[width=\textwidth]{figure/1-local-global.png}
% % 		%TODO: Remove ALE from local
% % 	\end{center}

% \begin{center}
%     \begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
%                         every label/.append style={align=left, font=\footnotesize, text width=6cm}]
%         \node[main node, text width=4cm, align=center] (1) { Model Interpretation };
%         \node[main node, text width=4cm, align=center, 
%             label={below:
%             \tab - Instance wise feature selections \\ 
%             \tab - Model-agnostic methods \\ \tab[1.5cm] - SHAP\\ 
%             \tab[1.5cm] - LIME}
%             ] (2) [below left = 1.3cm and 0cm of 1]  { Local Explanations };
%         \node[main node,  text width=4cm, align=center, 
%             label={below:
%             %Rule-based Explanations\\ 
%             \tab - Permutation feature importance \\ 
%             \tab - Functional ANOVA \\ 
%             \tab - PDP}
%             ] (3) [below right = 1.3cm and 0cm of 1] { Global Explanations };
%         \draw (1) -- (2);
%         \draw (1) -- (3);

%     \end{tikzpicture}  
% \end{center}
% \end{frame}


\begin{frame}{Fixed Model vs. Refits}
	\begin{itemize}
		\itemsep1em
		\item Most methods we will discuss analyze a fixed, trained model 
		\item Some methods require refitting of the model 
		\item Trained model $\Rightarrow$ Model is the object of analysis
		\item Refitting $\Rightarrow$ Learning process is the object of analysis
		\item  Advantage of refitting: It includes information about the variability in the learning process
	\end{itemize}
\end{frame}


% \begin{frame}{Intrinsic and Model-Agnostic Interpretation}
% \begin{itemize}
%   \item Intrinsically interpretable models:
%   \begin{itemize}
%   \item Examples are linear models and decision trees.
%   \item They are interpretable because of their simple structures, e.g.,
%   a weighted combination of feature values or a tree structure.
%   \item However, they are difficult to interpret with many features or complex interaction terms.
%   \end{itemize}
%   \lz
%   \item Model-agnostic interpretation methods:
%   \begin{itemize}
%   \item They are applied after training (post-hoc).
%   \item They also work for more complex black box models.
%   \item They can also be applied to intrinsically interpretable models, e.g.
%     feature importance for decision trees.
%   \end{itemize}
% \end{itemize}
% \end{frame}
%
% \begin{frame}{Model-Agnostic Interpretability}
%  \begin{itemize}
%   %\itemsep2em
%   \item Model-agnostic interpretability methods work for \textbf{any} kind of machine learning model.
%   \item Explanation type is not tied to the underlying model type.
%   \item Often, only access to data and fitted predictor is required. No further knowledge about the model itself is necessary.
%   \item We usually distinguish between \textbf{feature effect} and \textbf{feature importance} methods.
%  \end{itemize}
% \end{frame}
%
%
% \begin{frame}{Feature Effects vs. Feature Importance}
% \textbf{Feature effects} indicate the direction and magnitude of a change in predicted outcome due to changes in feature values.
% \begin{center}
% \includegraphics[page=1, width=\textwidth]{figure/feature-effects}
% \end{center}
%   \begin{itemize}
%     \item Methods include: Partial Dependence Plots, Individual Conditional Expectation, Accumulated Local Effects (ALE)
%     \item Pendant in linear models: Regression coefficient $\hat{\theta}_j$
%   \end{itemize}
% \framebreak
%
% \textbf{Feature importance} methods rank features by how much they contribute to the predictive performance or prediction variance of the model.
% \begin{columns}
% \begin{column}{0.6\textwidth}
% \begin{itemize}
%     %\itemsep1em
%     \item Methods include: Permutation Feature Importance, Functional Anova
%     \item Analog in linear models: Absolute t-statistic $\left|\frac{\hat{\theta}_j}{SE(\hat{\theta}_j)}\right|$
% \end{itemize}
% \end{column}
% \begin{column}{0.4\textwidth}
% \begin{center}
% \includegraphics[page=1, width=\textwidth]{figure/feature-importance}
% \end{center}
% \end{column}
% \end{columns}
% \end{frame}
%
%
% \begin{frame}{Global and Local Interpretability}
% Global interpretability methods explain the expected model behavior for the entire feature space by considering all available observations (or representative subsets). For example:
%   \begin{itemize}
%     \item Permutation Feature Importance
%     \item Partial Dependence Plot
%     \item Functional Anova
%     \item ...
%   \end{itemize}
% \lz
% Local interpretability methods explain single predictions or a group of similar observations. For example:
%  \begin{itemize}
%   \item Individual Conditional Expectation (ICE) Plots
%   \item Local Interpretable Model-Agnostic Explanations (LIME)
%   \item Shapley Values
%   \item ...
%  \end{itemize}
% \end{frame}
%
%
% \begin{frame}{Fixed model vs. refits}
%   \begin{itemize}
%      %\itemsep2em
%      \item Most methods presented in this lecture analyze a fixed, trained model
%      (e.g., permutation feature importance).
%      \item Some methods require refitting the model (e.g., PIMP).
%      \item Trained model $\Rightarrow$ Model is the object of analysis.
%      \item Refitting $\Rightarrow$ Learning process is the object of analysis.
%      \item The advantage of refitting is that it includes information about the variability in the learning process.
%   \end{itemize}
% \end{frame}

\begin{frame}{Levels of interpretability}
	\begin{center}
        \begin{tabular}{ >{\centering\arraybackslash}m{0.1\textwidth} >{\centering\arraybackslash}m{0.01\textwidth} >{\centering\arraybackslash}m{0.36\textwidth} >{\centering\arraybackslash}m{0.005\textwidth} >{\centering\arraybackslash}m{0.38\textwidth} } 
         && \textbf{Requirement} && \textbf{Approach} \\ &&&\\
         
         1\textsuperscript{st} level view && \cellcolor{imldarkblue}\color{white}\textit{understand} (deployed) models \centerline{$\theta \mapsto \hat{f}(\theta)$} && \cellcolor{imldarkblue}\color{white} 1. Develop/extend interpretation methods for improved insights  \newline 2. Design optimization algorithms that favor interpretable models  \\ &&&\\
         
         2\textsuperscript{nd} level view && \cellcolor{imlmedblue}\color{white}\textit{understand} decisions made by AutoML systems for a given task 
         \centerline{$\lambda\mapsto \hat{c}(\lambda)$} 
         && \cellcolor{imlmedblue}\color{white}\centerline{Adapt IML methods}\leavevmode \centerline{to interpret AutoML systems} \leavevmode\\ &&&\\
         
        3\textsuperscript{rd} level view && \cellcolor{imllightblue}\textit{understand} properties of optimizers / ML algorithms in general && \cellcolor{imllightblue}Conduct large-scale benchmarks \centerline{and analyze the results} \leavevmode\\ &&&\\
    \end{tabular}
\end{center}
\end{frame}

\endlecture
\end{document}


%%