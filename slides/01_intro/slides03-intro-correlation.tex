%TODO: Chapter needs to be improved a lot
\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\usepackage[export]{adjustbox}
\usepackage[most]{tcolorbox}

\newtcolorbox{BlueBox}[2][]{%
   enhanced,
   colback   = blue!5!white,
   colframe  = blue!65!black,
   arc       = 1mm,
   outer arc = 1mm,
   fonttitle = \Large\slshape\textbf,
   center title,
   title     = #2,
   #1}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Feature dependencies}

\lecturechapter{Correlation and dependencies}
\lecture{Interpretable Machine Learning}
%
% \begin{frame}{Interpretable ML}
% \begin{itemize}
% %\itemsep2em
% \item ML algorithms algorithmically train predictive models with no or little pre-specifications and assumptions about the data.
% \item Several algorithms such as decision tree learning create interpretable models. However, most algorithms create models which can be considered a black box.
% \item We use the term black box, although the internal workings of the model are in fact accessible, but too complex for the human mind to comprehend.
% \end{itemize}
% \end{frame}
%
% \begin{frame}{Explainable AI}
% \begin{itemize}
% %\itemsep1em
% \item IML is often used synonymously with Explainable AI (XAI).
% \item There is no unified standard for these terminologies. We find that XAI often is specifically concerned with the interpretation of neural networks, whereas IML is used as an encompassing term for everything related to model interpretability.
% \item The nature of (deep) neural networks allows for powerful model-specific interpretation techniques, e.g., layer-wise relevance propagation (LRP) and saliency maps.
% \item Also covering model-specific NN methods would exceed the timeframe of this lecture. This lecture will concentrate on model-agnostic techniques, as they are both versatile, and receive a lot of attention in industry and academia.
% \end{itemize}
% \end{frame}
%
% \begin{frame}{XAI - Saliency Maps}
%
% A saliency map is a heatmap indicating pixel influence on the prediction (e.g., a classification of an image): \footnote[frame]{Mundhenk, T., Chen, B.Y., Friedland, G. (2019). Efficient Saliency Maps for Explainable AI. ArXiv, abs/1911.11293.
% }
% \medskip
% \begin{figure}
% \includegraphics[width = 0.8 \textwidth]{figure/saliencymap}
% \end{figure}
% \end{frame}
%
% \begin{frame}{What is Interpretability?}
% \begin{itemize}
% %\itemsep1em
% \item There is no scientific consensus on the definition of interpretability.
% \item We need to differentiate between interpretations of a model or reality. The latter is distorted by all modeling fallacies involved in predictive modeling, e.g., data quality, under- and overfitting, or model extrapolations.
% \item We use a practical definition of interpretability.
% Think back to the foundations of statistical modeling:  the linear regression model (LM). The LM, with its known equation of beta coefficients, represents a paradigm for statistical interpretability.
% \item It follows that it would be beneficial to create techniques that give us an interpretation similar to the one of an LM.
%
% \end{itemize}
% \end{frame}


% \begin{frame}{Correlation and Dependence}
% \begin{itemize}
% \item \textbf{Correlation:} Often linear correlation (Pearson $\rho$) between pairs of features is meant
% \item \textbf{Dependence:} More general dependence structure (e.g., non-linear relationships)
% \item $X_j$, $X_k$ independent $\Leftrightarrow$ joint probability density function (PDF) is product of marginal PDFs:
% %$$\text{PDF}_{X_j, X_k}(x_j, x_k) = \text{PDF}_{X_j}(x_j) \cdot \text{PDF}_{X_k}(x_k)$$
% $$\P(X_j, X_k) = \P(X_j) \cdot \P(X_k)$$


% $\P(X_j|X_k) = \P(X_j)$ knowing $X_k$ does not tell us anything about $X_j$ and vice versa
% \item $X_j$, $X_k$ independent $\Rightarrow$ $X_j$, $X_k$ uncorrelated \textbf{but} $X_j$, $X_k$ uncorrelated $\nRightarrow$ $X_j$, $X_k$  independent
% %\item Auf einer slide eklären, visuell und mathematisch
% \end{itemize}

% \centering

% \only<1>{
% \textbf{Example:} $X_1$, $X_2 \sim N(0,1)$ independent ($\rho = 0$) \hspace{10pt} $X_1$, $X_2 \sim N(0,1)$ dependent ($\rho = 0.8$) \hspace{30pt}

% \includegraphics[width = 0.4\textwidth]{figure/independent}
% \includegraphics[width = 0.4\textwidth]{figure/dependent}
% }


% \only<2>{
% \includegraphics[width = 0.5\textwidth]{figure/dependence_2}
% }
% % correlation reflects the noisiness and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of Y is zero.
% \end{frame}

\begin{frame}{Joint, Marginal and Conditional Distribution}
    For two discrete random variables $X_1, X_2$:\\
    \medskip
    \begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.5\textwidth}
    \textit{Joint distribution}
        \begin{align*}
            p_{X_1, X_2}(x_1,x_2) = \mathbb{P}(X_1 = x_1, X_2 = x_2) %
        \end{align*}
    \textit{Marginal distribution}
        \begin{align*}
            p_{X_1}(x_1) = \mathbb{P}(X_1 = x_1) = \sum_{x_2\in \mathcal{X}_2} p(x_1,x_2)
        \end{align*}
    \textit{Conditional distribution}
        \begin{align*}
            p_{X_1|X_2}(x_1 | x_2) = \mathbb{P}(X_1=x_1| X_2 = x_2) = \frac{p_{X_1, X_2}(x_1,x_2)}{p_{X_2}(x_2)}
        \end{align*}
    $\leadsto$ Analogue in the continuous case with integrals.
    \end{column}
    \begin{column}{0.5\textwidth}
    \begin{table}
    \scriptsize
        \begin{tabular}{|c|c|c|c|}
            \hline 
            $p_{X_1, X_2}$ & $\mathbb{P}(X_2 = 0)$ & $\mathbb{P}(X_2 = 1)$ & $p_{X_1}$ \\
            \hline
            $\mathbb{P}(X_1 = 0)$ & \cellcolor{gray}0.2 & \cellcolor{gray}0.3 & 0.5  \\
            \hline
            $\mathbb{P}(X_1 = 1)$ & \cellcolor{gray}0.1 & \cellcolor{gray}0.4 & 0.5  \\
            \hline
            $p_{X_2}$ & 0.3 & 0.7 & 1  \\
            \hline
        \end{tabular} \\
        \bigskip
        \begin{tabular}{|c|c|c|c|}
            \hline 
            $p_{X_1, X_2}$ & $\mathbb{P}(X_2 = 0)$ & $\mathbb{P}(X_2 = 1)$ & $p_{X_1}$ \\
            \hline
            $\mathbb{P}(X_1 = 0)$ & 0.2 & 0.3 & \cellcolor{gray}0.5  \\
            \hline
            $\mathbb{P}(X_1 = 1)$ & 0.1 & 0.4 & \cellcolor{gray}0.5  \\
            \hline
            $p_{X_2}$ & 0.3 & 0.7 & 1  \\
            \hline
        \end{tabular}\\
        \bigskip
        \begin{tabular}{|c|c|c|c|}
            \hline 
             & $x_2 = 0$ & $x_2 = 1$ \\
            \hline
            $\mathbb{P}(X_1 = 0|X_2 =x_2)$ & \cellcolor{gray} 0.67 & \cellcolor{gray}0.43   \\
            \hline
            $\mathbb{P}(X_1 = 1|X_2 =x_2)$ & \cellcolor{gray} 0.33 & \cellcolor{gray}0.57   \\
            \hline
            $\sum$ & 1 & 1  \\
            \hline
        \end{tabular}
    \end{table}
    \end{column}
    \end{columns} 
    %\bigskip
    
\end{frame}



\begin{frame}{Pearson's correlation coefficient $\rho$}

By \textbf{correlation} often Pearson's correlation is meant (measures only \textbf{linear relationship}) %between pairs of features


%\dfrac{s_{X_1 X_2}}{s_{X_1} \cdot s_{X_2}} \text{ with }

% \begin{itemize}
%     \item Sample covariance of $X_1$ and $X_2$:
%     $s_{X_1 X_2} = \frac{1}{n-1}\sum_{i=1}^{n}{(x_1^{(i)}-\bar{x}_1) \cdot (x_2^{(i)}-\bar{x}_2)}$
%     \item Sample standard deviation $s_{X_1}$ and $s_{X_2}$ of $X_1$ and $X_2$
% \end{itemize}

\begin{columns}[c, totalwidth=\textwidth]
\begin{column}{0.5\linewidth}
\includegraphics[width = \textwidth]{figure/pearson_cor}
\end{column}
\begin{column}{0.5\linewidth}

\centerline{$\rho(X_1, X_2) = \tfrac{\sum_{i=1}^{n}{(x_1^{(i)}-\bar{x}_1) \cdot (x_2^{(i)}-\bar{x}_2)}}{\sqrt{\sum_{i=1}^{n}{(x_1^{(i)}-\bar{x}_1)^2 \sum_{i=1}^{n}{(x_2^{(i)}-\bar{x}_2)^2 }}}} \in [-1, 1]$}

\medskip

Geometric interpretation of $\rho$:
\begin{itemize}
    \item Numerator is sum of rectangle's area with width $x_1^{(i)}-\bar{x}_1$ and height $x_2^{(i)}-\bar{x}_2$
    \item Areas enter numerator with positive (\textbf{+}) or negative (\textbf{-}) sign, depending on point position
    \item Denominator scales the sum to $ [-1, 1]$
    %\item $\rho = 0$ if area of all rectangles cancels out\\
    %$\leadsto$ $X_1, X_2$ uncorrelated
\end{itemize}
\end{column}
\end{columns}

\medskip
\pause
\begin{itemize}
    \item $\rho = 0$ if area of rectangles of all points cancels out
    $\leadsto$ $X_1, X_2$ linearly uncorrelated
    \item $\rho > 0$ if {\color{ggblue}positive areas} dominate {\color{ggred}negative areas} 
    $\leadsto$ $X_1, X_2$ positive correlated
    \item $\rho < 0$ if {\color{ggred}negative areas} dominate {\color{ggblue}positive areas} 
    $\leadsto$ $X_1, X_2$ negative correlated
\end{itemize}
%\textbf{But:} Pearson's $\rho$ does not measure non-linear dependencies, e.g., $\rho = 0$ but dependent:

\end{frame}


\begin{frame}{Coefficient of determination $R^2$}

Another method to evaluate \textbf{linear dependency} between variables is by calculating the $R^2$

\begin{columns}[c, totalwidth=\textwidth]
\begin{column}{0.5\linewidth}
\only<1>{\includegraphics[width = 0.95\textwidth]{figure/r_squared_Fahrenheit}}
\only<2>{\includegraphics[width = 0.95\textwidth]{figure/r_squared_compare}}
\only<3>{\includegraphics[width = 0.95\textwidth]{figure/r_squared}}
\end{column}
\begin{column}{0.5\linewidth}

\medskip

Idea for two-dimensional case:
\begin{itemize}
    \setlength\itemsep{0.7mm}
    \item Fit a linear model:
    $\hat{x}_2 = \hat{f}_{LM}(x_1) = \theta_0 + \theta_1 x_1$
    %($x_2 \sim x_1$) 
    % $\hat{f}_{LM}(x_1) = 2345 + 141.3 x_1$
    \item[] $\leadsto$ Slope = $0$ $\Rightarrow$ no dependence
    \item[] $\leadsto$ Very large slope $\Rightarrow$ strong dependence
    \item Exact $\theta_1$ score problematic 
    \pause
    \item[] $\leadsto$ Rescaling of $x_1$ or $x_2$ changes $\theta_1$ 
    \only<2>{\item e.g. °F $\rightarrow$ °C $\Rightarrow$ ${\color{blue}\theta_1=78.5}\rightarrow{\color{ggblue}\theta_1^*=141.3}$}
    \pause
    \item Set $SSE_{LM}$ in relation to $SSE$ of a constant model $\hat{f}_c = \bar{x}_2$
    \item[] $SSE_{LM} = \sum_{i=1}^n ({\color{ggblue}x_2^{(i)} - \hat{f}_{LM}(x_1^{(i)})})^2$
    \item[] $SSE_{c} = \sum_{i=1}^n ({\color{ggred}x_2^{(i)} - \bar{x}_2})^2$
\end{itemize}

\end{column}
\end{columns}

\begin{align*}
    &\Rightarrow \text{ Measure of fitting quality of LM: } R^2 = 1-\frac{SSE_{LM}}{SSE_{c}} \in [-1, 1] \\
    &\Rightarrow \rho(X_1, X_2) = R
\end{align*}

\end{frame}


\begin{frame}{Mutual Information}
    \begin{itemize}
        \item MI describes amount of information about one random variable obtained through another one or how different the joint distribution is from pure independence
        \item $MI(X_1 ; X_2 )$ is the Kullback-Leibler distance between joint distribution and product distribution $p_{X_1} p_{X_2}$:
        \begin{align*}
            MI(X_1 ; X_2 ) &= \sum_{x_1 \in \mathcal{X}_1} \sum_{x_2 \in \mathcal{X}_2} p(x_1, x_2) log\left(\frac{p(x_1, x_2)}{p(x_1) p(x_2)} \right) \\
            &= D_{KL} \left( p(x_1, x_2) \, || \, p(x_1) p(x_2) \right) \\
            &= \mathbb{E}_{p(x_1, x_2)} \left[ log\left(\frac{p(x_1, x_2)}{p(x_1) p(x_2)} \right) \right]
        \end{align*}
        \item MI measures amount of "dependence" between variables. It is zero if and only if the variables are independent.
        \item Unlike (Pearson) correlation, MI is not limited to real-valued random variables.
    \end{itemize}
\end{frame}



\begin{frame}{Correlation and Dependence}

Scatterplot with multivariate distribution (contour lines) and marginal density $X_1$, $X_2 \sim N(0,1)$ 

\only<1>{\begin{center}
\begin{minipage}[t]{0.3\textwidth}
\centering
 $\rho(X_1, X_2) = 0$
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
 $\rho(X_1, X_2) = 0.8$
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\centering
 $\rho(X_1, X_2) = -0.8$
\end{minipage}
\includegraphics[width = 0.9\textwidth]{figure/dnorm_correlation.pdf}
\end{center}}

\only<2>{\begin{center}
\begin{minipage}[t]{0.15\textwidth}
\centering{\scriptsize
 $\rho(X_1, X_2) = 0$}
\end{minipage}
\begin{minipage}[t]{0.15\textwidth}
\centering{\scriptsize
 $\rho(X_1, X_2) = 0.8$}
\end{minipage}
\begin{minipage}[t]{0.15\textwidth}
\centering{\scriptsize
 $\rho(X_1, X_2) = -0.8$}
\end{minipage}\\
\includegraphics[width = 0.45\textwidth]{figure/dnorm_correlation.pdf}
\end{center}


Examples with Pearson's correlation $\rho = 0$ but non-linear dependencies (MI $\neq 0$):\\
\medskip
\centering
\includegraphics[width = 0.8\textwidth]{figure/dependence_3}}
\end{frame}


\begin{frame}{Correlation and Dependence}
\textbf{Dependence:} Describes general dependence structure of features (e.g., non-linear relationships)

%\textbf{Definition:}
\begin{itemize}[<+->]
\item Definition: $X_j$, $X_k$ independent $\Leftrightarrow$ joint distribution is product of marginals:\\ \vspace{5pt}
\centerline{$\P(X_j, X_k) = \P(X_j) \cdot \P(X_k)$}
\vspace{5pt}
\item Equivalent definition (knowing $X_k$ does not tell us anything about $X_j$ and vice versa): \\
\vspace{5pt}
\phantom{AAA} $\P(X_j|X_k) = \P(X_j) \text{ and } \P(X_k|X_j) = \P(X_k)$ \hfill (follows from conditional probability) \phantom{AAA}
\vspace{5pt}
%$$\P(X_j|X_k) = \P(X_j) \hfill \text{(knowing $X_k$ does not tell us anything about $X_j$ and vice versa)}$$
%\medskip
\item Measuring complex dependencies is difficult but different measures exist, e.g., \\
$\leadsto$ Spearman correlation (measures monotonic dependencies via ranks) \\
$\leadsto$ Information-theoretical measures like mutual information \\
$\leadsto$ Kernel-based measures like Hilbert-Schmidt Independence Criterion (HSIC)
% nice further reading https://jejjohnson.github.io/research_journal/appendix/similarity/hsic/
\item \textbf{N.B.:} $X_j$, $X_k$ independent $\Rightarrow$ $\rho(X_j, X_k) = 0$ \textbf{but} $\rho(X_j, X_k) = 0$ $\nRightarrow$ $X_j$, $X_k$  independent \\
But equivalency holds if distribution is jointly normal
\item  $MI(X_j, X_k) = 0$ if and only if $X_j$, $X_k$ independent
\end{itemize}
\end{frame}


\begin{frame}{Correlation and Dependence}
	
\textbf{Example:}
\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.49\linewidth}
\includegraphics[width=\linewidth]{figure/independent_slice.pdf}
Conditional distributions at different vertical and horizontal slices (after normalizing area to 1) match their marginal distributions
\medskip

$\Rightarrow$ $\P(X_1|X_2) = \P(X_1) \text{ and } \P(X_2|X_1) = \P(X_2)$
\end{column}
\hfill\pause
\begin{column}{0.49\linewidth}
\includegraphics[width=\linewidth]{figure/dependent_slice.pdf}
Conditional distributions do not match their marginal distributions
\end{column}
\end{columns}


\end{frame}



\begin{frame}{Interpretations with Dependent Features}
\begin{itemize}

% \item Be clear about interpretation goal: Explain model or underlying relationship in data?
% %We need to differentiate between interpretations of a model or reality
% \\
% $\leadsto$ Latter might be distorted by modeling fallacies, 
% e.g., 
% %due to bad data quality, 
% under- and overfitting or extrapolation

%\pause
\item Highly correlated features contain similar information \\
$\leadsto$ Model might pick only one feature (regularization) (even if it is causally irrelevant)
%Some models might use only one correlated feature (even if it is causally not relevant)
\\
%Can confuse a model so that only one correlated feature is used (even if it is not relevant) \\
$\leadsto$ Produced explanations can be misleading (true to the model, but not to the data) % generating process
\\
$\leadsto$ Different IML models often produce different results in these situation, and not always trivial to understand which / why 
%Even for two causally and equally relevant correlated features it is not guaranteed that their explanations will be similar
% \pause
% \item Assume two causally and equally relevant but highly correlated features \\
% $\leadsto$ One might expect that their explanations (e.g., feature importance) will be similar \\
% %One might expect that explanations of two correlated features (e.g., feature importance) are similar as they share similar information \\
% $\leadsto$ Generally not true as the model can learn different relationships


% \begin{tikzpicture}[remember picture,overlay,shift=(current page.center)] %opacity=.2,text opacity=1
% \node[fill=white, opacity=1,text opacity=1] at (current page.center) {\includegraphics[width=0.6\textwidth]{figure/ridge_lasso}};
% \end{tikzpicture}
\pause

% \only<3>{
%  \begin{BlueBox}{Example}
%   \begin{columns}[c, totalwidth=\textwidth]
% \begin{column}{0.5\textwidth}
% \includegraphics[trim=0px 110px 30px 0px, clip, width=\textwidth]{figure/ridge_lasso}
% \end{column}
% \begin{column}{0.5\textwidth}
% \includegraphics[trim=0px 0px 0px 115px, clip, width=\textwidth]{figure/ridge_lasso}
% \end{column}
% \end{columns}}
% \end{BlueBox}
% }

%\only<3>{
\colorbox{blue!20}{\begin{columns}[c, totalwidth=\linewidth]
\begin{column}{0.5\linewidth}
\includegraphics[trim=0px 110px 30px 0px, clip, width=\linewidth]{figure/ridge_lasso}
\end{column}
\begin{column}{0.5\linewidth}
\includegraphics[trim=0px 0px 0px 115px, clip, width=\linewidth]{figure/ridge_lasso}
\end{column}
\end{columns}}
%}

\end{itemize}
\end{frame}



\begin{frame}{Extrapolation due to Dependencies}
%\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{figure/extrapolation}}
%\end{center}

\begin{itemize}
%\itemsep1em
\item Many interpretation methods are based on
%rely on varying feature values and create
artificially created data points \\
$\leadsto$ Many of these points can lie in low-density regions if features are dependent\\
$\leadsto$ Model predictions in such regions are subject to a high uncertainty\\ % in regions with no or few training data
$\leadsto$ Explanations may be biased as they often rely on predictions where model extrapolated\\
\pause
%\item 
%\item Essentially, we are interested in the prediction uncertainty\\
%, i.e., the less the training data on a feature subspace, the higher the prediction uncertainty.
%$\leadsto$ Models are rarely able to quantify uncertainty of predictions %Models are rarely capable of quantifying prediction uncertainty
\item There is no definition of when a model extrapolates and to what degree \\
$\leadsto$ Severity of extrapolation depends on model, some extrapolate more than others \\
$\leadsto$ Training density might serve as proxy to identify regions where extrapolation is likely \\
\phantom{$\leadsto$} But: Density estimation in many dimensions is often infeasible
%$\leadsto$ Some models might extrapolate more than others
%\item Density of training data might serve as proxy to identify regions where extrapolation is likely\\
%$\leadsto$ Density estimation in many dimensions is often infeasible
\end{itemize}

\end{frame}
% \begin{frame}{Extrapolation}
% \begin{itemize}
% %\itemsep1em
% \item Predictions of an ML model in regions with a low density of training points are subject to a high variance.
% \item Essentially we are interested in the prediction uncertainty, i.e., the less the training data on a feature subspace, the higher the prediction uncertainty. Unfortunately, virtually no ML models are capable of quantifying prediction uncertainty.
% \item There is no consensus definition of when a model extrapolates and to what degree. Furthermore, the severity of the problem depends on the model itself. Some models might extrapolate more reliably than others.
% \item Theoretically, we could use the training density as a proxy. However, density estimation in many dimensions is often infeasible.
% \end{itemize}

% \end{frame}

%%% dann das gleiche für interactions

%% wir können auch ein bsp mit feature importannce machen, wo korrelation alles schwieriger macht. im guyon paper und im i2ml featsel teil ist es drin

%% was kann mindestens zu korrel sagen:
%
% a) das extrapol problem. haben wir folien zu
% b) bei korrel kann man sagen: die vars haben die gkleich info, also breaucht man nur eine. das ist eine explanation. die kann aber falsch sein. siehe guyon bsp
% c) bei korrel könnte man sagen: die erklärungen zu vars sind die gleichen. zb deren importance. das ist aber auch nicht gegeben
% ---> generell: vorsicht!

%--------------------------------

% bei interactions:
% 1) definition davon bringen
% man kann das operationals / an der modell struktur erklären. als bsp lin modell und tree
% allgemeine definition von fanoava
% sagen dass eine funktin keine IA hat, wenn sie separierbar. verbindung zur optimierung bringen
% sagen dass man aus der fanbova die interactioons ablesen kann und auf die H-statistic für weiteres vereweisen



% \begin{frame}{Correlation vs. Interaction}
% \begin{itemize}
% %\itemsep2em
% \item Correlated (or dependent) features and feature interactions can cause misleading explanations. %One needs to be careful not to confuse them.
% \item Correlated features implies joint is not product of marginals: $\text{PDF}(x_1, \dots, x_p) \neq \text{PDF}(x_1) \cdot \ldots \cdot \text{PDF}(x_p)$.
% \item
% Many interpretation methods rely on varying feature values and may create data points that are out-of-distribution or located in low-density regions if features are correlated.\\
% $\leadsto$ May produce biased explanations that rely on predictions where the model extrapolated.
% % \item An interaction is a product term between features inside the prediction function. As such, interactions are detached from the constitution of the data, i.e., regardloss of the degree of correlation, the effect of a feature on the target will depend on the values of one or multiple other features.
% \end{itemize}
% \end{frame}

% \begin{frame}{Correlation vs. Interaction}

% 1000 randomly sampled observations with positive correlation between $x_1$ and $x_2$:
% \medskip
% \begin{figure}
% \includegraphics[width = 0.7\textwidth]{figure/correlation}
% \end{figure}
% \end{frame}


\endlecture
\end{document}
