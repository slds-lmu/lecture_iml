\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Why interpretability?
\item Developments until now?
\item Use cases for interpretability}

\lecturechapter{Introduction, Motivation, and History}
\lecture{Interpretable Machine Learning}

\begin{frame}{Why Interpretability?}
% 		\begin{itemize}
% 			\item Machine learning (ML) has a huge potential to aid the decision-making process in various  applications.
% 			\pause
% 			\smallskip
% 			\item ML models often are intransparent black boxes, e.g., XGBoost, RBF SVM, or NNs.
% 			\begin{itemize}
% 				\item[$\rightarrow$] too complex to be understood by humans.
% 			\end{itemize}
% 			\smallskip
% 			\item A lack in explanations diminishes trust in the model and creates barriers for adoption, especially in areas with critical decision-making consequences, e.g., medicine.
% 			\smallskip
% 			\item Such disciplines often rely on traditional models,\\ e.g., linear models, with less predictive performance.
% 			\smallskip
% 			\item Interpretable machine learning (IML) aims to bridge the gap between interpretability and predictive performance.
% 		\end{itemize}
\bigskip
    \begin{columns}[T]
    \begin{column}{0.8\textwidth}
		\begin{itemize}
			\item ML: huge potential to aid decision-making process due to its predictive performance
			%\pause
			%\smallskip
			\only<2->{\item ML models are often black boxes, e.g., XGBoost, RBF SVM or DNNs
			\begin{itemize}
				\item[$\leadsto$] too complex to be understood by humans
			\end{itemize}}
			%\pause
			%\smallskip
			\only<3->{\item Lack of explanation
			\begin{enumerate}
				\item hurts trust
				\item creates barriers
			\end{enumerate}}
			\only<4->{\item[$\leadsto$] Harder to adapt for critical areas with decisions affecting human life}
			\item<5>[\,$\leadsto$] Many disciplines with required trust rely on traditional models,\\ e.g., linear models, with less predictive performance
		\end{itemize}
	\end{column}
	\begin{column}{0.2\textwidth}  %%<--- here
        \only<2->{\includegraphics[width=0.9\textwidth]{figure/nn_model.png}}
        \only<2->{\includegraphics[width=0.9\textwidth]{figure/nn_landscape.png}
        \centering \citebutton{Liu 2021}{https://davideliu.com/2021/12/12/visualizing-loss-landscape-of-gail/}}
    \end{column}
    \end{columns}
    % \bigskip
    % \begin{columns}[T]
    %     \begin{column}{0.5\textwidth}
    %     \begin{itemize}
    %         \only<5>{\item[$\leadsto$] Harder to adapt for critical areas with decisions affecting human life}
    %         \item<5>[\,$\leadsto$] Many disciplines with required trust rely on traditional models,\\ e.g., linear models, with less predictive performance
    %     \end{itemize}
    %     \end{column}
	   % \begin{column}{0.5\textwidth} 
	   %     \only<5>{\includegraphics[width=\textwidth]{figure/intro_lm_bike.pdf}}
	   % \end{column}
    %     \end{columns}
\end{frame}
	
	
\begin{frame}{Need for Interpretability in High-stakes Decisions}

Examples of critical areas where decisions based on ML models can affect human life 
    % \begin{itemize}
    %     \item Credit scoring and insurance applications
    %     \citebutton{Society of Actuaries}{https://www.soa.org/resources/research-reports/2021/interpretable-machine-learning}
    % \end{itemize}
    \begin{columns}[T]
    \begin{column}{0.55\textwidth}
		\begin{itemize}
		\item Credit scoring and insurance applications
        \citebutton{Society of Actuaries}{https://www.soa.org/resources/research-reports/2021/interpretable-machine-learning}
        \begin{itemize}
            \item Providing reasons for not granting a loan
            \item Fraud detection in insurance claims
        \end{itemize}
        \item<2-> Medical applications
        \begin{itemize}
            \item Identification of diseases
            \item Chance of recovering
            \item Recommendations of treatments
        \end{itemize}
        \item<2-> \ldots
    \end{itemize}
    \end{column}
	\begin{column}{0.45\textwidth}
        \includegraphics<1->[page=1, width=\textwidth, trim = 0 250 0 0, clip]{figure/counterfactual.pdf}
        \medskip
	    \only<2->{\includegraphics[width=\textwidth]{figure/medicine.png}
	    \centering \citebutton{Miliard (2020)}{https://www.healthcareitnews.com/news/new-ai-diagnostic-tool-knows-when-defer-human-mit-researchers-say}}
	\end{column}
    \end{columns}
\end{frame}


\begin{frame}{Need for interpretability in High-stakes Decisions}

    Need for interpretability also becoming increasingly important from a legal perspective
    
    \begin{itemize}
    \item General Data Protection Regulation (GDPR) requires for some applications that models have to be explainable \citebutton{Goodman \& Flaxman (2017)}{https://doi.org/10.1609/aimag.v38i3.2741}\\
    $\leadsto$ \textit{EU Regulations on Algorithmic Decision-Making and a ``Right to Explanation''} 
    
    \item \textit{Ethics guidelines for trustworthy AI}
    \citebutton{European Commission (2019)}{https://doi.org/10.2759/346720}

    \end{itemize}
    \medskip
    
    \centering\includegraphics[width=0.45\textwidth]{figure/performance_vs_interpretability.png}
    % Quelle: https://docs.google.com/presentation/d/12ZPrTjBKEUT-7drdyUJCQGK0oHVDtYIVd2_6byE62f0/edit?usp=sharing
    %\centering \citebutton{Scott Fortmann-Roe (2012)}{http://scott.fortmann-roe.com/docs/BiasVariance.html}
\end{frame}


	%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Brief History of Interpretability}
	\begin{columns}[T]
	\begin{column}{0.75\textwidth}
	    \begin{itemize}
			\item 18th and 19th century: \\linear regression models (Gauss, Legendre, Quetelet)
			\medskip
			\item<2-> 1940s:\\ emergence of sensitivity analysis (SA)
			\medskip
			\item<3-> Middle of 20th century:\\ Rule-based ML, incl. decision rules and decision trees
			\medskip
			\item<4-> 2001:\\ built-in feature importance measure of random forests
			\medskip
			\item<5-> >2010: \\Explainable AI (XAI) for deep learning
			\medskip
			\item<6> >2015: \\IML as an independent field of research
		\end{itemize}
	\end{column}
	\begin{column}{0.25\textwidth}
	    \includegraphics[width=0.8\textwidth]{figure/Carl_Friedrich_Gauss_1828.jpg}
        \centering \citebutton{Carl Friedrich Gauss}{https://commons.wikimedia.org/w/index.php?curid=2404149}
        \centering \citebutton{Wikipedia}{https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss}
        \bigskip\\
        \only<3->{\includegraphics[width=0.9\textwidth]{figure/Random_Forest.png}}
        % https://docs.google.com/presentation/d/15HjwMHdTtZ9N0cniUPsiztRSRg1UDY3Y5M8tXsaqt2I/edit?usp=sharing
	\end{column}
	\end{columns}
\end{frame}

	%-----------------------------------------------------------------------------------------------------------------------------
% \begin{frame}{When do we need interpretability?}
% \begin{columns}[T]
% \begin{column}{0.6\textwidth}
% %  \begin{itemize}
% %   \item Debugging machine learning models
% %   \item Increasing trust in models
% %   \item Analyzing newly developed systems with unknown consequences
% %   \item Decisions about humans
% %   \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
% %   \item When loss function does not cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
% % \end{itemize}
% \begin{itemize}
%   \item To \textbf{Discover}: Gain insights about data, distribution and model
%   \pause 
%   \item To \textbf{Debug, audit and improve}: Insights help to identify flaws (in data or model), which can be corrected (debug and audit)\\
%   $\leadsto$ Global perspective
%   \pause
%   \item To \textbf{explain individual decisions}: Explaining individual decisions can prevent unwanted actions based on the model\\
%   $\leadsto$ Local perspective
%   \pause 
%   \item To \textbf{Justify}: Investigate if and why biased, unexpected or discriminatory predictions were made, or improve/reject the model\\
%   $\leadsto$ Fairness perspective

% \end{itemize}
% \end{column}
% \begin{column}{0.4\textwidth}  %%<--- here
%  %\vspace{0.5cm}
% %  \begin{center}
% %  \begin{figure}
%   \includegraphics[width=0.9\textwidth]{figure/explain-to}
%   \centering \citebutton{Adadi and Berrada 2018}{https://doi.org/10.1109/ACCESS.2018.2870052}
% %  \end{figure}
% %  \end{center}
% \end{column}
% \end{columns}
%      %\lz
%     %\footnote[frame]{Doshi-Velez, F., \& Kim, B. (2017). Towards A Rigorous Science of Interpretable Machine Learning. arXiv: 1702.08608.}
%     %\footnote[frame]{A. Adadi and M. Berrada, "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)," in IEEE Access, vol. 6, pp. 52138-52160, 2018.}
% \end{frame}

\begin{frame}{When do we need interpretability}
% \begin{columns}[T]
% \begin{column}{0.55\textwidth}
% \begin{itemize}
%   \item To \textbf{Discover}: Gain insights about data, distribution and model
%   %\pause 
%   \item<2-> To \textbf{Improve}: Insights help to identify flaws (in data or model), which can be corrected (debug and audit)\\
%   $\leadsto$ Global perspective
%   %\pause
%   \item<3-> To \textbf{Control}: Explaining individual decisions can prevent unwanted actions based on the model\\
%   $\leadsto$ Local perspective
%   %\pause 
%   \item<4> To \textbf{Justify}: Investigate if and why biased, unexpected or discriminatory predictions were made, or improve/reject the model\\
%   $\leadsto$ Fairness perspective

% \end{itemize}
% \end{column}
% \begin{column}{0.4\textwidth}
%     \vspace{0.5cm}
    \begin{center}
    \def\firstcircle{(90:1cm) circle (2.5cm)}
    \def\secondcircle{(210:2.5cm) circle (2.5cm)}
    \def\thirdcircle{(330:2.5cm) circle (2.5cm)}
    \def\fourthcircle{(90:-3.5cm) circle (2.5cm)}
    \resizebox{7cm}{7cm}{
    \begin{tikzpicture}
        \begin{scope}[ fill opacity=0.5]
            \fill[red] \firstcircle;
            \fill[green] \secondcircle;
            \fill[blue] \thirdcircle;
            \fill[cyan] \fourthcircle;
        \end{scope}
        \draw \firstcircle node[text=black,above, text width=2cm, align=center] {Explain to discover};
        \draw \secondcircle node [text=black, left, text width=2cm, align=center] {Explain to improve};
        \draw \thirdcircle node [text=black,right, text width=2cm, align=center] {Explain to control};
        \draw \fourthcircle node [text=black,below, text width=2cm, align=center]{Explain to justify};
    \end{tikzpicture}}
    \end{center}
% \end{column}
% \end{columns}
\end{frame}



% \begin{frame}{Why is Interpretability Important?}

% 	\begin{itemize}
% 	    \item Machine learning is (mostly) about discovering patterns in data.
% 	    \medskip
% 	    \item Unfortunately, it is not guaranteed that ML will identify the correct patterns.

% 	    \medskip
% 	    \item We humans might not be able to discover patterns ML models discovered.
% 	    \begin{itemize}
% 	        \item Good for science or to get new insights.
% 	        \item Bad in applications where unexpected behavior is not desired.
% 	    \end{itemize}
% 	    \medskip

% 	    \item \alert{How can you check whether the model is correct in its inference?}
% 	\end{itemize}

% \end{frame}

\begin{frame}{Explain to discover}
$\leadsto$ Gain insights about data, distribution and model \\
\medskip
\textbf{Example:} Bike Sharing Dataset (predict number of bike rentals per day) \\
\textit{Exemplary question:} Which feature influences the model performance and to what extent?
\begin{center}
\includegraphics[width=0.6\textwidth]{figure/bike-sharing02.png}
\end{center}

%\textbf{Interpretation:}

\begin{itemize}
    \item Year (\code{yr}) and Temperature (temp) most important features
    \item Holiday (\code{holiday}) less important (Can we drop it?)
\end{itemize}

\end{frame}

% \begin{frame}{Example (Explain to improve): Clever Hans \citebutton{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4} }

% 	\centering
% 	\begin{columns}[T]
% 	\begin{column}{0.6\textwidth}
% 	\includegraphics<1>[width=\textwidth]{figure/horse_without_label.PNG}
% 	\includegraphics<2>[width=\textwidth]{figure/horse_with_label.PNG}
% 	\includegraphics<3>[width=\textwidth]{figure/horse_map_with_label.PNG}
% 	\includegraphics<4>[width=\textwidth]{figure/horse_map_without_label.PNG}
% 	\end{column}
% 	\begin{column}{0.4\textwidth}

% 	\begin{itemize}
% 	    \item Horse with claimed math skills
% 	    \item Answered questions correctly by hoof tapping or head shaking
% 	    \item Correct answers were traced to involuntary cues from human's body language $\Rightarrow$ no math skills %asking person
% 	    %(e.g., tense attitude)
% 	    \item<2-> Image classification: \\
% 	    source tag present \\
% 	    \onslide<3->{$\Rightarrow$ classified as horse}
% 	    \item<4-> no source tag \\ $\Rightarrow$ not classified as horse
% 	\end{itemize}

% 	\end{column}
% 	\end{columns}
% \end{frame}

\begin{frame}{Explain to improve}
$\leadsto$ Insights help to identify flaws (in data or model), which can be corrected (debug and audit) \\
\medskip
\textbf{Example:} Neural Net Tank \citebutton{gwern.net}{https://www.gwern.net/Tanks} \\
	\centering
	\begin{columns}[T]
	\begin{column}{0.45\textwidth}
	\centering
	% pictures from pixabay
	\only<1>{\includegraphics[width=\textwidth]{figure/tank.jpg}}
	\only<2>{\includegraphics[width=0.7\textwidth]{figure/tank.jpg}
	        \includegraphics[width=0.7\textwidth]{figure/landscape.jpg}}
	\end{column}
	\begin{column}{0.55\textwidth}
    Cautionary tale:
	\begin{itemize}
	    \item Creation of neural network to detect tanks
        \item Model shows good predictive performance in training data set
        \item Application outside training data set: failure
        \item<2-> Reasons vary depending on the source, in general: NN based its decision on irrelevant points. 
        \item<2-> E.g. model detecting weather situations: Tanks always photographed under cloudy skies; photos without tanks always taken in sunny weather.
	\end{itemize}

	\end{column}
	\end{columns}
\end{frame}


% \begin{frame}{Clever Hans \citebutton{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}

% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figure/boats_maps.PNG}

% \end{frame}

\begin{frame}{Explain to control}
    $\leadsto$ Explaining individual decisions can prevent unwanted actions based on the model \\
    \medskip
    \textbf{Example:} Credit Risk Application. $\textbf{x}$: customer and credit information; $y$: grant or reject credit
	
	\only<1>{\begin{center}\includegraphics[width=0.6\linewidth, page=1]{figure/counterfactuals_credit.pdf} \end{center}

	Questions:}
	\begin{itemize}
		\item Why was the credit rejected?
		\item Is it a fair decision?
		\item \textbf{How should $\xv$ be changed so that the credit is accepted?}
	\end{itemize}
	
	%Counterfactual Explanations provide answers in the form of "What-If"-scenarios.
	\only<2>{\begin{center}\includegraphics[width=0.6\linewidth, page=2]{figure/counterfactuals_credit.pdf} \end{center}

	``If the person was more skilled and the credit amount had been reduced to \$8.000,\\ the credit would have been granted."} 
\end{frame}

%%%

\begin{frame}{Explain to justify: prediction on subgroups and fairness predictions}
    $\leadsto$ Investigate if and why biased, unexpected or discriminatory predictions were made \\
    \bigskip
    \textbf{Example:} COMPAS
    \smallskip
    \begin{itemize}
        \item Correctional Offender Management Profiling for Alternative Sanctions (COMPAS)
        \smallskip
        \item Commercial algorithm used by judges to assess defendant’s likelihood of re-offending
        \pause
        \smallskip
        \item Predict recidivism risk
        \begin{itemize}
            \item i.e., criminal re-offense after previous crime, resulting in jail booking
            \smallskip
            \item different risk levels: high risk, medium risk or low risk
        \end{itemize}
        \pause
        \smallskip
        \item Evaluation of recidivism risk based on a questionnaire the defendant has to answer
    \end{itemize}

\end{frame}

\begin{frame}{Explain to justify: COMPAS~\citebutton{Larson et al. 2016}{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}}
    $\leadsto$ Investigate if and why biased, unexpected or discriminatory predictions were made \\
    \medskip
    Descriptive data analysis: 
    
    \centering
    \includegraphics[width=0.7\textwidth]{figure/compass_black_white.PNG}
    % source https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm

    Decile score: 1 (low risk) to 10 (high risk)

	$\leadsto$ Model skewed towards low risk for white defendants
	
	$\leadsto$ Strong indication that the model is discriminating black defendants
	
	$\leadsto$ Use IML to investigate if and how much the model uses the defendants' origin.
\end{frame}

\begin{frame}{Explain to justify: COMPAS~\citebutton{Alvarez-Melis and Jaakkola 2018}{https://arxiv.org/pdf/1806.08049.pdf}}
    $\leadsto$ Investigate if and why biased, unexpected or discriminatory predictions were made \\
    \medskip
    The underlying classifier is a logistic regression. Feature effects analysis for two exemplary defendants, using different interpretation methods (SHAP and LIME): \\
    $\leadsto$ The methods give for every feature a number mirroring the impact on violence score. 
    \vspace{0.5cm}
    
    \centering
    \includegraphics[width=0.95\textwidth]{figure/COMPAS_shap_lime_example.png}
    
    \vspace{0.2cm}
    $\leadsto$ In both cases the race (african american) has a noticeable positive impact on violent score
\end{frame}





% \begin{frame}{Motivation - Adversarial Examples \citebutton{Goodfellow et al. 2016}{https://arxiv.org/pdf/1412.6572.pdf}}

%     \begin{center}
%     \includegraphics[width=0.7\textwidth]{figure/panda-airplane.pdf}
%     \end{center}
% 	\bigskip

% 	$\rightarrow$ ML Models might not capture human-like understanding
% \end{frame}


% \begin{frame}{Adversarial Noise \citebutton{Goodfellow et al. 2016}{https://arxiv.org/pdf/1412.6572.pdf}}
%     \begin{center}
%     \includegraphics[width=0.65\textwidth]{figure/adv-noise.pdf}
%     % https://arxiv.org/pdf/1412.6572.pdf
% 	\end{center}
% 	\normalsize
% 	%\bigskip
% 	$\rightarrow$\textbf{Adversarial Noise:} Noise not visible to \textbf{humans} but results in incorrect classification results
% \end{frame}

% \begin{frame}[c]{Adversarial Examples~\citebutton{Goodfellow et al. 2016}{https://arxiv.org/pdf/1412.6572.pdf}}
    
%     \centering
%     \includegraphics[width=0.7\textwidth]{figure/adv-noise-2.pdf}
	
% \end{frame}

\endlecture
\end{document}
