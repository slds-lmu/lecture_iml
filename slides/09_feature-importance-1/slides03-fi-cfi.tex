\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}

% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}
%\bibliography{feature-importance}

\begin{document}
	\newcommand{\titlefigure}{figure_man/feature-importance.png}
    \newcommand{\learninggoals}{
    	\item Extrapolation and Conditional Sampling
    	\item Conditional Feature Importance (CFI)
    	\item Interpretation of CFI and difference to PFI}
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
% 	\input{../../latex-math/basic-math.tex}
% 	\input{../../latex-math/basic-ml.tex}
% 	\input{../../latex-math/ml-interpretable.tex}
	
	\lecturechapter{Conditional Feature Importance (CFI)}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

%TODO: Add CFI sampling: Conditional subgroup, knockoffs/CPI, others?
%TODO: Add more on when to use conditional, when marginal?
%TODO: Conditioning on everything is not a good idea: CFI=0 for highly correlated variables? Other examples? Relative Importance (Gunnar)?

% CFI IDEA
\begin{frame}{CFI Motivation}

\begin{itemize}[<+->]
    \item \textbf{PFI Idea:} Replace feature(s) $X_S$ with perturbed $\tilde X_S$ to preserve marginal distibution $\P(X_S)$ so that $\tilde X_S \indep Y$ (independent), e.g., by random permutations
    \item \textbf{Problem:} Breaks not only association between $X_S$ and $Y$ (what we want) but also between $X_S$ and $X_{-S}$ $\Rightarrow$ $\P(X_S, X_{-S}) \neq \P(\tilde X_S, X_{-S})$ (extrapolation)
    \item \textbf{CFI Idea:} Replace feature(s) $X_S$ with perturbed $\tilde X_S$ to preserve joint distibution so that $\P(X_S, X_{-S}) = \P(\tilde X_S, X_{-S})$ (no extrapolation) while still $\tilde X_S \indep Y$
    %Sample $X_S$ from conditional dist. $\P(X_S|X_{-S})$ to preserve joint dist., i.e., $\P(X_S|X_{-S}) \P(X_{-S}) = \P(X_S, X_{-S})$
\end{itemize}
%\\
%\lz
%\\
%\lz

\medskip

\visible<4>{
\textbf{Example:} Conditional permutation scheme 

\textbf{Black dots:} $X_2 \sim \mathcal{U}(0,1)$ and 
$X_1 \sim \mathcal{N}(0,1)$ (if $X_2 < 0.5$) or $\mathcal{N}(4,4)$ (if $ X_2 \geq 0.5$)

\medskip

\begin{columns}[T, totalwidth = \textwidth]
\begin{column}{0.47\textwidth}

\includegraphics[width=\linewidth]{figure_man/conditional_sampling.pdf}
\centerline{\citebutton{Molnar et. al (2020)}{https://arxiv.org/abs/2006.04628}}
\end{column}
\begin{column}{0.52\textwidth}
%\begin{itemize}
% $X_2 \sim U(0,1)$ and 
% \begin{align*}
% X_1 \sim \begin{cases}
% N(0, 1) \text{, if } X_2<0.5\\
% N(4, 4), \text{else}
% \end{cases}
%\end{align*}
    %\item 
    \textbf{Left:} For $X_2<0.5$, permuting $X_1$ (crosses) preserves marginal (but not joint) distribution \\
    $\leadsto$ Bottom: Marginal density of $X_1$
    
    \medskip 
    
    %\item 
    \textbf{Right:} Permuting $X_1$ within subgroups $X_2<0.5$ \& $X_2\geq 0.5$ reduces extrapolation\\
    $\leadsto$ Bottom: $X_1$-density conditional on groups
    %\item \textbf{Bottom left:} Marginal density of $X_1$
    %\item \textbf{Bottom right:} Densities of $X_1$ conditional on the groups
%\end{itemize}
\end{column}
\end{columns}
%$X_2 \sim U(0,1)$ and $X_1 \sim N(0, 1)$ if $X_2<0.5$, else $X_1 \sim N(4,4)$ (black dots)
}
\end{frame}


%TODO: Changed in PFI slides? Also change here!
% RECAP EXTRAPOLATION
\begin{frame}{Recall: Extrapolation in PFI}

 \textbf{Recall:} 
 Let $y = x_3 + \epsilon_y$, with $\epsilon_y \sim \mathcal{N}(0, 0.1)$.

\begin{itemize}
  \item $x_1 := \epsilon_1$, $x_2 := x_1 + \epsilon_2$ are highly correlated  
        ($\epsilon_1 \sim \mathcal{N}(0,1)$, $\epsilon_2 \sim \mathcal{N}(0, 0.01)$)
  \item $x_3 := \epsilon_3$, $x_4 := \epsilon_4$, with $\epsilon_3, \epsilon_4 \sim \mathcal{N}(0,1)$ and all noise terms $\epsilon_j$ are independent
  \item Fitting a linear model yields $\hat{f}(\xv) \approx 0.3 x_1 - 0.3 x_2 + x_3$
\end{itemize}

 % \textbf{Example:} Let $y = x_3 + \epsilon_y$ with $\epsilon_y \sim N(0, 0.1)$ where $x_1 :=  \epsilon_1$, $x_2 := x_1 + \epsilon_2$ are highly correlated ($\epsilon_1 \sim N(0,1), \epsilon_2 \sim N(0, 0.01)$) and $x_3 := \epsilon_3$, $x_4 := \epsilon_4$,  with $\epsilon_3, \epsilon_4 \sim N(0,1)$. All noise terms are independent.
 % Fitting a LM yields $\fh(\xv) \approx 0.3 x_1 - 0.3 x_2 + x_3$.

% %\begin{figure}
% % \hfill
% %   \includegraphics[width=0.3\linewidth]{figure_man/pfi_hexbin_pre.pdf}\hfill
% %   \includegraphics[width=0.3\linewidth]{figure_man/pfi_hexbin_post.pdf} \hfill
% %   \includegraphics[width=0.39\linewidth]{figure_man/pfi_extrapolation.pdf} \hfill
% \centerline{\includegraphics[width=0.9\linewidth]{figure_man/pfi_hexbin_extrapolation.pdf}}
% Hexbin plot of $x_1, x_2$ before permuting $x_1$ (left), after permuting $x_1$ (center), and PFI scores (right)
% \lz
% % \caption{Density plot for $x_1, x_2$ before permuting $x_1$ (left) and after permuting $x_1$ (center). Right: PFI including $.05$ to $.95$ quantiles.}
% %\end{figure}
% % 
% $\Rightarrow$ $x_1$ and $x_2$ should be irrelevant for the prediction $\fh(\xv)$ for $\{\xv: \P(\xv) > 0\}$ as $0.3 x_1 - 0.3 x_2 \approx 0$ \\
% $\Rightarrow$ PFI evaluates model on unrealistic obs. outside $\P(\xv)$ $\leadsto$ $x_1$ and $x_2$ are considered relevant
% %$\Rightarrow$ Since PFI evaluates the model on unrealistic observations, the features $x_1$ and $x_2$ are nevertheless considered relevant

\centerline{\includegraphics[width=\linewidth]{figure_man/pfi_hexbin_extrapolation.pdf}}

Hexbin plot of $(x_1, x_2)$ before (left) and after (center) permuting $x_1$;  
PFI scores (right).

\begin{itemize}
  \item[$\Rightarrow$] $x_1, x_2$ cancel in $\hat{f}$ and should be irrelevant
  \item[$\Rightarrow$] But PFI evaluates model on unrealistic inputs (caused by permutation)\\
  $\leadsto$ $PFI > 0$ for $x_1, x_2$ due to extrapolation\\
  $\leadsto$ $x_1, x_2$ are misleadingly considered relevant
\end{itemize}
 \end{frame}


 
%  % CONDITIONAL SAMPLING PRESERVES THE JOINT
%  \begin{frame}{Conditional Sampling preserves the Joint}
 
%  Let $\pert{x}{S}{-S} = (\pert{x}{S}{-S}_S, x_{-S})$ be the feature vector where feature values $x_S$ were replaced with an independent sample from $\P(x_S|x_{-S})$ while all other remaining feature values $x_{-S}$ stay the same. %We partition $\pert{x}{S}{-S}$ into $(\pert{x}{S}{-S}_S, x_{-S})$ containing features indexed by $S$ and the remaining features by $-S$). 
%  Using the definition of conditional probability:
% %
% \begin{align*}
%   \P(\pert{x}{S}{-S}) = \P(\pert{x}{S}{-S}_S, x_{-S}) &= \P(\pert{x}{S}{-S}_S|x_{-S}) \P(x_{-S})\\
%   &= \P(x_S|x_{-S}) \P(x_{-S}) =  \P(x_S, x_{-S})= \P(x)
% \end{align*}
% \pause

% Consequence: The joint distribution is preserved in the perturbed dataset.\\
% $\Rightarrow$ CFI only evaluates the model within the observational covariate distribution\\
%  \lz
%  \textbf{Note:} That does not imply $\P(x,y) = \P(\pert{x}{S}{-S}, y)$! In the perturbed data, the relationship between $x_S$ and $y$ may still be destroyed (if $x_S$ is not independent from $y | x_{-S}$).\\
%  \end{frame}



% CFI DEFINITION
\begin{frame}{CFI \citebutton{Strobl et al. (2008)}{https://doi.org/10.1186/1471-2105-9-307} \citebutton{Hooker et al. (2021)}{https://arxiv.org/abs/1905.03151}}
\normalsize

%  {\color{blue}\textbf{with feat. values $x_S$}} and {\color{red}\textbf{with permuted feat. values $\pert{x}{}{}_S$}} 
CFI for $X_S$ using test data $\D$:
\begin{itemize}
  \item Measure the error \color{blue}\textbf{with unperturbed features $x_S$}\color{black}.
  \item Measure the error \textbf{\color{red}with perturbed feature values $\pert{x}{}{}_S \sim \P(X_S|X_{-S})$} 
  %\color{black} $\pert{x}{S}{-S}$, where $\pert{x}{S}{-S}_S \sim \P(x_S|x_{-S})$
  \item Repeat perturbing $X_S$ (e.g., $m$ times) and average difference of both errors: 
$$\widehat{CFI}_S = \tfrac{1}{m} \textstyle\sum\nolimits_{k = 1}^{m} \riske (\fh, {\color{red}\pert{\D}{S}{-S}_{(k)}}) - \riske (\fh, {\color{blue}\D})$$
\end{itemize}

Here, $\pert{\D}{S}{-S}$ denotes data, where $x_S$ values are conditionally resampled given $x_{-S}$.

\medskip
\textbf{Illustrative example:} Conditional permutation when $X_{-S}$ is categorical:

\begin{columns}[T, onlytextwidth]
\scriptsize
    \begin{column}{0.5\textwidth}
    \centering
    \textbf{Original Data}
    
    \rowcolors{2}{white}{gray!10}
    \begin{tabular}{ccc}
\toprule
ID & $X_{-S}$ & $X_S$ \\
\midrule
1 & \cellcolor{blue!10}A & \cellcolor{blue!10}3.1 \\
2 & \cellcolor{blue!10}A & \cellcolor{blue!10}2.7 \\
3 & \cellcolor{blue!10}A & \cellcolor{blue!10}3.4 \\
\midrule
4 & \cellcolor{red!10}B & \cellcolor{red!10}6.0 \\
5 & \cellcolor{red!10}B & \cellcolor{red!10}5.4 \\
6 & \cellcolor{red!10}B & \cellcolor{red!10}6.2 \\
\bottomrule
\end{tabular}
    \end{column}
    
    \begin{column}{0.5\textwidth}
    \centering
    \textbf{Permuted Conditionally on $X_{-S}$}

    \rowcolors{2}{white}{gray!10}
    \begin{tabular}{ccc}
\toprule
ID & $X_{-S}$ & $X_S$ \\
\midrule
1 & \cellcolor{blue!10}A & \cellcolor{blue!10}2.7 \\
2 & \cellcolor{blue!10}A & \cellcolor{blue!10}3.1 \\
3 & \cellcolor{blue!10}A & \cellcolor{blue!10}3.4 \\
\midrule
4 & \cellcolor{red!10}B & \cellcolor{red!10}6.2 \\
5 & \cellcolor{red!10}B & \cellcolor{red!10}6.0 \\
6 & \cellcolor{red!10}B & \cellcolor{red!10}5.4 \\
\bottomrule
\end{tabular}
    \end{column}
\end{columns}


\medskip
{
Here, $X_S$ is permuted \textit{within} each group of $X_{-S}$ to preserve $\mathbb{P}(X_S, X_{-S})$.
}

%\footnote[frame]{\fullcite{Strobl2008}}

\end{frame}


%TODO: What is meant by "mechanistically"?
% INTERPRETATION OF CFI
\begin{frame}{Implications of CFI \citebutton{König et al. (2020)}{https://arxiv.org/abs/2007.08283}}

\textbf{Interpretation:} Due to the conditional sampling w.r.t. all other features, CFI quantifies a feature's unique contribution to the model performance.\\
\lz\pause
\textbf{Entanglement with data:}
\begin{itemize}
  \item If feature $x_S$ does not contribute unique information about $y$, i.e., $x_S \indep y | x_{-S}$\\
  $\Rightarrow$ CFI $= 0$
  \item Why? Under the conditional independence 
  $\P(\pert{X}{}{}_S, X_{-S}, Y) = \P(X_S, X_{-S}, Y)$
  %$\P(\pert{x}{S}{-S}, y) = \P(x,y)$
  \\
  $\leadsto$ no prediction-relevant information is destroyed by permutation of $x_S$ conditional on $x_{-S}$
\end{itemize}
\lz\pause
\textbf{Entanglement with model:}
\begin{itemize}
  \item If the model does not use a feature $\Rightarrow$ CFI $= 0$
  \item Why? Then the prediction is not affected by any perturbation of the feature\\
  $\leadsto$ model performance does not change after conditional permutation
  %(and consequently the performance is invariant).
\end{itemize}

%\footnote[frame]{\fullcite{konig_relative_2021}}
\end{frame}


% IMPLICATIONS OF CFI
\begin{frame}{Implications of CFI}

Can we gain insight into whether ...

\begin{enumerate}
    \item<1-3> the feature $x_j$ is causal for the prediction?
    \begin{itemize}
      \item $CFI_j \neq 0$ $\Rightarrow$ model relies on $x_j$ (converse does not hold, see next slide)
    \end{itemize}
    \item<2-3> the variable $x_j$ contains prediction-relevant information?
    \begin{itemize}
      \item If $x_j \not \indep y$ but $x_j \indep y | x_{-j}$ (e.g., $x_j$ and $x_{-j}$ share information) $\Rightarrow$ $CFI_j = 0$
      \item  $x_{j}$ is not exploited by model (regardless of whether it is useful for $y$ or not) $\Rightarrow$ $CFI_j = 0$
      %\item If a feature is not exploited by the model, CFI is zero, irrespective of whether the feature is useful or not.
    \end{itemize}
    \item<3> Does the model require access to $x_j$ to achieve its prediction performance?
\begin{itemize}
      \item $CFI_j \neq 0 \Rightarrow$ $x_j$ contributes unique information (meaning $x_j \not \indep y | x_{-j}$)
      %Nonzero CFI implies that the feature contributes unique information (meaning $x_S \not \indep y | x_{-S}$).
      \item Only uncovers the relationships that were exploited by the model
    \end{itemize}
\end{enumerate}
\end{frame}

%TODO: Better example? 
% RECAP EXTRAPOLATION
\begin{frame}{Extrapolation: Compare PFI and CFI }

\textbf{Recall:} Let $y = x_3 + \epsilon_y$, with $\epsilon_y \sim \mathcal{N}(0, 0.1)$.

\begin{itemize}
  \item $x_1 := \epsilon_1$, $x_2 := x_1 + \epsilon_2$ are highly correlated  
        ($\epsilon_1 \sim \mathcal{N}(0,1)$, $\epsilon_2 \sim \mathcal{N}(0, 0.01)$)
  \item $x_3 := \epsilon_3$, $x_4 := \epsilon_4$, with $\epsilon_3, \epsilon_4 \sim \mathcal{N}(0,1)$ and all noise terms $\epsilon_j$ are independent
  \item Fitting a linear model yields $\hat{f}(\xv) \approx 0.3 x_1 - 0.3 x_2 + x_3$
\end{itemize} 
 % \textbf{Example:} Let $y = x_3 + \epsilon_y$ with $\epsilon_Y \sim N(0, 0.1)$ where $x_1 :=  \epsilon_1$, $x_2 := x_1 + \epsilon_2$ are highly correlated ($\epsilon_1 \sim N(0,1), \epsilon_2 \sim N(0, 0.01)$) and $x_3 := \epsilon_3$, $x_4 := \epsilon_4$,  with $\epsilon_3, \epsilon_4 \sim N(0,1)$. All noise terms are independent.
 % Fitting a LM yields $\fh(\xv) \approx 0.3 x_1 - 0.3 x_2 + x_3$.%\\
 %
\begin{figure}
%\hfill
  \includegraphics[width=0.3\linewidth]{figure_man/pfi_hexbin_pre.pdf}\hfill
  \includegraphics[width=0.7\linewidth]{figure_man/cfi_pfi.pdf} %\hfill
  \caption{Density plot for $x_1, x_2$ before permuting $x_1$ (left). PFI and CFI (right).}
\end{figure}
% 
%$\Rightarrow$ $x_1$ and $x_2$ are irrelevant for the prediction for $x: \P(x) > 0$ \\
%$\Rightarrow$ Since PFI evaluates the model on unrealistic observations, the features $x_1$ and $x_2$ are nevertheless considered relevant \\
\begin{itemize}
    \item $x_1$ and $x_2$ cancel in $\fh(\xv)$ and should be irrelevant for the prediction
    \item PFI evaluates model on unrealistic obs. $\leadsto$ $x_1$, $x_2$ appear relevant (PFI $> 0$) 
    \item CFI evaluates model on realistic obs. (due to conditional sampling)\\
$\leadsto$ $x_1$, $x_2$ appear relevant (CFI $= 0$)
\end{itemize}
% $\fh(\xv)$ for $\{\xv: \P(\xv) > 0\}$ as $0.3 x_1 - 0.3 x_2 \approx 0$ \\
%$\Rightarrow$ Since $x_1$ can be reconstructed from $x_2$ and vice versa, CFI considers $x_1$ and $x_2$ to be irrelevant

 \end{frame}

% \begin{frame}{Bibliography}
%   \printbibliography
% \end{frame}

\endlecture
\end{document}
