\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}

% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}
%\bibliography{feature-importance}

\begin{document}
	\newcommand{\titlefigure}{figure_man/feature-importance.png}
    \newcommand{\learninggoals}{
    	\item Extrapolation and Conditional Sampling
    	\item Conditional Feature Importance (CFI)
    	\item Interpretation of CFI and difference to PFI}
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
% 	\input{../../latex-math/basic-math.tex}
% 	\input{../../latex-math/basic-ml.tex}
% 	\input{../../latex-math/ml-interpretable.tex}
	
	\lecturechapter{Conditional Feature Importance (CFI)}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

%TODO: Add CFI sampling: Conditional subgroup, knockoffs/CPI, others?
%TODO: Add more on when to use conditional, when marginal?
%TODO: Conditioning on everything is not a good idea: CFI=0 for highly correlated variables? Other examples? Relative Importance (Gunnar)?

% CFI IDEA
\begin{frame}{Conditional Feature Importance Idea}

\begin{itemize}[<+->]
    \item \textbf{Permutation Feature Importance Idea:} Replace the feat. of interest $x_j$ with an indep. sample from the marginal dist. $\P(x_j)$, e.g. by randomly perm. obs. in $x_j$
    \item \textbf{Problem:} Under dependent features, permutation leads to extrapolation
    \item \textbf{Conditional Feature Importance Idea:} Resample $x_j$ from the cond. dist. $\P(x_j|x_{-j})$, s.t. the joint dist. is preserved, i.e., $\P(x_j|x_{-j}) \P(x_{-j}) = \P(x_j, x_{-j})$
\end{itemize}
%\\
%\lz
%\\
%\lz
\visible<4>{
\textbf{Example:} Conditional permutation scheme \citebutton{Molnar et. al (2020)}{https://arxiv.org/abs/2006.04628}
\begin{columns}[T, totalwidth = \textwidth]
\begin{column}{0.47\textwidth}
\includegraphics[width=\linewidth]{figure_man/conditional_sampling.pdf}
\end{column}
\begin{column}{0.52\textwidth}
\begin{itemize}
    \item $X_2 \sim U(0,1)$ and $X_1 \sim N(0, 1)$ if $X_2<0.5$, else $X_1 \sim N(4,4)$ (black dots)
% $X_2 \sim U(0,1)$ and 
% \begin{align*}
% X_1 \sim \begin{cases}
% N(0, 1) \text{, if } X_2<0.5\\
% N(4, 4), \text{else}
% \end{cases}
%\end{align*}
    \item \textbf{Left:} For $X_2<0.5$, permuting $X_1$ (crosses) preserves marginal (but not joint) distribution \\
    $\leadsto$ Bottom: Marginal density of $X_1$
    \item \textbf{Right:} Permuting $X_1$ within subgroups $X_2<0.5$ \& $X_2\geq 0.5$ reduces extrapolation\\
    $\leadsto$ Bottom: Density of $X_1$ conditional on groups
    %\item \textbf{Bottom left:} Marginal density of $X_1$
    %\item \textbf{Bottom right:} Densities of $X_1$ conditional on the groups
\end{itemize}
\end{column}
\end{columns}
}
\end{frame}


%TODO: Changed in PFI slides? Also change here!
% RECAP EXTRAPOLATION
\begin{frame}{Recall: Extrapolation in PFI}
 
 \textbf{Example:} Let $y = x_3 + \epsilon_y$ with $\epsilon_y \sim N(0, 0.1)$ where $x_1 :=  \epsilon_1$, $x_2 := x_1 + \epsilon_2$ are highly correlated ($\epsilon_1 \sim N(0,1), \epsilon_2 \sim N(0, 0.01)$) and $x_3 := \epsilon_3$, $x_4 := \epsilon_4$,  with $\epsilon_3, \epsilon_4 \sim N(0,1)$. All noise terms are independent.
 Fitting a LM yields $\fh(\xv) \approx 0.3 x_1 - 0.3 x_2 + x_3$.

%\begin{figure}
% \hfill
%   \includegraphics[width=0.3\linewidth]{figure_man/pfi_hexbin_pre.pdf}\hfill
%   \includegraphics[width=0.3\linewidth]{figure_man/pfi_hexbin_post.pdf} \hfill
%   \includegraphics[width=0.39\linewidth]{figure_man/pfi_extrapolation.pdf} \hfill
\centerline{\includegraphics[width=0.9\linewidth]{figure_man/pfi_hexbin_extrapolation.pdf}}
Hexbin plot of $x_1, x_2$ before permuting $x_1$ (left), after permuting $x_1$ (center), and PFI scores (right)
\lz
% \caption{Density plot for $x_1, x_2$ before permuting $x_1$ (left) and after permuting $x_1$ (center). Right: PFI including $.05$ to $.95$ quantiles.}
%\end{figure}
% 
$\Rightarrow$ $x_1$ and $x_2$ should be irrelevant for the prediction $\fh(\xv)$ for $\{\xv: \P(\xv) > 0\}$ as $0.3 x_1 - 0.3 x_2 \approx 0$ \\
$\Rightarrow$ PFI evaluates model on unrealistic obs. outside $\P(\xv)$ $\leadsto$ $x_1$ and $x_2$ are considered relevant
%$\Rightarrow$ Since PFI evaluates the model on unrealistic observations, the features $x_1$ and $x_2$ are nevertheless considered relevant

 \end{frame}


 
%  % CONDITIONAL SAMPLING PRESERVES THE JOINT
%  \begin{frame}{Conditional Sampling preserves the Joint}
 
%  Let $\pert{x}{S}{-S} = (\pert{x}{S}{-S}_S, x_{-S})$ be the feature vector where feature values $x_S$ were replaced with an independent sample from $\P(x_S|x_{-S})$ while all other remaining feature values $x_{-S}$ stay the same. %We partition $\pert{x}{S}{-S}$ into $(\pert{x}{S}{-S}_S, x_{-S})$ containing features indexed by $S$ and the remaining features by $-S$). 
%  Using the definition of conditional probability:
% %
% \begin{align*}
%   \P(\pert{x}{S}{-S}) = \P(\pert{x}{S}{-S}_S, x_{-S}) &= \P(\pert{x}{S}{-S}_S|x_{-S}) \P(x_{-S})\\
%   &= \P(x_S|x_{-S}) \P(x_{-S}) =  \P(x_S, x_{-S})= \P(x)
% \end{align*}
% \pause

% Consequence: The joint distribution is preserved in the perturbed dataset.\\
% $\Rightarrow$ CFI only evaluates the model within the observational covariate distribution\\
%  \lz
%  \textbf{Note:} That does not imply $\P(x,y) = \P(\pert{x}{S}{-S}, y)$! In the perturbed data, the relationship between $x_S$ and $y$ may still be destroyed (if $x_S$ is not independent from $y | x_{-S}$).\\
%  \end{frame}



% CFI DEFINITION
\begin{frame}{Conditional Feature Importance \citebutton{Strobl et al. (2008)}{https://doi.org/10.1186/1471-2105-9-307} \citebutton{Hooker et al. (2021)}{https://arxiv.org/abs/1905.03151}}
\normalsize

Conditional feature importance (CFI) for features $x_S$ using test data $\D$:
\begin{itemize}
  \item Measure the error \color{blue}\textbf{with unperturbed features}\color{black}.
  \item Measure the error \color{red}\textbf{with perturbed feature values} \color{black} $\pert{x}{S}{-S}$, where $\pert{x}{S}{-S}_S \sim \P(x_S|x_{-S})$
  \item Repeat permuting the feature (e.g., $m$ times) and average the difference of both errors: 
$$\widehat{CFI}_S = \tfrac{1}{m} \textstyle\sum\nolimits_{k = 1}^{m} \riske (\fh, {\color{red}\pert{\D}{S}{-S}_{(k)}}) - \riske (\fh, {\color{blue}\D})$$
\end{itemize}

Here, $\pert{\D}{S}{-S}$ denotes the dataset where features $x_S$ where sampled conditional on the remaining features $x_{-S}$.

%\footnote[frame]{\fullcite{Strobl2008}}

\end{frame}


%TODO: What is meant by "mechanistically"?
% INTERPRETATION OF CFI
\begin{frame}{Implications of CFI \citebutton{KÃ¶nig et al. (2020)}{https://arxiv.org/abs/2007.08283}}

\textbf{Interpretation:} Due to the conditional sampling w.r.t. all other features, CFI quantifies a feature's unique contribution to the model performance.\\
\lz\pause
\textbf{Entanglement with data:}
\begin{itemize}
  \item If feature $x_S$ does not contribute unique information about $y$, i.e., $x_S \indep y | x_{-S}$ $\Rightarrow$ CFI $= 0$
  \item Why? Under the conditional independence $\P(\pert{x}{S}{-S}, y) = \P(x,y)$\\
  $\leadsto$ no prediction-relevant information is destroyed by permutation of $x_S$ conditional on $x_{-S}$
\end{itemize}
\lz\pause
\textbf{Entanglement with model:}
\begin{itemize}
  \item If the model does not use a feature $\Rightarrow$ CFI $= 0$
  \item Why? Then the prediction is not affected by any perturbation of the feature\\
  $\leadsto$ model performance does not change after conditional permutation
  %(and consequently the performance is invariant).
\end{itemize}

%\footnote[frame]{\fullcite{konig_relative_2021}}
\end{frame}


% IMPLICATIONS OF CFI
\begin{frame}{Implications of CFI}

Can we gain insight into whether ...

\begin{enumerate}
    \item<1-3> the feature $x_j$ is causal for the prediction?
    \begin{itemize}
      \item $CFI_j \neq 0$ $\Rightarrow$ model relies on $x_j$ (converse does not hold, see next slide)
    \end{itemize}
    \item<2-3> the variable $x_j$ contains prediction-relevant information?
    \begin{itemize}
      \item If $x_j \not \indep y$ but $x_j \indep y | x_{-j}$ (e.g., $x_j$ and $x_{-j}$ share information) $\Rightarrow$ $CFI_j = 0$
      \item  $x_{j}$ is not exploited by model (regardless of whether it is useful for $y$ or not) $\Rightarrow$ $CFI_j = 0$
      %\item If a feature is not exploited by the model, CFI is zero, irrespective of whether the feature is useful or not.
    \end{itemize}
    \item<3> Does the model require access to $x_j$ to achieve its prediction performance?
\begin{itemize}
      \item $CFI_j \neq 0 \Rightarrow$ $x_j$ contributes unique information (meaning $x_j \not \indep y | x_{-j}$)
      %Nonzero CFI implies that the feature contributes unique information (meaning $x_S \not \indep y | x_{-S}$).
      \item Only uncovers the relationships that were exploited by the model
    \end{itemize}
\end{enumerate}
\end{frame}

%TODO: Better example? 
% RECAP EXTRAPOLATION
\begin{frame}{Comparison: PFI and CFI}
 
 \textbf{Example:} Let $y = x_3 + \epsilon_y$ with $\epsilon_Y \sim N(0, 0.1)$ where $x_1 :=  \epsilon_1$, $x_2 := x_1 + \epsilon_2$ are highly correlated ($\epsilon_1 \sim N(0,1), \epsilon_2 \sim N(0, 0.01)$) and $x_3 := \epsilon_3$, $x_4 := \epsilon_4$,  with $\epsilon_3, \epsilon_4 \sim N(0,1)$. All noise terms are independent.
 Fitting a LM yields $\fh(\xv) \approx 0.3 x_1 - 0.3 x_2 + x_3$.%\\
 %
\begin{figure}
\hfill
  \includegraphics[width=0.25\linewidth]{figure_man/pfi_hexbin_pre.pdf}\hfill
  \includegraphics[width=0.6\linewidth]{figure_man/cfi_pfi.pdf} \hfill
  \caption{Density plot for $x_1, x_2$ before permuting $x_1$ (left). PFI and CFI (right).}
\end{figure}
% 
%$\Rightarrow$ $x_1$ and $x_2$ are irrelevant for the prediction for $x: \P(x) > 0$ \\
%$\Rightarrow$ Since PFI evaluates the model on unrealistic observations, the features $x_1$ and $x_2$ are nevertheless considered relevant \\
$\Rightarrow$ $x_1$ and $x_2$ are irrelevant for the prediction $\fh(\xv)$ for $\{\xv: \P(\xv) > 0\}$ as $0.3 x_1 - 0.3 x_2 \approx 0$ \\
$\Rightarrow$ PFI evaluates model on unrealistic obs. outside $\P(\xv)$ $\leadsto$ $x_1$, $x_2$ are considered relevant (PFI $> 0$) \\
$\Rightarrow$ Since $x_1$ can be reconstructed from $x_2$ and vice versa, CFI considers $x_1$ and $x_2$ to be irrelevant

 \end{frame}

% \begin{frame}{Bibliography}
%   \printbibliography
% \end{frame}

\endlecture
\end{document}
