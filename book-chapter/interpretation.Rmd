---
title: "Untitled"
output: html_document
date: "2023-04-26"
---


# Model Interpretation {#sec-interpretation}

```{r, include = FALSE, cache = FALSE}
library("data.table")
library("mlr3")
library("mlr3book") # remotes::install_github("https://github.com/mlr-org/mlr3book")
requireNamespace("bbotk")
library("mlr3verse")

lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

knitr::opts_chunk$set(error=TRUE)
```

<!-- Predictive models have numerous applications in virtually every area of life.  -->
The increasing availability of data and software frameworks to create predictive models has allowed the widespread adoption of machine learning in many applications. 
However, high predictive performance of such models often comes at the cost of `r index("interpretability")`: 
Many trained models by default do not provide further insights into the model and are often too complex to be understood by humans such that questions like ''What are the most important features and how do they influence a prediction?'' cannot be directly answered. 
This lack of explanations hurts trust and creates barriers to adapt predictive models especially in critical areas with decision affecting human life, such as credit scoring or medical applications.

<!-- TODO: Not sure if the interpretation goals below are too broad as we do not mention HOW and with which IML methods they can be addressed? -->
Interpretation methods are valuable from multiple perspectives:

1. To gain global insights into a model, e.g., to identify which features were overall most important. 
2. To improve the model after flaws were identified (in the data or model), e.g., whether the model unexpectedly relies on a certain feature.
3. To understand and control individual predictions, e.g., to identify how a given prediction
changes when changing the input.
4. For justification purposes or to assess fairness (see @sec-fairness), e.g., to inspect whether the model adversely affects certain subpopulations or individuals. 
<!--TODO: reference to fairness chapter if there is one in the future --> 

In this chapter, we focus on some important methods from the field of interpretable machine learning that can be applied `r index("post-hoc")`, i.e. after the model has been trained, and are `r index("model-agnostic")`, i.e. applicable to any model without any restriction to a specific model class.
<!-- TODO: Interactions fehlen... -->
Specifically, we introduce methods implemented in the `r ref_pkg("iml")`, `r ref_pkg("DALEX")`, and `r ref_pkg("counterfactuals")` packages. Most of the methods are described in more detail in the introductory book about interpretable machine learning by @Molnar2022.
<!-- Model-agnostic means that the methods can be applied to any machine learning model without any restriction to a specific model class. -->
<!-- We distinguish between local and global methods:  -->
<!-- If we want to explain the model behavior over the entire population by taking all observations into account, we should use `r index("global interpretation methods")`. -->
<!-- If we want to understand how the model behaves for single data instances (and the close neighborhood around them), we should use `r index("local interpretation methods")`.  -->

## Training or Test Data? {#sec-dataset}

According to @sec-performance, performance evaluation should not be conducted on the training data but on unseen data to receive unbiased estimates for the performance. 
Similar considerations play a role in the choice of the underlying data set used for post-hoc interpretations of a model: 
Should model interpretation be based on training data or test data? 

This question is especially relevant when we use a performance-based interpretation method such as the permutation feature importance (PFI, see also @sec-feat-importance). 
For example, if a the PFI is computed based on the training data, the reported feature importance values may overestimate the actual importance: the model may have learned to rely on specific features during training that may not be as important for generalization to new data.
This comes from the fact that the performance measured on the training data is overly optimistic and permuting a feature would lead to a huge drop in the performance.
In contrast, computing the PFI on the test data provides a more accurate estimate of the feature importance as the performance measured on test data is a more reliable estimate.
Thus, for the same reasons performance evaluation of a model should be performed on an independent test set (see @sec-performance), it is also recommended to apply a performance-based interpretation method such as the PFI on an independent test set.

::: {.callout-note}
For performance evaluation, it is generally recommended to use resampling with more than one iteration, e.g., k-fold cross validation or (repeated) subsampling, instead of a single train-test split as performed by the hold-out method (see @sec-performance). 
<!-- However, not all resampling strategies can be combined with all interpretation methods.  -->
However, for the sake of simplicity, we will use a single test data set when applying the interpretation methods to produce explanations throughout this chapter.
Since produced explanations can be considered as statistical estimates, a higher number of resampling iterations can reduce the variance and result in a more reliable explanation [@molnar2021relating].

<!-- However, if the test data is used, the prediction performance is an estimate of the generalization performance and consequently the calculated feature importance reflect a feature's importance for good predictions on unseen data.  -->
For prediction-based methods that do not require performance estimation such as ICE/PD (@sec-feature-effects) or Shapley values (@sec-shapley), the differences in interpretation between training and test data are less pronounced [@Molnar2022pitfalls]. 
<!-- Generally, we recommend to use test data to. -->
<!-- TODO: reference missing!--> 
:::


## German Credit Classification Task

Throughout this chapter, we use the `r ref("rchallenge::german")` data, which contains corrections proposed by @groemping2019south. 
The data includes 1000 credit applications from a regional bank in Germany with the aim to predict creditworthiness, labeled as "good" (which is the positive class) and "bad" in the column `credit_risk`.
For the sake of clarity and simplicity, we use only the following out of 20 features:

```{r interpretation-003, message=FALSE, warning=FALSE}
library("mlr3")
task = tsk("german_credit")
task$select(cols = c("duration", "amount", "age", "status", "savings", "purpose",
  "credit_history", "property", "employment_duration", "other_debtors"))
task
```

We want to fit a random forest model using `r ref_pkg("mlr3")`.
For the reasons outlined in @sec-dataset, we use 2/3 of the data to train the model and the remaining 1/3 to analyze the trained model using different interpretation methods. 
We fit a `"classif.ranger"` learner, i.e. a classification random forest, to predict the probability of being creditworthy (by setting `predict_type = "prob"`): 

```{r, interpretation--005-model, message=FALSE,warning=FALSE}
library("mlr3learners")
set.seed(1L)
split = partition(task)
learner = lrn("classif.ranger", predict_type = "prob")
learner$train(task, row_ids = split$train)
```

In the next sections, we will illustrate how the described interpretation methods can be applied to our trained random forest.
To do so, we introduce the following three packages:

-   `r ref_pkg("iml")` presented in @sec-iml, 
-   `r ref_pkg("counterfactuals")`presented in @sec-counterfactuals that implements counterfactual explanation methods, and
-   `r ref_pkg("DALEX")` presented in @sec-dalex.

The `r ref_pkg("iml")` and `r ref_pkg("DALEX")` packages offer similar functionality, but they differ in design choices. 
Both `r ref_pkg("iml")` and `r ref_pkg("counterfactuals")` are based on the R6 class system, thus working with them is more similar in style to the `r ref_pkg("mlr3")` package.
In contrast, `r ref_pkg("DALEX")` is based on the S3 class system and focuses on comparing multiple diverse models on the same plot.

## The `iml` Package {#sec-iml}

The `r ref_pkg("iml")` package [@Molnar2018] implements a variety of model-agnostic interpretation methods and is based on the R6 class system just like the `r ref_pkg("mlr3")` package.
It provides a unified interface to the implemented methods and facilitates the analysis and interpretation of machine learning models.
<!-- Each interpretation method has its own R6 class and inherits from the same parent class -->
<!-- The implemented methods internally originate from the same parent class and use the same processing framework. -->
<!-- Thus, calls to each interpretation method follow the same syntax and also the output and functionalities are consistent (for example, all methods have a `$plot()` method).  -->
<!-- This makes it easy to analyse machine learning models using multiple interpretation tools. -->
Below, we provide examples on how to use the `r ref_pkg("iml")` package using the random forest model fitted above with the `r ref_pkg("mlr3")` package.

### The Predictor Object

The `r ref_pkg("iml")` package supports machine learning models (for classification or regression) fitted by *any* R package.
To ensure that this works seamlessly, fitted models need to be wrapped in a `r ref("iml::Predictor")` object to unify the input-output behavior of the trained models.
A `r ref("iml::Predictor")` object contains the prediction model as well as the data used for analyzing the model and producing the desired explanation.
We construct the `r ref("iml::Predictor")` object using the test data which will be used as the basis for the presented model interpretation methods:

```{r iml-Predictor, message=FALSE, warning=FALSE,  fig.align='center'}
library("iml")
credit_x = task$data(rows = split$test, cols = task$feature_names)
credit_y = task$data(rows = split$test, cols = task$target_names)
predictor = Predictor$new(learner, data = credit_x, y = credit_y, type = "prob")
```

::: {.callout-tip}
The constructor `r ref("iml::Predictor", text = "iml::Predictor$new()")` has an (optional) input argument `predict.function` which requires a function that predicts on new data. 
For models fitted with the packages `r ref_pkg("mlr3")`, `r ref_pkg("mlr")` or `r ref_pkg("caret")`, a default `predict.function` is already implemented in the `r ref_pkg("iml")` package and the `predict.function` argument is not required to be specified.
For models fitted with other packages, the model-specific `predict` function of that package is used by default. 
Passing a custom `predict.function` to unify the output of the model-specific `predict` function might be necessary for some packages.
This is especially needed if the model-specific `predict` function does not produce a vector of predictions (in case of regression tasks or classification tasks that predict discrete classes instead of probabilities) or a `data.frame` with as many columns as class labels (in case of classification tasks that predict a probability for each class label).
<!-- This argument only needs to be specified if the model was not built with the `r ref_pkg("mlr3")`, `r ref_pkg("mlr")` or `r ref_pkg("caret")` packages.  -->
<!-- Since the random forest (`learner`) was fitted with `r ref_pkg("mlr3")`, `predict.function` is already implemented in the `r ref_pkg("iml")` package and does not need to be specified by us. -->
:::





### Feature Importance {#sec-feat-importance}


When deploying a model in practice, it is often of interest which features are contributing the most towards the *predictive performance* of the model.
On the one hand, this is useful to better understand the problem at hand and the relationship between the features and the target to be predicted.
On the other hand, this can also be useful to identify irrelevant features and potentially remove those using feature importance methods as the basis for feature filtering (see @sec-fs-filter).
In context of this book, we use the term `r index("feature importance")` to describe global methods that calculate a single score per feature reflecting the importance regarding a certain quantity of interest such as the model performance. 
<!-- As we focus on performance-based feature importance methods here, the *quantity of interest* usually refers to the improvement regarding a performance measure such as the classification error. -->
The calculated feature importance is reported by a single numeric score per feature and allows to rank the features according to their importance.

::: {.callout-tip}
It should be noted that there are also other notions of feature importance not based on a performance measure such as the SHAP feature importance, which is based on Shapley values (see @sec-shapley and @lundberg2019consistent) or the partial dependence-based feature importance introduced in @greenwell2018simple, which is based on the variance of a PD plot (see @sec-feature-effects).
:::
<!-- based on computing Shapley values for each observation and averaging their absolute values (SHAP feature importance) -->
<!-- quantify the importance of features regarding a certain quantity of interest. -->
<!-- and help us to identify the features that are most important for a prediction. -->

One of the most popular methods is the permutation feature importance (PFI), originally introduced by @breiman2001random for random forests and adapted by @Fisher2019pfi as a model-agnostic feature importance measure, which they called *model reliance*.
The PFI quantifies the importance of a feature regarding the model performance (measured by a performance measure of interest). 
It calculates the decrease in model performance (or, equivalently the increase in the model's prediction error) after destroying the information in the feature by permutation.
Permutation means that the observed feature values in a dataset are randomly shuffled. This destroys the dependency structure of the feature with the target variable and all other features while maintaining the marginal distribution of the feature. 

The intuition of PFI is simple: if a feature is not important, permuting that feature should not affect the model performance.
<!-- TODO: mention here (or at least later in the implementation) that we can compute the difference or the quotient? -->
However, the worse the performance under permutation is in relation to the original model performance, the more important this feature is w.r.t. the prediction performance.
The method requires a labeled dataset (for which both the feature and true target are given), a trained model, and a performance measure to quantify the decrease in model performance after permutation. 
<!-- Then, the following steps need to be conducted for each feature: -->
<!-- 1. Generate a new dataset by permuting the column of the feature of interest.  -->
<!-- 2. Estimate the performance of the model on the permuted data.  -->
<!-- 3. Compare the performance under permutation with the original model performance.  -->



First, we analyze which features are the most important features to classify creditworthiness with the random forest model according to the permutation feature importance method described in @sec-feat-importance. 
The importance of a feature is measured as the factor by which the model's prediction error increases when the feature values are shuffled. 
We initialize a `r ref("iml::FeatureImp")` object with the model and the classification error (`"ce"`) as a loss function and visualize the results with the `$plot()` method.

```{r iml-007, message=FALSE, warning=FALSE, fig.height=3, fig.align='center'}
effect = FeatureImp$new(predictor, loss = "ce")
effect$plot()
```

We see that `r effect$results$feature[1]` is the most important feature. 
That is, if we permute the `r effect$results$feature[1]` column in the data, the classification error of our model increases on average by a factor of around `r effect$results$importance[1]`. 
We say on average because the `r ref("iml::FeatureImp")` method repeats by default the shuffling process 5 times.
In each repetition, the classification error is computed and the results are then averaged.
In general, repeating the shuffling process and averaging the results is recommended because the results can be unreliable due to the randomness of the permutation process. 
The number of repetitions can be set with the input argument `n.repetitions` of `r ref("iml::FeatureImp", text = "iml::FeatureImp$new()")`.
The `$plot()` method of the `r ref("iml::FeatureImp")` object shows the median of the 5 resulting importance values (as a point) and the boundaries of the error bar in the plot refers to the 5 % and 95 % quantiles of the importance values.

::: {.callout-tip}
By default, `r ref("iml::FeatureImp")` uses the ratio of the prediction performance before and after permutation as importance value. 
Alternatively, the difference of the two performance measures can be used by setting `compare = "difference"` in `r ref("iml::FeatureImp", text = "iml::FeatureImp$new()")`.
:::



### Feature Effects {#sec-feature-effects}
`r index("Feature effect")` methods describe how or to what extend a feature contributes towards the *model predictions* by analyzing how the predictions change when changing a feature.
For example, these methods are useful to identify whether the model estimated a non-linear relationship between a feature of interest and the target.
<!-- or to identify whether the model contains interactions. -->
In general, we distinguish between local and global feature effect methods.
Global feature effect methods refer to how a prediction changes *on average* when a feature is changed. 
In contrast, local feature effect methods address the question how a *single* prediction of a considered observation changes when a feature value is changed.
To a certain extent, local feature effect methods can also reveal interactions in the model that become visible when the local effects are heterogeneous, i.e., if changes of the local effect is different across the observations.

A popular global method to visualize feature effects is the partial dependence (PD) plot [@Friedman2001pdp]. 
It visualizes how the model predictions change on average if we vary the feature values of a certain feature of interest.
Later, @Goldstein2015ice discovered that the global feature effect as visualized by a PD plot can be disaggregated into local feature effects associated to a single observation. 
Specifically, they introduced individual conditional expectation (ICE) curves, a visual tool for local feature effects and showed that the PD plot is just the average of ICE curves. 
ICE curves display how the prediction of a *single* observation changes when varying a feature of interest while all other features stay constant (ceteris paribus). 
<!-- TODO: Add a figure for illustration from iml lecture? -->
<!-- To visualize the ICE values of a single feature -- the ICE curve -- the prediction changes are inspected for multiple points (e.g., on an equidistant grid of the feature's value range). -->
Hence, for each observation, the values of the feature of interest are replaced by multiple other values (e.g., on an equidistant grid of the feature's value range) to inspect the changes in the model prediction.
Specifically, an ICE curve visualizes on the x-axis the set of feature values used to replace the feature of interest and on the y-axis the prediction of the model after the original feature value of the considered observation has been replaced.
Hence, each ICE curve is a local explanation that assesses the feature effect of a *single* observation on the model prediction. 
If the shape of the ICE curves is heterogenous and there are lots of crossing ICE curves, this would indicate that the model may have estimated an interaction involving the considered feature.
<!-- To receive an estimate of the global effect, the ICE curves of all observations in a given data set can be averaged. -->

::: {.callout-note}
Feature effects are very similar to regression coefficients $\beta$ in linear models which offer interpretations such as 
''if you increase this feature by one unit, your prediction increases on average by $\beta$ if all other features stay constant''. 
However, they cannot only convey linear effects but also more complex ones (similar to splines in generalized additive models) and can be applied to any type of predictive model.
:::

Now, we inspect how the feature `amount` influences the predictions of the creditworthiness classification. 
For this purpose, we compute feature effects using PD plots and ICE curves.
We generally recommend to accompany PD plots by ICE curves because showing the PD plot alone might obfuscate heterogeneous effects and interactions. 
ICE curves that do not have a similar shape, i.e., are not parallel for a feature, suggest that the feature interacts with other features. 
To plot PDP and ICE, we initialize a `r ref("iml::FeatureEffect")` object with the feature name and select `method = "pdp+ice"`. 
Again, we used the `$plot()` method to visualize the results:

```{r iml-pdp, message=FALSE, warning=FALSE, fig.height=3, fig.align='center'}
effect = FeatureEffect$new(predictor, 
  feature = "amount", method = "pdp+ice")
effect$plot()
```

As we have binary classification task at hand, we see PD and ICE plots for the predicted probabilities of both predicted classes separately.
The plot shows that if `amount` is smaller than roughly 10,000 DM (standing for Deutsche Mark the currency in Germany before the Euro was introduced as cash in 2002), there is on average a high chance that the creditworthiness is `good`.
<!-- We also see that the effect of the amount on the probability of being creditworthy is homogeneous because most of the ICE curves are parallel. -->

<!-- We can also compute and visualize the feature effects of all numeric features at once with `r ref("iml::FeatureEffects")`.  -->

<!-- ```{r iml-pdp2, message=FALSE, warning=FALSE, fig.cap='Feature effects of all numeric features computed with the PDP method implemented in `iml::FeatureEffect` for the penguin classification task and random forest model.',  fig.align='center'} -->

<!-- num_features = c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g", "year") -->

<!-- effect = FeatureEffects$new(predictor, features = num_features, method = "pdp") -->

<!-- effect$plot() -->

<!-- ``` -->

<!-- All numeric features except for study `year` (either 2007, 2008 or 2009) provide meaningful interpretable information. -->

### Surrogate Models

<!-- Interpretable models such as decision trees or linear models can be used as `r index("surrogate models")` to approximate the behavior of a (often very complex) black-box model.  -->
<!-- Surrogate models are simpler models such as a decision trees or linear models that can be trained to mimic the behavior of the black-box model, allowing for greater interpretability and explainability. -->
<!-- `r index("Surrogate models")` aim at approximating the (often) very complex machine learning model using an inherently interpretable model such as a decision tree or linear model as a surrogate. -->
Interpretable models such as decision trees or linear models can be used as `r index("surrogate models")` to approximate or mimic a (often very complex) black-box model.
Inspecting the surrogate model provides insights into the behavior of a black-box model, as the (learned) model structure or (learned) parameters of the surrogate model allows for a certain interpretation. 
For example, the learned model structure of a decision tree or the coefficients of a linear model can be easily interpreted and provide information on how the features formed the prediction.
<!-- Suitable surrogate models are, e.g., decision trees or linear models whose tree structure or coefficients can be easily interpreted.  -->

We differentiate between local surrogate models, which approximate a model locally around a specific data point of interest, and global surrogate models that approximate the model across the entire input space [@Ribeiro2016lime;@Molnar2022].
<!-- While it is very difficult to approximate the whole model, it is much simpler if we only do so for a small area in the feature space surrounding a specific point -- the point of interest.  -->
The surrogate model is usually trained on the same data used to train the black-box model or on data having the same distribution to ensure a representative input space.
For local surrogate models, the data used to train the surrogate model is usually weighted according to the closeness to the data point of interest.
In either case, the predictions obtained from the black-box model are used as the target to train a surrogate model that approximates the black-box model.
<!-- Since surrogate models approximate the black-box model of interest, the target to train the surrogate model are the predictions obtained from the black-box model. -->


#### Global Surrogate Model

With `r ref("iml::TreeSurrogate")`, we can fit a tree-based surrogate model which trains a `r ref("partykit::ctree()")` model from the `r ref_pkg("partykit")` package to the predictions of our random forest model we want to analyze:

```{r iml-globalsurrogate, message=FALSE, warning=FALSE,  fig.align='center'}
treesurrogate = TreeSurrogate$new(predictor, maxdepth = 2L)
treesurrogate$plot()
```

As we have used `maxdepth = 2`, the surrogate tree performs two binary splits yielding 4 leaf nodes.
<!-- By default, the plot method of `r ref("iml::TreeSurrogate")` extracts the decision rules found by the tree surrogate (that lead to the leaf nodes) and visualizes the distribution of the predicted outcomes for each leaf node. -->
`r ref("iml::TreeSurrogate")` extracts the decision rules found by the tree surrogate (that lead to the leaf nodes), and the `$plot()` method visualizes the distribution of the predicted outcomes for each leaf node.
For example, we can see that the first leaf node that results from the split `(status = 0 <= ... < 200 DM | status = ... >= 200 DM / salary for at least 1 year) & duration <= 36` and only consists of persons with good creditworthiness.

We can access the trained tree surrogate via the `$tree` field of the fitted `r ref("iml::TreeSurrogate")` object and can apply the `plot()` function to visualize the underlying structure of the tree surrogate:

```{r iml-globalsurrogate-tree, message=FALSE, warning=FALSE, fig.height=8,  fig.align='center'}
treesurrogate$tree
plot(treesurrogate$tree)
```

<!-- If a penguin comes from the Biscoe island, the model derives the species based on the flipper length. If a penguin comes from the other islands, the model determines the species from the bill length. -->
<!-- It is important to note that statements such as "the bill length and the island determine the species" are in general not valid, since the surrogate model never sees the real outcomes of the underlying data. -->
Since the surrogate model does not see the real outcomes of the underlying data used to train the black-box model (here, the random forest), general conclusions can not be drawn.
Consequently, the conclusions drawn from the surrogate model only hold for the prediction model (if the approximation of the surrogate model is accurate). 
<!-- We can only draw conclusions on the data if the surrogate model approximates the prediction model accurately and the prediction model accurately predicts the species. -->

To evaluate whether the surrogate model approximates the prediction model accurately, we can compare the predictions of the tree surrogate and the predictions of the black-box model.
For example, we can produce a cross table comparing the predicted classes of the random forest and predicted classes of the surrogate tree:

```{r iml-crosstable, message=FALSE, warning=FALSE}
pred_surrogate = treesurrogate$predict(credit_x, type = "class")[,1]
pred_rf = learner$predict_newdata(credit_x)$response
table(pred_rf, pred_surrogate)
```

Mostly, the black-box predicted class and the surrogate predicted classes overlap.
<!-- Furthermore, the `r ref("iml::TreeSurrogate")` object has a field `$r.squared` -->

<!-- R squared measures how well the surrogate tree approximates the underlying black-box model.  -->
<!-- It is calculated as 1 - (variance of prediction differences / variance of black-box model predictions). For the multi-class case, r.squared contains one measure per class. -->

<!-- ```{r} -->
<!-- treesurrogate$r.squared -->
<!-- ``` -->

#### Local Surrogate Model

In general, it can be very difficult to accurately approximate the black-box model with an interpretable one in the entire feature space.
However, it is much simpler if we only focus on a small area in the feature space surrounding a specific point -- the point of interest. 
For a local surrogate model, we conduct the following steps: 

1.  We obtain predictions from the black-box model for a given data set.
2.  We weight the observations in this data set by their proximity to our point of interest.
3.  We fit an interpretable model on the weighted data set using the predictions of the black-box model as the target.
4.  We exploit the interpretable nature of the surrogate model to explain the prediction of our point of interest.

<!-- How can we approach this with the `r ref_pkg("iml")` package?  -->
To illustrate this using the `r ref_pkg("iml")` package, we first select a data point we want to explain. 
Here, we select an observation in the data set, let us call him Steve.
For Steve, the model predicts the following class probabilities:

```{r steve,  message=FALSE, warning=FALSE,  asis='results'}
steve = credit_x[249, ]
steve
predictor$predict(steve)
```

The model predicts the class `r names(which.max(predictor$predict(steve)))` with `r round(max(predictor$predict(steve))*100, 1)`%  probability.
As a next step, we use `r ref("iml::LocalModel")`, which fits a locally weighted linear regression model that provides an explanation why Steve was classified as having bad creditworthiness by inspecting the estimated coefficients.
The underlying surrogate model is L1-penalized such that only a pre-defined number of features per class, say `k` will have a non-zero coefficient.
The default is `k = 3L`.

```{r iml-localsurrogate,  fig.height=3, message=FALSE, warning=FALSE, fig.align = 'center'}
predictor$class = "good"
localsurrogate = LocalModel$new(predictor, steve)
plot(localsurrogate)
localsurrogate$results
```

From the table we see that the three *locally* most influential features are the status, duration, and the credit history.
A closer look at the coefficients reveals that all three features have a negative effect.

<!-- Compared to the global surrogate model, the local surrogate does not have to be accurate w.r.t. the prediction of the black-box model on the whole data set but only w.r.t. to the prediction of the black-box model on the local neighborhood of the point of interest. -->


### Shapley Values {#sec-shapley}
`r index("Shapley values")` were originally developed in the context of cooperative game theory to study how the payout of a game can be fairly distributed among the players that form a team.
This concept has been adapted for use in machine learning as a local interpretation method to explain the contributions of each input feature to the final model prediction of a single observation [@Trumbelj2013Shapley].
The analogy is as follows: 
In the context of machine learning, the cooperative game played by a set of players refers to the process of making a prediction by a set of features for a single observation whose prediction we want to explain.
Hence, the features are considered to be the players.
The total payout, which should be farily distributed among the players, refers to the difference of the individual observation's prediction with the mean prediction.

In essence, Shapley values aim to answer the question of how much each input feature contributed to the final prediction for a single observation (after subtracting the mean prediction).
By assigning a value to each feature, we can gain insights into which features were the most important ones for the considered observation.
Compared to the penalized linear model as a local surrogate model, Shapley values guarantee that the prediction is fairly distributed among the features.
<!-- Shapley values are calculated by considering all possible combinations of features and computing the difference in the model prediction with and without each feature included. -->
<!-- The Shapley value of a feature is then defined as the average contribution of the feature across all possible combinations. -->
<!-- To compute Shapley values, we need to inspect how the prediction changes if a feature value is present vs. when a feature value  -->
<!-- is not present. Not being present means that the feature is set to a different value than the one of the single observation.  -->
<!-- For inspecting the prediction changes, the feature values of the other features do not necessarily have to stay constant (as it is demanded for ICE curves or PD plots) but they can also differ. This is necessary such that the interaction effects are also taken into account.  -->

::: {.callout-note}
Shapley values are sometimes misinterpreted: The Shapley value of a feature does *not* display the difference of the predicted value after removing the feature from the model training.
The exact Shapley value of a feature is calculated by considering all possible subsets of features and computing the difference in the model prediction with and without the feature of interest included. 
<!-- The contribution of a feature is then defined as the difference in the expected prediction with and without the considered feature, averaged over all possible subsets that include the feature. -->
Hence, it refers to the marginal contribution of a feature to the difference between the actual prediction and the mean prediction, given the current set of features.
:::

With the help of `r ref("iml::Shapley")`, we now generate Shapley values for Steve's prediction. Again, the results can be visualized with the `$plot()` method.

```{r iml-006, message=FALSE, warning=FALSE,  fig.height=3, fig.align='center'}
shapley = Shapley$new(predictor, x.interest = steve)
plot(shapley)
```

If we focus on the plot, the Shapley values ($\phi$) of the features show us how to fairly distribute the difference of Steve's probability to be creditworthy to the data set average probability to be creditworthy among the given features. Steve's duration of 9 month has the most positive effect on the probability of being creditworthy, with an increase in the predicted probability of about 5 %. The most negative effect on the probability of being creditworthy has the credit history, with a decrease in the predicted probability of more than 15 %.

<!-- ### Independent Test Data {#subsec-iml-testdata} -->

<!-- It is also interesting to see how well the model performs on a test data set. For this section, 2/3 of the penguin data set will be used for the training set and 1/3 for the test set (default of the holdout method in `r ref("mlr3::resample")`): -->

<!-- ```{r iml-008, message=FALSE, warning=FALSE} -->
<!-- train_set = sample(task_peng$nrow, 2/3 * task_peng$nrow) -->
<!-- test_set = setdiff(seq_len(task_peng$nrow), train_set) -->
<!-- learner$train(task_peng, row_ids = train_set) -->
<!-- prediction = learner$predict(task_peng, row_ids = test_set) -->
<!-- ``` -->

<!-- First, we compare the feature importance on training and test set -->

<!-- ```{r iml-009, message=FALSE, warning=FALSE,  fig.height=3, fig.cap='FeatImp on train (left) and test (right)',  fig.align='center'} -->
<!-- # plot on training -->
<!-- model = Predictor$new(learner, data = penguins[train_set, ], y = "species") -->
<!-- effect = FeatureImp$new(predictor, loss = "ce") -->
<!-- plot_train = effect$plot() -->

<!-- # plot on test data -->
<!-- model = Predictor$new(learner, data = penguins[test_set, ], y = "species") -->
<!-- effect = FeatureImp$new(predictor, loss = "ce") -->
<!-- plot_test = effect$plot() -->

<!-- # combine into single plot -->
<!-- library("patchwork") -->
<!-- plot_train + plot_test -->
<!-- ``` -->

<!-- In both cases, the bill lengths is the most important feature. Since all other features have similar, much lower importance values, the ranking between training and test data slightly changes. The magnitude of values differs between training and test data. For test data FI values of $>$ 15 are measured while for train data the values are $\le$ 0.3. This is because fitting a model means that the model parameters are adapted to have low prediction error on the training data. -->

<!-- We follow a similar approach to compare the feature effects: -->

<!-- ```{r iml-010, message=FALSE, warning=FALSE, fig.cap='FeatEffect train data set', fig.align='center'} -->
<!-- num_features = c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g", "year") -->
<!-- model = Predictor$new(learner, data = penguins[train_set, ], y = "species") -->
<!-- effect = FeatureEffects$new(predictor, method = "pdp") -->
<!-- plot(effect, features = num_features) -->
<!-- ``` -->

<!-- ```{r iml-011, message=FALSE, warning=FALSE, fig.cap='FeatEffect test data set',  fig.align='center'} -->
<!-- model = Predictor$new(learner, data = penguins[test_set, ], y = "species") -->
<!-- effect = FeatureEffects$new(predictor, method = "pdp") -->
<!-- plot(effect, features = num_features) -->
<!-- ``` -->

<!-- As is the case with `FeatureImp`, the test data results are similar to the training results but the magnitude of effects differs slightly. This would be a good opportunity for the reader to inspect the effect of varying amounts of features and the amount of data used for both the test and train data sets on `FeatureImp` and `FeatureEffects`. -->

## The `counterfactuals` Package {#sec-counterfactuals}

Counterfactual explanations try to identify the smallest possible changes to the input features of a given observation that would lead to a different prediction [@Wachter2017].
<!-- answer the question of how we can minimally change the feature values of a given observation such that we obtain a different prediction [@Wachter2017].  -->
<!-- This allows for statements such as: ''If you had the following feature values instead of the present ones, you would have received the desired outcome.'' -->
In other words, a counterfactual explanation provides an answer to the question: "What changes in current feature values do I need to make to achieve a different prediction?"
Such statements are valuable explanations because we can understand which features affect a prediction and what actions can be taken to obtain a different, more desirable prediction.
The goal is to identify the smallest possible set of changes needed to achieve a desired prediction, while still keeping the changes as realistic and interpretable as possible.
Counterfactual explanations can have many applications in different areas such as healthcare, finance, and criminal justice, where it may be important to understand how small changes in input features could affect the model's prediction.
For example, a counterfactual explanation could be used to suggest lifestyle changes to a patient to reduce their risk of developing a particular disease, or to suggest actions that would increase the chance of a credit application being approved.

The simplest method to generate counterfactuals is the What-If approach [@Wexler2019].
For a data point, whose prediction should be explained, the it usually returns a single counterfactual which is equal to the closest data point of a given data set with the desired prediction. 
Instead of a single counterfactual, we might be interested in multiple counterfactuals.
@Dandl2020 introduced the multi-objective counterfactuals method (MOC) that can also generate multiple counterfactuals. 
Compared to the What-If method, the counterfactuals generated by MOC are not equal to observations in a given dataset but are artificially generated data points. 
The generation of counterfactuals is based on an optimization problem that aims for counterfactuals that 

1) have the desired prediction,
2) are close to the observation of interest (here, our penguin Steve),
3) only require changes in a few features, and
4) originate from the same distribution as the observations in the given dataset.

All these four objectives are optimized simultaneously via a multi-objective optimization method.

In previous years, multiple methods to generate counterfactuals have been proposed.
They mainly differ in how the counterfactual explanations are generated, the number of counterfactuals they return, and the properties of counterfactuals targeted by a method (e.g., sparsity or actionability of the proposed minimal changes to the input space). 
<!-- that differ in the counterfactual properties they target, the generation approach, or the number of returned counterfactuals.  -->


The `r ref_pkg("counterfactuals")` package depends on the `r ref_pkg("iml")` package and can be used for methods that generate counterfactual explanations, or short counterfactuals. Counterfactual explanations explain the prediction of a data point by presenting which minimal changes in the point are sufficient to receive a different, desired outcome.

An example for a counterfactual for the `r ref("palmerpenguins::penguins")` data would be: "If the penguin has a bill length of 45 mm instead of 35 mm and a flipper length of 210 mm instead of 180 mm, the penguin would have been classified as Gentoo with a probability of \> 50 %" (@fig-counterfactuals-penguins). By revealing the feature changes that alter a decision, the counterfactuals reveal which features are the key drivers for a decision.

```{r interpretation-counterfactuals-fig, echo=FALSE, out.width = '50%', fig.align='center'}
#| label: fig-counterfactuals-penguins
#| fig-cap: Illustration of a counterfactual explanation. The blue dot displays a counterfactual for a given point (brown dot) which proposed changes in bill and flipper length such that the prediction changes from Adelie to Gentoo with > 50 \% probability.
#| fig-alt: Illustration of counterfactual explanations. Two dots are shown one that is the point whose prediction we want to explain and the other is it's counterfactual. The counterfactual proposes to increase the bill and flip length such that the point is classified as a Gentoo instead of Adelie with a probability of more than 50 \%.
knitr::include_graphics("Figures/counterfactuals_penguins.png")
```

Many methods were proposed in previous years to generate counterfactual explanations. These methods differ in what targeted properties their generated counterfactuals have (for example, are the feature changes actionable?) and with which method (for example, should a set of counterfactuals be returned in a single run?). Due to the variety of methods, counterfactual explanations were outsourced into a separate package instead of integrating these methods into the `r ref_pkg("iml")` package. Currently, three methods are implemented in the R package but the R6-based interface makes it easy to add other counterfactual explanation methods in the future.

### What-If Method

<!-- In the following, we focus on the simplest method among the three implemented ones: the What-If approach [@Wexler2019]. For a data point, whose prediction should be explained, the returned counterfactual is equal to the closest data point of a given data set (here, the test data) with the desired prediction. To illustrate the overall workflow of the package, we generate counterfactuals for Steve, the first penguin in the data set. -->

To illustrate the overall workflow of the package, we generate counterfactuals for Steve (the first person in the data set) using the What-If approach [@Wexler2019].
The `r ref_pkg("counterfactuals")` package relies on `r ref("iml::Predictor()")` as a model wrapper and can, therefore, explain any prediction model fitted with the `r ref_pkg("mlr3")` package, including the random forest model we trained above.

```{r interpretation-predictsteve, message=FALSE, warning=FALSE}
predictor$predict(steve)
```

We saw above that the random forest classifies Steve with `r round(max(predictor$predict(steve))*100, 1)`% as being `r predictor$class` creditworthy. The What-If method can be used to answer, e.g., how the features need to be changed such that Steve is classified as `r predictor$class` with a probability of more or equal to 60%. Since we have a classification model we initialize a `r ref("counterfactuals::WhatIfClassif()")` object. By calling `$find_counterfactuals()`, we generate a counterfactual for Steve.

```{r interpretation-whatif, message=FALSE, warning=FALSE}
library("counterfactuals")
whatif = WhatIfClassif$new(predictor, n_counterfactuals = 1L)
cfe = whatif$find_counterfactuals(steve, 
  desired_class = "good", desired_prob = c(0.6, 1))
```

`cfe`is a `r ref("counterfactuals::Counterfactuals")` object which offers many visualization and evaluation methods. For example, `$evaluate(show_diff = TRUE)` shows how the features need to be changed.

```{r interpretation-whatifevaluation, message=FALSE, warning=FALSE}
cfe$evaluate(show_diff = TRUE)
```

To receive a probability of more than 60 % for being creditworthy, the age is reduced by 8 years and the amount is reduced by 3603 DM while the duration is enlarged by 3 months. Furthermore, Steve's credit history must become such that he paid all his credits at this bank back duly instead of having a critical account/other credits elsewhere and his status must be no checking account. For the employment duration, other debtors, property, purpose and the savings no changes are required. Additional columns reveal the quality of the counterfactuals, for example, the number of required features changes (`no_changed`) or the distance to the closest training data point (`dist_train`) which is 0 because the counterfactual *is* a training point.

### MOC method 

<!-- Instead of a single counterfactual, we can also generate multiple counterfactuals with the multi-objective counterfactuals method of @Dandl2020.  -->
<!-- Compared to the What-If method, the counterfactuals generated by MOC are not equal to observations in a given dataset but are artificially generated.  -->
<!-- The generation of counterfactuals is based on an optimization problem that aims for counterfactuals that  -->

<!-- 1) have the desired prediction, -->
<!-- 2) are close to the observation of interest (here, our penguin Steve), -->
<!-- 3) only require changes in a few features, and -->
<!-- 4) originate from the same distribution as the observations in the given dataset. -->

<!-- All these four objectives are optimized simultaneously via a multi-objective optimization method. -->

Calling the MOC method is very similar to calling the What-If method. Instead of a `r ref("counterfactuals::WhatIfClassif()")` object, we initialize a `r ref("counterfactuals::MOCClassif()")` object. 
We set the `epsilon` parameter to 0 to penalize counterfactuals in the optimization process with predictions outside the desired range. 
With MOC, we can also prohibit changes in specific features, here year and sex, via the `fixed_features` argument.
For illustrative purposes, we let the multi-objective optimizer only run for 30 generations. 

```{r interpretation-mocmulti,echo=TRUE, message=FALSE, warning=FALSE}
library("counterfactuals")
set.seed(123L)
moc = MOCClassif$new(predictor, epsilon = 0, n_generations = 30L, 
  fixed_features = c("age"))
cfe_multi = moc$find_counterfactuals(steve,
  desired_class = "good", desired_prob = c(0.6, 1))
```

Since the multi-objective approach does not guarantee that all counterfactuals have the desired prediction, we remove all counterfactuals with predictions not equal to the desired prediction via the `$subset_to_valid()` method.
```{r interpretation-mocmulti-subset, message=FALSE, warning=FALSE}
cfe_multi$subset_to_valid()
cfe_multi
```
Overall we generated `r nrow(cfe_multi$data)` counterfactuals.
For a concise overview of the required feature changes, we can use the `plot_freq_of_feature_changes()` method.
It visualizes the frequency of feature changes across all returned counterfactuals.

```{r interpretation-mocfreq, message=FALSE, warning=FALSE, fig.height=3.5, out.width = '80%', fig.align='center'}
cfe_multi$plot_freq_of_feature_changes()
```

We see that the credit history and status are the only changed features.
The`$parallel_plot()` method shows *how* the features have been changed:
The blue line corresponds to the original feature values of Steve, while the gray line displays the counterfactuals.

```{r interpretation-mocparallel, message=FALSE, warning=FALSE, fig.height=3.5, out.width = '80%', fig.align='center'}
feat_freq = cfe_multi$get_freq_of_feature_changes(subset_zero = TRUE)
cfe_multi$plot_parallel(feature_names = names(feat_freq))
```
We see that the credit histories are better or equal to Steve's value of having a critical account/other credits elsewhere (changes to all credits at this bank paid back duly, existing credits paid back duly till now, no credits taken/all credits paid back duly) and that the status is one time lowered to no checking account, three times stayed on ... < 0 DM and one time increased to ... >= 200 DM / salary for at least 1 year. 
To assess whether the proposed features changes are minimal, we can visualize counterfactuals with only two changed features (see column `no_changed` for the number of changed features) on a 2-dim ICE plot, also called surface plot.

```{r interpretation-moc2features,message=FALSE, warning=FALSE, fig.height=3.5, out.width = '80%', fig.align='center'}
cfe_multi$evaluate("no_changed", show_diff = TRUE)
```

Our generated set of counterfactuals contains only one observation with two feature changes. 
For this counterfactual, credit history and status differ from the values of Steve. 
The feature names of these two features serve as an input to the `$plot_surface()` method to generate the 2-dim ICE plot.

```{r interpretation-mocsurface,message=FALSE, warning=FALSE, fig.height=3.5, out.width = '80%', fig.align='center'}
cfe_multi$plot_surface(feature_names = names(feat_freq[1:2]))
```
The colors and contour lines indicate the predicted value of the model when credit history and status differ while all other features are set to the values of Steve. 
The white point displays Steve, the black point is the counterfactual that only proposed changes in the two features. The rugs show the marginal distributions of the features in the observed dataset.
We can see that the counterfactual is in a cluster surrounded by other points with predictions larger or equal to 0.6.
