\documentclass[10pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\titlemeta{
PLACEHOLDER
}{
Shapley Values Aggregation
}{
figure/sample-dgp-2d.pdf
}{

\item Understand structure of tabular data in ML
\item Understand difference between target and features
\item Understand difference between labeled and unlabeled data
\item Know concept of data-generating process
}


\begin{vbframe}{Shapley Values Aggregations}

  \begin{itemize}
    \item Shapley Values are local explanations.
    \item When computed for many observations, can be aggregated to global explanations.
      \begin{itemize}
        \item Feature Importance
        \item Summary plots
        \item dependence plots
        \item Interactions plots
        \item clustered Shapley values
      \end{itemize}
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Feature Importance}
\end{vbframe}

\begin{vbframe}{Summary Plot}
\end{vbframe}

\begin{vbframe}{Dependence Plot}
\end{vbframe}

\begin{vbframe}{Interaction Plots}
\end{vbframe}

\begin{vbframe}{Clustered Shapley values}
\end{vbframe}





\endlecture
\end{document}
